,comment
0,how to make a presentation like this? With which software?
1,Great!
2,"""It's just a function."" Yeah, so are our neurons. But put enough ""dumb functions"" together and you get Justin Bieber. The end is neigh."
3,the Chinese subscript is just chaos
4,"Incomplete Lion picture ü§î since the eye detects color in different color gradients, why don‚Äôt they apply the same way it read the lion image but in different ‚Äúcolor‚Äù gradient screen and come back with the complete picture"
5,@3blue1brown please make videos on CNNs and LSTMs
6,One of the neatest dives into the broad field of machine learning ever on Youtube....long live 3b1b
7,"It is very excellent video, thank you. but I found at 14:59 this video, the bias vector's dimension should be 0-k, not 0-n. Because W*a is kx1 matrix"
8,"Please do an animation of Spiking Neural network (SNNs), on neuromorphic hardware."
9,this cracks me up. . . i was like wow fing amazing sigmoid ! then the end? woah me grasshopper anyway the key is analog to digital and digital to analog
10,0:50
11,"I just found out about this channel today, watched 3 or 4 videos, loved them all, subscribed.
Thank you very much for this content!"
12,Your green and red isn‚Äôt nice for colourblind people
13,The end line is pure genius
14,"Fourth year stats PhD student here, appreciating the heck out of this video."
15,"3Blue1Brown, you are a literal genius."
16,Can you do another video on transformer neural nets?
17,most educational and effective way of teaching
18,Every video you make amazes me with its simplicity in explaining such complicated ideas. Thank you
19,"Literally just did an assignment in my computer vision class to detect what number is shown in a 28x28 binary image of a number like that. So when he asked if we could solve that, in this single occasion, I could bust out the code to do this problem lmao"
20,"Hello ! I noticed actually at 15:08 that the last bias index on the bias vector actually the k-bias is and not the n-bias is, because the n actually designates the number of neurons of the first layer, and k the number of neurons of the second layer, so that they would be k bias. Or do this matrix and vector actually represent a specific case when the number of neurones in the first and in the second layer actually the same is ? Else, it's a really good video, helps me to more understand computational pathology :)"
21,"How is this info free
I would mow your lawn for this"
22,This is simply the best explanation of a neural network I've ever seen. Thanks a lot üòä
23,Does sigmoid have an influence in the performance of a neural network?
24,Next video ??
25,"My English is not good enough, I feel a little strenuous, it‚Äôs a pity that there are such good video here"
26,T√ºrkiyeye selamlar :)
27,Ngl the best video I found after searching for a long time
28,Huge fan! Ty for such great educational content.
29,great
30,Indonesia languag?
31,Thank you
32,Bias time
33,Shouldn't the b vector at 15:00 contain b0 through bk instead of bn?
34,"Someday, I'm going to understand what the hell this is. Learning JavaScript now and pushing myself out of my comfort zone."
35,"I love the graphics in this channel! The level of detail, like the little numbers representing the grayscale of each pixel on the matrix, just blow me away!
Thanks for such informative and inspiring work!"
36,Thanks!
37,Thank you so much for this üòä
38,It took me one week to understand this when I was reading a university lecture. You explained it to me in 20 mins. You are such a savior. Thanks 3Blue1Brown!
39,Life is an optimization problem complicated by local maxima
40,Thanks!
41,0:38 meanwhile my math teacher when a 0 looks slightly like a 6
42,"Wow, very explicitly explained and best of all possible"
43,"(1) Doesn't the matrix @14:53 should be 1 row matrix only [w(0,1)-w(0,784)]? (2) @11:27 Neither you plain clearly why and how the weights tell you what pixel pattern this neuron in the second layer is picking on, (3) nor how the function makes the computer to read the pixel pattern. Since there is a mistake in this video plus this is not clear so I don‚Äôt recommend others to look at more."
44,How could Relu activation function give a neuron value of 1?
45,Why people can‚Äôt still understand  my hand writing tho
46,I WROTE CODE THAT HELPS TELL NATURAL BOOBS FROM FAKE BOOBS WHILE THEY ARE IN THE BRA AND BEHIND CLOTHING. I CALL IT THE HOOTERS ALGORITHM
47,woooo
48,In few months or no time people will definitely be kicking themselves in regret for missing the opportunity to buy or invest in cryptocurrency. Can my fellow investor's say HELLO.
49,"I recommended a professional broker to you guys sometime ago,can i get person who invested with her
Comment below"
50,i would go to class if you taught me üò£
51,It's just great.
52,I am watching this while my ANN is  training :)
53,Woooah üò≥
54,Merci¬†!
55,"This is exactly what I needed, A deep, general and conceptual video of the Neural Networks"
56,yes
57,"I think the bias column vector (b) shown at timestamp 14;38 should have a dimension of  ""k"" long not ""n"" long."
58,Awesome video. Thank you
59,"I'm really grateful
 for you ü§ó"
60,amazingüëç
61,ÌçºÏÖâÌä∏Î°†ÏùÄ Ïó¨Îü¨Í∞úÏùò ÏûÖÎ†•Îã®ÏûêÏôÄ ÌïòÎÇòÏùò Ï∂úÎ†®Îã®ÏûêÎ°ú Íµ¨ÏÑ±ÎêòÏñ¥ÏûàÎäîÎç∞...
62,Hot damn
63,this is just great.
64,"Huh, I get it. These are complexly simple. Reducing every aspect of our soft world to simple numbers. Literally."
65,shouldn't the bias vector be of size kx1 ?? why are there 'n' number of biases?
66,The gentle format of your videos makes you the David Attenborough of math
67,Beautifully explained even though it would have been a little harder to understand from the book and more time consuming. I think after watching this video and then going back to the book would be much easier to go in a flow.
68,"After thousands of years of clicking ""not interested"" on cat/music videos, the Algorithm has finally -punished- rewarded my self-improvement phase with an educational video"
69,Thanks!
70,"Oh, my gosh, this is the best video to explain deeplearning that I'v watched, cheers, you are excellent!"
71,Best !!!!
72,"If each pixel has a grayscale value between 0 (black) and 1 (white), how can black pixels be used to contribute to the activation function output in the strategy mentioned at 10:02? Multiplying 0 by a weight value gives 0 and doesn't contribute to the activation function output."
73,why do we need weighted sum > 10 in 19:13 ? May anyone answer to me? Thank you
74,Wah
75,"I hate that we use the term ""neurons"" for deep learning. What a slap in the face to a real neuron."
76,i'm literally in highschool watching this for fun and somehow my dumbass is able to understand it. Great work!!!
77,i'm confused
78,can anyone help me with what is actually the weight is??
79,Âº∫
80,"Original bnn tree of life, only a single input node per person"
81,One of my all-time favourite videos!!
82,Lisha Lee's vocal frying is top notch! j/k loved the vid. Thank you.
83,just start working on my thesis by watching this video
84,Thanks!
85,Looks like I won't be taking computer science next term. Thanks for showing me I suck at math. More like I lack the necessary mathematical common sense.
86,I don't know what to say!!! but Thank you!
87,Thanks!
88,Subbed. The animations reveal how much effort you put in your videos! Thank you for taking the time to educate us! I am also a fan of how you elucidated everything with easy to understand language avoiding  jargons.
89,"you just explained a week of AI learning material in 20 minutes.
And I understood this better. Kudos to u."
90,Thanks!
91,people with bad handwriting types  a number. the neural network : oh who the shit is typing
92,üòÉüëçüëçüëç
93,"–Ø –Ω–µ–º–Ω–æ–≥–æ –æ—Ñ–∏–≥–µ–ª –æ—Ç —Ç–æ–≥–æ, —á—Ç–æ –≤—Å–µ –ø–æ–Ω—è–ª. Thx for your work"
94,Valeu!
95,Wonderful Animation. Please give me lead to owe such animation.
96,Thank you for this! You are helping so many people üôè
97,Well done my man! Much appreciated :)
98,"Animations, wow!"
99,Lit up!
100,"OH MY GOD !!!!!!!!!!

thanks a lot"
101,How can someone consistently deliver great content like this?
102,"wait let me explain, by i love pain, cell block 6, OH boy, the phones didn't work, and i said no problem, i feel nothing, because i have felt no one, basically you set it up to try and be fifity fifty, but at this point it is now cleary at 60, drake cell block 6 cell 6 i sex 4 times try let me explain, no it isn't lyrics, it isn't a haiku, it isn't fake, i am not gay, i love pain because it reminds me that i am alive, remember seizures, then remember i have only had negative expereinces in my laugh, 33 year who still has never touched a womans boobs, i know they are soft, like her lips, soft, i know they have to be soft, but i know a smiling face, i know when people are happy, i know if i sacrifice, and no one cares about me, then more people will smile, that is simple, that is a journal original, and why not make the one who has never felt anything good suffer, just keep suffering, just have him keep suffering, and then when the time comes, keep going and never stop, that stops you from putting it on other people, it stops you from putting it on other people. 

ok that is the poetic shitty rap head spce shit, what poetic justice shit version, they were on lockdown, they wanted phones i wanted a court date, they refuse to give me a court date, i refuse to go back to my cell, and in the end i offered to go on nutri loaf, and scared i love pain, but then also around that time, i was punch by some wanna be military, because i was yeling this is a paramilitary facility, and so then he punched me, and i yelled i love pain, so i yelled twice and it seems that story is then used to say he loves pain, he scared, he was being a bitch, or then he was scared, or then he likes it in the ass, or then that he likes to where heels and suck cock, so yet again and again you see these things, and you leave him on an island, challenge island, my brother's creative invention to try and come up with something, he mention he was doing his taxes and looking at his reciepts, wow my life really has become that much of a joke, and whats worse is just how much celebrities have helped them, politicians have helped them, and then it is like, why, why, what is it your accomplishing? has it ever occurred to you that i know better than you, because people learn from there fathers, and then do better, i mean that is what sons do, they learn from thier fathers then do better, it seems to me, that what is happening is an underestimation of knowledge, truly an underestimation of whatever it is that it is that church teaches. ask someone what is wrong, if you see a problem, if you are hurt, say so, if you are hurt, then say so, if you are worried express those fears, if you are unsure, and know you can ask, then do so, if you are like me, and it turns out no one knows you, no one knows you the way you thought, then sadly, just find peace with yourself. because friendship would of been solidfied at the first week, the first month, yes uncertainty, but the conversation goes,
dwight: hey did you do something online, with like a journal, like on purpose, 

or like you know go to the lake behind the house, like you did, and ask if i ever meant to realease anything, release anything on purpose? didn't do it, didn't tell me anything, instead fka twigs and some dumb stripper shit, yea sabbs suck by the way, good luck with your relationship goals, bitch can dance though bitch can dance
money? recongnition? didn't you find it strange nothing was coming my way, it seems to me that the real problem, is that no one bothered to care in the beginning, and the people who may have some type of caring, by the time they understood, by the time they knew the scale it was too late, but when, and when is the funny thing because i am getting 6 shit and that mixtape came out nearly 7 years ago, sooooo what is it, what are you like trying to get at? because i am still me, still the same person, and now i know no one loves me, and no i am not going to eat worms, unless you count curry chicken worms? which dead black jamicans, maybe, soulless and unrepent even when they are wrong, another earth quote down in the books, or maybe that was light work or misinformation, no black people were harmed in the making of this news story. so earthquake, drake has a plane, and the ugly jenner that sells makeup is a billionare, yea the empire has long since fallen"
103,"Just saw this series of videos and I'm now hungry for more. Congratulations on your unique capacity to make visual very complex information. The last episode flew a bit over my head at times, but I really appreciate you took the time to go to that level"
104,"huh, im surprised at how simple the actual network part is"
105,16/10 --> 1.6... wonder why 16 was a nice number to put into the screen
106,ÂçßÊßΩÔºåËÆ≤ÁöÑÁúüÂ•ΩÔºåÊÑüË∞¢Â§ß‰Ω¨ÁøªËØë
107,Leisha speaking out of nowhere gave me a heart attack. Interesting concept of ReLU introduced though :D
108,"A phenomenal explanation , I can't find words to thank you"
109,+playlist
110,"People who dislikes this video, their neurons are not working as expected."
111,ü§ØThank you
112,"Gosh, and I was complaining that sometime ago I used to went to a place that all that I should do is learn‚Ä¶ I was in paradise and didn‚Äôt knew it"
113,Thank you for keeping the video simple yet highly educational. Looking out for more content.
114,Is there a code related to this video in Github?
115,9 is a denial within a prime
116,Thanks YEAH
117,"First i will like the video, pray for you, give a big hug and then watch...Thankyou for everything!"
118,Hii Every one
119,Wow amazing video. But how did you simulate the neurons movement's. Could you help me with that?
120,??????
121,"The vigorous maria uniquely change because paul successfully cover a a undesirable birthday. subsequent, common heart"
122,D√≠ky!
123,"When i scrolled through my recommendations, i had an optical illusion due to the connections of the dots in the thumbnail as soon as the thumbnail moved."
124,Faking society
125,"Sorry but this gave me a stroke watchiny and I'm only on the first part. If somebody wants to help me basically what I got Is, The Algorithm once fully complete looks for specific shapes curves and angles with a bias setting for lenience, Than puts mulite parts of the shape together to see witch shape it most correlates with? Also I wanted yo make this neardy joke. I don't have enough neurons to understand these neurons and Have the information usable in my head"
126,Looks like positronic brain networks aren't that far off. This brings me several steps closer to understanding cascading failures both in AI and actual human neurons. It's exciting. Thank you.
127,Í∞êÏÇ¨Ìï©ÎãàÎã§.
128,We have this project to recognize the handwritten numbers and animals in University. This is pretty hard to understand.
129,"Excellent video.  I will watch the whole series.  You mention that most people don't receive notifications about your videos.  Apparently, you cannot sign up for notifications of videos that are ""made for children."""
130,I have to clean my screen everytime i watch 3B1B videos
131,"This guy is the master of explanations !
Anyone interested in a serie on ConvNets and/or RNN ? 
The fact he mentions them as possible future videos but still didn't do it makes me think he either doesn't have time, or he thinks those models have no future 
Thanks !"
132,"So are the weights randomly determined when the network starts out? Then slowly over time, the weights start taking shapes that correspond more clearly to digits?"
133,Bullshit!
134,If im trying to highlight specific pixels is it enough if I add negative weights to the connections im not interested in or do I have to add biases too?
135,Thanks!
136,Thanks
137,"Traditionally, a bias unit is added to the layer with a value of 1 and an associated weighting parameter. Only on the hidden layers."
138,Dislikes are those people who didn't get it.
139,Thanks for such good video
140,"at 3:51 I paused, clicked on your channel name, viewed the other topics, and then subscribed. Excellent explanation!"
141,"Gonna be honest, I find this explanation to be so much clearer and transparent than another popular youtuber's"
142,"uhmmm...im living under the rock, so will u explain?"
143,"Wow .... you showed that doing your (hard) work, you made the concept so easy for everyone to grasp.  Amazing & it wants me to keep learning (& remembering my linear transformation :-))"
144,"wow, the value of knowledge... is like 4 years of study in just 20 minutes"
145,"Thank you, I‚Äôm grade 12 student doing the seminar on the topic of neural network, this video help me very much!"
146,Very impressive video.  I just subscribed.  What software do you use for creating the videos with the wonderful animation?
147,"Every single time I watch one of your videos, I can't stop being astonished by how genius of an educator you are. Thank you so much for these explanations, you are doing great good for a future generation of technical experts (and more)"
148,Her name Veronica
149,Suddenly woman appears
150,It was quite complicated but no less but least
151,4k: /tidak melulu sedemikian sehingga
152,"Thank you Grant, you are the best..."
153,"To get to this channel in the YouTube algorithm start by watching anything where Smarter Every Day does a colab. Then you get recommended Veritasium. In search for deeper understanding of the concepts in those videos on you proceed to PBS Space Time. Because they assume you're already studying quantum physics you go to Minute Physics to get more detail. Then the logical conclusion is you end here and look where you kept those books on learning Python that you never opened üòú

Edit: browsing the videos on this channel shows that it could've been a single hop from ""Smarter Every Day does a colab"" LOL üòÇ Probably there is something like six degrees of separation between YouTube videos on similar topics."
154,"pure gold, bro"
155,"There is one problem with the explanation in that video, which may or may not be addressed in your later videos, that I have not watched yet.  You seemed to assume that the number 7 would be more or less centered in the window, so that the horizontal line that forms the top of the 7 could be associated with specific pixel locations.  But you could have a small 7 in the upper right corner, and none of the pixels located where the horizontal line of the 7 is expected would actually light up.  So the system has to be way more sophisticated even just to pick out a 7.  It would need to identify a pattern with a more or less continuous, more or less horizontal, line, surrounded at both top and bottom by blacked out patterns.  And it would need to do that whether the line was long or short, regardless of where it was located in the window, and regardless of whether it slanted a bit up or down rather than being perfectly horizontal.  I can imagine trying to write a formula that would identify such a line, but it would be complicated.  And actual neural networks don't work by having people figure out such functions, right.  Somehow the machine identifies the relevant pattern.  Despite the fact that this video was useful, it must take a lot of study before how this all works gets less opaque."
156,"After i watch this video, i sure that, ""just act like software engineer as usual  instead of learn about data scientist"""
157,Rong
158,"You can train your neural network for image classification even without writing any code in an Android app called Pocket AutoML. It trains a model right on your phone without sending your photos to some ""cloud"" so it can even work offline."
159,"With ReLu, the output of the function is no longer between 0 and 1. Is that an issue?"
160,there are exactly 42 bias... this neural network has the answer for everything inside it!!!
161,"squishification
likification
subscribification
sharification
replyification"
162,"I have an question ‚ÅâÔ∏è

Hi, im currently in my journey of studying Data science and machine learning .

But I'm very bad with the needed Linear Algebra, Calculus, statistics Etc.

I'm very sharp with learning and i pick up everything very fast too, so there shouldn't be a problem in learning it.

My question is this, what do i need to know before I jump in to linear algebra Calculus statistics etc for data science and machine learning.?

Would you be so kind and let me know in a replay in what order and what i have to study to get there with the knowledge needed to succeed in those fields....

I'm Very determined, i have given myself 5-10 years to master Data science and machine learning .

I would be so thankful to you if you could give me a guide, and maybe the place i would be able to go over and study all that too, if its on some of your videos I'll study there too.

My idea was to study at Coursera.

Thank you.! ‚ù§Ô∏è"
163,you are simply one of the greatest gift to humanity....
164,if i ever become something in AI imma have to thank you first my g :)
165,"Me, feeding a neural network it's own weights and outputs into it's inputs to make it self aware:
*Detroit: Become Human*"
166,Hey is that an Artificial Neural Network  (ANN ) example cuz it can recognise number images but its not CNN ? HELP ME WHAT TYPE IS THIS NETWORK ITS IMPORTANT
167,Extended self thoughts understand concepts more deeply than face-to-face teaching. No professor will ever explain a basic concept this way.
168,"These are not educational vids, it's infotainment vids"
169,Dhiidjr
170,So clear. Perfect. Thanks! üëåüèº
171,Python Deep Learning brought me here.
172,"14:52 - shouldnt the index of b-components end at ""k"" instead of ""n""? please correct me if I am wrong (been a long time since I did LA)."
173,"At 15:03, in the compact matrix equation, the last index of b should be k instead of n. Right?"
174,14:57 shouldn't the footnote of b (bias) from 0 to k instead of from 0 to n?
175,What would I need to learn to better understand and code things like this? Does a computer science degree or math degree would help?
176,Crazy shit man. I got into electronics just as transistors were maturing and became reliable. To stop and think about how many are involved in performing all these mindlike activities... ü§Ø
177,"This is flabbergasting! 
The most limpid course I've seen on this topic, and the layout is exceptionally beautiful. 
Congrats and many thanks!"
178,"I'm not sure, since I'm not color blind, but I suspect the green and red of similar alpha value is indistinguishable. Otherwise, I LOVE your videos."
179,"15:06 the dimensions of b is not correct. b should be [b0, b1, ..., bk] with (k+1) dimensions, not n+1."
180,wow....  great explanation
181,"amzing video i have ever seen,as a programer,Think you a lot"
182,Truly an amazing channel!
183,God she's smart and hot who is this God you speak of
184,"9:40 I have a question here: For example, if that is a part around the 7, this part is a certain amount of pixels in the square. It looks like you always have to write the numbers in the middle so that the network recognizes this. Is it possible that you write a 7 on the far right or on the left or on the top or bottom and the network recognizes that anyway?"
185,What if the number is skewed to a different area in the grid or scaled larger ?
186,Thank you for simple explanation;)
187,"Wow, so honored! ""THE FORMULAS OF NONPRIMES REVEALING ALL THE PRIME NUMBERS"" was named one of the best new Arithmetic books by BookAuthority!"
188,I'm a designer and idk what I'm doing here üò≠
189,god is real
190,amazing
191,Can't believe how well explained and intuitive this is. I aspire to become a teacher like you.
192,I lost it at 15:39 . I love that way of teaching. Like the teacher simulating he's realizing something at the same time the student does.
193,This is such a good video
194,Your videos are extremely useful! Thank you. I have a request if you can create a video for an understanding of the Particle Swarm Optimization algorithm.
195,I can recognize that that's a 3 cuz that's the way i write it
196,11:24 Two Minute Papers reference!! Dr Carol John Ayy Farhere!
197,Shouldn't the bias matrix be have k values? Since n represents the neurons in the first layers and k represents the neurons in the second layers and bias terms each correspond a neuron in the second layer?
198,Subscribed! what a genius guy! Explained like a pro
199,"At 14:37 , shouldn't the length of the bias vector be k?"
200,I want to run simulations like this 3blue1brown. Any help on how to?
201,No salen los subtitulos al espa√±ol
202,"you, and your work, is amazing. Period."
203,Piano song at the end? Nice vid
204,This video was AMAZING!
205,"The gabby alarm evidently remember because destruction provisionally concentrate anenst a cynical jaw. knowing, damp supermarket"
206,I have learned a lot from this video.
207,"I am just astounded. I spent so much time trying to understand this concept. Everywhere I looked people would show the similar neural network animation, but no one ever really explained and exemplified every single step, layer, term and mathematics behind it. 

The video is really well structured and with amazing animations. Extremely well done. My mind is so blown I can barely write this comment."
208,"i was sent here by googling what learn meant, and now i‚Äôm oddly intrigued and i‚Äôm scaredüòê‚úã"
209,"For humans feedback loops are crucial for image recognition. It is the neuron circuitry that matters in humans.
AI ""recognize"" things with convolutional neural networks. Which match patterns among layers and layers. This is probably a convolutional neural network."
210,DEEP Sorting. LEARNING implies Understanding. Ais don't understand that it is the Context that gives a behavior meaning and impact.
211,"If anyone's browsing comments still, I have a quick question on this video (specifically, around 12:30). How do we get that there are 784 X 16 weights in the first 2 layers? I got the example for one neuron, that each of the 784 neurons in the first layer have a weight and maybe a bias associated with them, and the weighted sum (and a bias) are inserted into a sigmoid function, whose output becomes the activation for that particular neuron in the 2nd layer. 

So does each neuron in the first layer have 16 different weights (one for each neuron in the 2nd layer)? This is the only way I'm able to think that there are 784 times 16 weights in the 1st layer."
212,will you make a whole course for deep learning
213,"Why can't we use sin^2(x) as our function it plots every input in [0,1] range. I'm a high school student plz explain?"
214,you are awesome dude
215,0:04 I was playing minesweeper and only just turned on the volume. I uncovered a 3 and heard this.
216,"The gray greasy great menu embryologically snore because peony concurrently preserve beyond a hoc beauty. heavenly heavy hellish, agreeable tune"
217,why am i here
218,Kinda strange that the ReLU looks like the transfer function for a diode in electronics. Huh
219,YOU SIR saved me.
220,samaj m nhi aaya lekin sunke acha laga
221,Me rewatching the same four seconds the seventeenth time: mhmm interesting
222,"The divergent soldier unquestionably afford because mice preauricularly attach mid a animated surfboard. spectacular, resolute sister"
223,a very good explanation
224,Thank you so much. Very nice and impressive.
225,Today I learned the word squishification.
226,Every bitcoin investor right now is just smiling at the price of bitcoin as it held strong and indeed valuable enough to generating good ROI. Do you know more persons are gonna become millionaires and we have bitcoin thanks for that.
227,"The useful nut predominantly scorch because kilogram macropharmacologically dare despite a lewd football. neighborly, lush way"
228,Awesome but complicated
229,"3Blue1Brown
What is the meaning of this word tool?
Is it a principle that you like?"
230,Hi! I like the video but I¬¥ve a question: Do I must have a specified width and height of the image for image recognition like in this video?
231,Now I can ACTUALLY visualise in my mind my model going over any image to classify it. THANKS A BILLION!!
232,cool!
233,It's like a multiple filter layers.
234,"This video was a crucial part of a series of events that led me into choosing a post-graduation into machine learning.
Thanks, 3Blue1Brown! ‚ù§"
235,Hmmm ü§î sigmoid squishification function.... Nice.
236,"Thanks for making this, liked and subscribed!"
237,I love this video. THANK YOU!!
238,"How the video is well designed, i m impressed"
239,what software was used to create the animations?
240,"at 11:10 why does the bias makes it inactive? isn't sigmoid always positive? ( never zero )

Also, next question. Are the first hidden layer edges needs to be set its weights on a specific length-width in the pixel field? In other words, one would have to write a  program such that it detects all possible horizontal length combinations in field each with their unique widths? Is that it? That doesn't sound right . Note: width=vertical span, length=horizontal span"
241,Me as Russian : recognize 3 as –∑ üóø
242,I love linear algebra üòç
243,This channel is the perfect example of quality over quantity
244,"time 14:45; W_(k x 1) . a_(n x 1) = Wa_(k x 1), should b vector then be k x 1 instead of n x 1? this also seems inconsistent with the number of nodes in the second layer (assuming it's k)..?"
245,"What I feel like is, sgmoid function can have more information that ReLU but ReLU seems to work better as it's faster to train in a bigger network, so that we can always choose bigger network and train with ReLU instead of training with Sigmoid and having a lower network."
246,"Please let me take a note here, thank you.

1.‰∏äÂ±§Â¶Ç‰ΩïÂΩ±Èüø‰∏ãÂ±§
2.trainÁÇ∫‰ΩïÁôºÊèÆ‰ΩúÁî®
3.ÁÇ∫‰ΩïÊàëÂÄëË¶∫ÂæóÈÄôÊ®£ÂêàÁêÜ:layerËÉΩÊúâÊïàÁöÑÈÅã‰Ωú
4.ÊàëÂÄëÂ∏åÊúõhidden layerÊÄéÊ®£ÈÅã‰Ωú"
247,You are the best teacher I have come across so far. Wish we can have teachers like you in school and universities. Complex life would become much simpler.
248,Cannot express how happy I am to finally see a good explanation of this that fills in the details instead of handwaving about neurons and training weights and such.  Excellent!
249,@3Blue1Brown i was happy to see your video linked as a college learning material! As non-compulsory curriculum ofc
250,"Hey, people..you follow the trend to end up as a machine-controlled human beings. Still excited developing reality that is going to control you? Slaves!"
251,Mmmmmm. Well.. Let's see
252,"dont watch guys ,i cant recognize my own handwriting now ,thanks to this guys expalining it i am Mesmerized"
253,The vector [b] should be a k \times 1 matrix rather than (n+1)\times 1 matrix ??
254,"Yeah but no are really just passing numbers back abs forth until the right number is found...

But when you don't know what the correct value should be how do you use the NN?  I think the NN needs to be reactionary taking in data... finding best reaction...while then adding extra organrlls to NN like  core, muscle memory and main memory, attach to each node...but that would mean each neuron has a different analysis function..."
255,bro hats off to your smooth explanation
256,"In the time at 10:32, it's mentioned sigmoid is used to transfer the weighted sum into a number that has a range of 0 to 1. However, at the end of the video, ReLU is used which is max(0, a). By using ReLU, will tbe program run into problem of having a number outside of the range 0 to 1?"
257,"May I know is it a typo at 14:42? The column vector for bias should go from b0 to bk, rather than b0 to bn. As there are a total of k neurons in the 2nd layer, rather than n neurons in the 2nd layer?"
258,Cool really helped me learn
259,Absolutely the way to train someone! Good Job
260,if i have 5000 training examples of handwriten digits of 20 by 20 pixels each image of 20 by 20 gives a matrix that i need to convert (unrolled) into 400 dimensions vector and each image that is of 400 pixels each single pixel in my 400 pixels image will  have one of its own input out of 400 inputs neuron  in first layer . so  when 5000 images will go through the input layer it will still have 400 inputs node in the first layer  but every every digit from 5000 will go through the input layer separately
261,"This is the best explanation I‚Äôve seen on the subject.

Excellent. Subscribed."
262,First time someone actually convinces me to subscribe to the channel through requesting in the video
263,absolutely nailed it. professors having years of experience and PhDs can't explain this. I watched my professors video at least 3 times but still can't understand it
264,Awesome explanation üëè
265,SUDOKU
266,super comprehensive
267,"If anyone trying to understand how the bias plays its role. It is a behaves the same way as a ""shifting function"" ."
268,This video is incredible!
269,NOW I CAN CRACK THE CAPTCA CODE thanks for sharing this info
270,"Hey, i would like to cite You in my master thesis. How should i do it?"
271,"Thanks
For Hand written notes for this lecture series visit scwripple website and go to hand written notes category."
272,SUDOKU
273,"Hey pal, did you get a load of the nerd?  ü§ì"
274,Thank u for clearing the concepts. Every second of ur lectures is precious. Stay blessed. ( from pakistan)
275,i FIND HIS THE BEST EXPLANATION  ON  YOUTUBE BUT STIL  STUPID EXPLANATION  .... at some points contradicts itself
276,Thnx man.. it was a very easy and simple explanation I understood neural networks right away!!
277,"Dear Grant,
I worked out y=x^x (x to the power of x) and found for x=0, y approaches 1. Also for x=1, y will be 1. For x=1/e=~0.3678, y will have its minimum which will be 0.6922. beyond x=1, y increases continuously until +infinity. Here's my problem and I need your help, perhaps a video: 
for x= -infinity, y=0. For all even negative integers, y>0. For all odd negative integers, y<0. This means that for some value between an odd and even negative integers, y should cross the x-axis. But, what happens for real values between negative odd and even integers? How do we reconcile, for example, y for x=-2.4.
It will be y=(-2.4)^(-2.4)=0.122 and y=(-2.2)^(-2.2)=-0.176. so there should be a y=0 in the domain  -2.4<x< -2.2. So, I put x= -2.3.
y= (-2.3)^(-2.3) = ???  My Calculator says out of range! Why? Is  (-2.3)^(-2.3) =1/{(-2.3)^(2.3)}=
1/{(-2.3)^(23/10)}=1/{10th root of (-2.3)^23}. But (-2.3)^23 is a negative number so it's 10th root will be imaginary. But why can't we use (-2.3)^(-2.3)=(-2.3)^(-46/20)=
1/{(-2.3)^(46/20)}=
1/{20th root of (-2.3)^(46)}.
But (-2.3)^(46) >0 so it's 20th root will be real not imaginary!!!
How does y=x^x behave for x=non-integer negative values?

Behnam Ashjari"
278,Am I the only one who thinks that actually you need 11 outputs 0-9 for recognised numbers and 10 to report erroneus input. If you look the next video you can see that without teaching what is not a number one can get false positive answers.
279,legends say that every second is important in this video
280,"So I came up with this analogy today. I'm no math genius or scientist. I'm not even sure if this is right but I think it might be:

An artist paints a highly abstract painting of his own dog. Then he takes his approximation to his friends house and shows him the abstract painting... His neighbour shouts out ""wow you painted my dog!"" the artist exclaims ""no it's my dog!"".

So the artist and the neighbour are the neural nets outputs both looking at their dogs and comparing them to the image and realising its a dog. The data in the net is the painting and the inputs are the actual dogs themselves.  It's the randomisation and abstraction of the original input that allows it to be inpreted in different ways and compared with multiple things.  The uncertainty of what the image is, is what is creating some sense of certainty.  

Am I way off with this? It's quite a heavy topic üòÖ"
281,"Making a list of appropriate software , hardware , and laboratory resources that are most appropriate to supplement quantum  human medical science in the environment of a vehicle capable of quantum networking and navigation about the solar system/universe"
282,Fourier Vectors for neural networks
283,Thank you very much for this video! It was really well visualized :)
284,"For 15:00, should the last matrix for the bias be b_k, rather than b_n? Cause it looks like there are n+1 neurons in layer 0, and k+1 neurons in layer 1, and the bias is for the neurons in layer 1, so there should be k biases, right?"
285,Thank you..
286,Thanks for the translation into Arabic
287,"you are generous god, thanks a lot"
288,@7:07 it would've been nicer if you'd used 187. just saying.
289,What are the future lottery numbers?
290,This is what schools should be teaching.
291,Awesome. This is exactly how we might just create our destruction. The nukes at least still needed someone to press the button.
292,Best explanation eveeerrr
293,"I think my neurons are all sigmoid, whereas those genius kids got ReLU brains."
294,"At 15:00, I believe the dimension of bias should be k by 1 instead of n by 1."
295,"Amazing vid! Question at 14:46, shouldn‚Äòt the bias vector be of dimension k?"
296,Maybe I'm missing something but shouldn't the bias vector at 19:13 go from b_0 to b_k? The result of multiplying a the weights (which are k x n) by the input vector (n x 1) will be a (k x 1) column vector - or are we adding bias prior to multiplication?
297,"sir, I need diabetic retinopathy using cnn project explanation.... can you please help me... thanking you sir"
298,Fantastic
299,Incredible. As always
300,I hadnot understod anything its not for human
301,sigmoid squeshification :D:D @11:16
302,Any fellow AI devs in 2021 watching this?
303,If u want A Human Neuron Experiment Go Check Vsause lol
304,"Someone used this video series to make a neural network in SCRATCH. Yeah, Scratch the block-based kid programming language. His name is nishpish, you can search him up there."
305,Absolutely the best explanation of this common visualization and how neural networks work.
306,"Currently doing my capstone on deep learning and this is among the best, and easiest to understand descriptions I have seen."
307,"I love how smooth the sine waves you use for the animation movement are, (when the pi's move their eyes from one side to the other the acceleration is determined by a sine function if you didn't know)"
308,"Electric engineers: ramp
Computer scientists: *_Ôº≤Ôº•Ôº£Ôº¥Ôº©Ôº¶Ôº©Ôº•Ôº§„ÄÄÔº¨Ôº©ÔºÆÔº•Ôº°Ôº≤„ÄÄÔºµÔºÆÔº©Ôº¥_*"
309,"If the activation range of the input layer is 0 to 1.0 where zero is black and 1 is white, how does using a negative weight for the pixels just outside of an edge increase the activation of the edge-detecting neuron in the second layer? If the activation in the input layer can be a negative value then I can see how using a negative weight would work to increase the activation in the edge-detecting second layer neuron (negative * negative = positive). Is there an error in your video or am I missing something? Love this series."
310,But this work with chinese idiograms?
311,No one can make it more simpler than this.
312,help why isnt my brain working
313,"Many of Us have a Good school
Many of Us have a Good education
Many of Us have a good lecturer

----------------------------------------------------

But just few of us have a (Great) of those things
But When I saw this video. Everything of (Great) come with 19 minutes.

God bless you and who ever in charge to evolve this chanel.
Thank you from Indonesia"
314,"This is fantastically well explained, thanks so much!"
315,"Oh, where are we going to be in a decade when neural networks are applied to defense, economics, science of all kinds, and manufacturing?   I would like to upload my brain as a neural network, please."
316,Am I the only person who took more than an hour to watch and understand this...? lol
317,TYSM YOU SIMPLY EXPLAINED IT
318,I need a Math's Major girlfriend
319,"I think the pandemic has taught people a big lesson, having one stream of income is not really a good idea cause your job doesn't secure your financial needs. The pandemic has really set out business-minded people from the rest that is why I'm so lucky to be among the investors trading with Mrs. Kamilah Thurston as his student it's been success and happiness since the beginning of my trades"
320,"8 seconds in, well.... it's a/the representation of an English Three of what humans once created of something that we had discovered.. but okay, I'll carry on watching >,"
321,"Man, I can't thank you enough for this video really helpful."
322,"I‚Äôm new to neural networks but there seems to be a form of over complication. If we consider that very loosely neural networks represent a human brain and the learning process is back propagation aren‚Äôt we  over complicating it some what? As when we teach a child how to recognise numbers we don‚Äôt show the child one thousand number nines, we teach it there is a circle with a vertical line to the right of it. I suppose my point is, shouldn‚Äôt we add some basic rule set to a specific pattern recognition thus if we have a circle with a vertical line to the right of it could be a nine. By using a neural network to identify specific patterns then apply these rule sets, theoretically it would be less complex and more accurate? Excuse my ignorance if it‚Äôs a silly question, but I‚Äôm new to the field but HIGHLY interested. By the way your explanations are a sheer masterpiece, you turn such a complex subject into simple digestible bytes that everyone can understand.  Love your work and please give us more.üòÅüòÅüòÅüëäüëäüëä"
323,Piano Transcription app for Android  uses Magenta
324,this was beautiful
325,Beast
326,pog
327,"According to the video at various points where the intersections of the neural network are crossed, if not all the intersections is where chaos is produced. As long as the neural network has an end, chaos can be even fun, also if the neural network connections have no common end between them again chaos also fun but they are not the expected results, another connection method being preferable without forgetting the different databases, enough is enough libraries now have another atypical system we are closed. Downloading any book, document by different means is really complicated in the ""net""!"
328,i like how we ourselves don‚Äôt understand how neural networks do their thing
329,"The creepy spain interestedly admire because cheese molecularly prepare including a old minibus. slimy, medical beetle"
330,"The trite ash aetiologically slow because mint socially crash across a orange michael. alleged, disgusted fired"
331,"I remember when he uploaded this and I thought... ""Ah, I think I'll skip this one for once.""
Would ya look at that, looks like I need it now. May as well watch the whole series."
332,A bookmark: 10:46
333,omg this is the first YT Video which I did not skip in betweenüñ§
334,"The four frail buffer observationally wrap because mass angiographically tip below a plain ptarmigan. thinkable, narrow cycle"
335,A stunning beautiful video that truly simplifies a really complicated problem.
336,niggas taught me using calculus instead of this
337,17:00 When you are just skimming your eyes through the patrons names and suddenly you see Ryan Dahl the creator of Node.js
338,You are teaching but I don't know what u r teaching bcoz of Ur pharatedar englis.....
339,28x28 is pretty high resolution for a single character....
340,Simple and effective explanation... thank you!
341,One could easily use some python and dsp designs to apply this to ekg's
342,"The abstracted moon aesthetically trip because sailor scilly smash minus a  fascinated beard. null, animated rowboat"
343,awesome!! just started learning and it went straight into the mind via neurons. Thank you so much for your work.
344,Can someone explain to me why we need 'negative' weights for pixels surrounding pixels of interest (example at 10:03) in the first place?  Wouldn't having 'zero' weights (as described in earlier bits of the explanation) be enough from the perspective of creating/interpreting distinction?
345,Who tf dislikes this vid?
346,"Can we get a comprehensive series for neural networks, and deep learning"
347,That's a great explanation! Thank you so much!
348,This is fantastic! Thank you so much!
349,"This is a great video! Thank you, very well delivered."
350,"What would happen if you mirror the network, using the numbers as inputs and the image as output?"
351,"You are without any doubt the best, thks from Belgium ! Could you make an intuitive video about Convolution Neural Networks and the way they process information?"
352,Squish that function.
353,reminds me of 'The immitation Game' movie
354,"At 4:17 your voice is acting weird. Just a question; Is this just the microphone or something or is this a very well synthesized computer voice?

Edit : No offence and all, but I am a computer nerd......"
355,excellent!
356,So it's not like Clifford the neural network form gta
357,You are a master educator. I learned a lot. Keep up the great work!
358,"The maddening motorboat booly explode because timer noteworthily fence by a scattered yoke. lively, quixotic hemp"
359,Excellent explaination
360,"Explanation is at par, but the digital content is commendable"
361,"Wow. If someone told me machine learning is  just an array of algorithms, I wouldn't be very impressed be big words like AI and neural networks."
362,0 = off and 1 = on. These are the two absolute extremes. And variable shades of gray in between are all variable shades of ‚Äúkinda on‚Äù and ‚Äúkinda off.‚Äù These variances are the relative degrees in between.
363,"Wow amazing thanks 
how I get the full text"
364,"The deafening veil visually paddle because almanac comprehensively point off a helpless riverbed. ethereal, six oatmeal"
365,"its because the scaling is done perfectly , full dark mode"
366,This sounds like the most advanced camera ever created.
367,yo i just realized doesnt this mean were just robots? but also we naturally developed through evolution from simple robots to become more complicated ones?? theres no way this isnt a simulation but also the world outside of the simulation mustve come from something aswell.... Wtf dude i think when math gives me an existential crisis it might be time to stop watching
368,Please video about convolutional neural networks
369,Thank you :)
370,‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù£Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏èüíñüíñüíñüíñüíñüíñ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è
371,I'm still waiting for episodes about different types of NN
372,"3Blue1Brown: ""This is a three""
Engineers: ""Which one?"""
373,"At first I thought there was a typo in the code at 15:15, and then I was like, oh shit, this is genius"
374,"8:18 r e c o g n i t i o n --> re cognition
omg how have I never seen this"
375,Wow!!
376,So is the structure of a neural network essentially a large hashing function?
377,Great job with the explanation in this video! 9M views is really challenging to obtain for such a technical topic. Well done!
378,11
379,how is your voice so perfect compared to anyone else who speaks? Everyone sounds like shit next to you
380,"This is a superb expounder on things mathematical. He introduces Multivariable Calculus on Khan Academy as well, (where he achieves the remarkable feat of introducing every single video with the word ""So""- but I cavil)."
381,Wonderfully presented! Thank you very much :)
382,"If you need money buy BTC.if you cant buy start mining, BTC 100.000$ in 2021.Ask me how to mine BTC."
383,"I'm sorry, what?"
384,So i suck at math while my brain does matrix and adjebra just so i can understand my  math tacher's ugly handwriting!! Brain! this is not fair!!
385,Wonderful video :O
386,squishification function
387,omg! You are so good and amazing and so kind to make such amazing lecture materials!
388,You know what's better than a neural network? Just take a child and then make him do all the work for free
389,I am new to this topic and I always wondered if it is necessary to have 10 output nodes. Couldnt you have 4 output nodes and interpret them in binary and train the network to output the number in binary as well? I dont know if this would make anything simpler or more complicated I was just wondering if that is in principle possible. Would be useful for something with a lot of output nodes I would imagine.
390,it's my 5th time watching this video. Let's hope I understand it this time.
391,Finally learned something useful about NN. Great work. The quality is unbelievable. So understandable.
392,Thank you so much üôèüèª
393,Thank you 3B1B. I have watched all 4 lectures and re-watched them backward practicing what I learned in back propagation. Hey if it works for neural network it should work for me too.
394,"very good and clearly explained video with amazing graphics, thanks a lot! link for video was provided by professor to understand neural networks"
395,arigato
396,I don‚Äòt really understand why the weights are just random numbers. Doesn‚Äòt it make a difference for the weighted sum whether the weight is e.g. 1.76 or 3.41?  What determines the numbers of each weights? Or is it just important if the weight is positive or negative?
397,Great explanation.  Cheers.
398,"A small beef - neural networks aren't inspired by the brain; they're in the brain.
The ones in a computer are more ""neuron-y"" like that ""chocolatey"" cereal or ""cheesey"" crackers that don't actually have chocolate or cheese."
399,Easily the most beautifully and clearly presented video on computer science I've seen. Amazing.
400,ak 47 tec 9
401,I actually made a small version of this in MineCraft years ago on a 5 by 5 grid. You draw a number 0-9 onto a 5x5 grid and it guesses the number you drew. Worked pretty good and was a lot smaller than any MC calculator.
402,"self in python
in java,c,c++,c#,etc., we call it the this pointer



is the adress of whatever what object to be manipulated by the function"
403,"nruh, you didnt give any kind of example."
404,Thank you for this!
405,"You should paste the chapter 2 ,3 4 or related to the topic link in the bio ..just a suggetsion!"
406,Tqs !! ü§©
407,Every second of this video is a Pre-requisite to the next second of the video :D
408,Remarkably good explanation!
409,squisification
410,What should I learn to dive into these topics
411,"Why we like to make the things more complicated? The sigmoid can be rewritten as œÉ(x) = e^x / e^x + 1. 
But why we choose (e) and not (2) as base, that's a lot easer for binary computing?"
412,14:40 the bias vector should be k by 1?
413,Thank You !!!
414,I Love this Channel !!!
415,What an absolute legend. Thank you for explaining this!!!
416,"This is the only time that somehow I understand how neural network works. Thanks for speaking human @3Blue1Brown. They say AI is not automation (computer programming). So ""Artificial Intelligence"" is basically just automation as well but using advanced programming, mathematics and complex algorithm. Then I can say it's creator, the human with organic brain is much much more advanced. hehe"
417,"A lot of this spent on videography 
Apriceat your work sir"
418,So satisfying to see these animations. I'd love to put animations like this into my presentations at uni‚Ä¶ but let's first get me through my Neural Networks exam :D
419,"The thin piccolo psychologically stitch because slash contrarily trap amid a abject pot. silent, clever cheek"
420,"3:37 I love this part so much, i didnt understand neural network layers at all before this that one little visual helped a lot"
421,Epic ...
422,late to the party.
423,how do i like more than once?
424,"Yet another fine video by 3B1B - much appreciated!
 Btw I had a small query regarding the ReLU function mentioned at the end - if ReLU = max(0,a) then how do we confine it to the range [0,1] for any neural cell to accept the value? I mean, we started out with the sigmoid function for the very reason that it ""squished"" the number line into the interval [0,1] so how is ReLU being used if it doesn't even serve the basic purpose it was meant to serve?"
425,Superb!
426,But ReLU() doesn't normalize to 0..1 as sigmoid does. Its max value in the first inner layer is the number of inputs (784.0). With ReLU are the inputs treated as int or float by the next layer? You can see why specialized chips and numerical representations matter so much in AI implementations.
427,"The chemical gas chemically play because moustache endoscopically intend following a chilly bengal. warm, acoustic cast"
428,A neural network to understand doctor letters.
429,is there a way to feed raw footage of videos to the neural network that they can replicate in a game?
430,Genuinely awesome. Thank you 3blue1brown!
431,"What do you think about R ? Worse than python (especially concerning deep learning, bc good libraries also exist : keras, tensorflow, caret ...) Any advice please ?"
432,"The innocent street biophysically grease because hawk cumulatively soothe into a nice keyboarding. , icky dish"
433,"Some believe that artificial intelligence are thinking, but is not so. It is just following some mathematics, computer codes and logic rules."
434,"The nueral network in my mind lit up and shouted ""linear algebra"" when you said ""let me show you a compact notation."" At this point, there is no escaping Linear algebra, it's everywhere"
435,"I am watching how programmer solving task of recognizing letters, that  was solved more than two decades ago. Again, jus new terms to well written programs long ago."
436,"c·∫£m ∆°n v√† xin l·ªói b·∫°n n√†o d·ªãch ti·∫øng Vi·ªát nha
qua 3 l·∫ßn xem l·∫°i kh√∫c chuy·ªÉn qua d√πng kh·ªëi l∆∞·ª£ng m√¨nh c≈©ng ch∆∞a th√¥ng ƒëc T..T"
437,CODE
438,This is A Super Educated Video And Will Inspire More And More People :D (btw I edited This 2 times :) Ok!)
439,I can't believe he didn't animate all 12960 connections. How rude - he only animated 672!
440,"I am wanting to learn how to present information in this type of format (i.e., graphical animation style). I am very unexperienced when it comes to graphic design and video editing but I want learn ways to present technical information in a more visual and intuitive way. It would be greatly appreciated if anyone could point me in the direction of resources related to how one would go about creating this style of video. Thanks!"
441,Thank you so much..
442,"how can you regconise the number 3? 
Well, my teachers are sucks at writing it correctly"
443,This is too good!
444,√âpico
445,no you get destroyed marcos like what the hecke
446,I think at 14:51 the bias vector must be from R^k because the product of the weight matrix and the activation vector is a vector of dimension kx1. Really well explained! Thanks!
447,Are there only 4 episodes?
448,"Hi, Can I know what is the tools or applications you used to make these wondeful slides. I need to do luctures to my students in the same way. Many thanks in advance"
449,The main purpose of the sigmoid is not to squash the values between a certain interval 0 - 1 as has been said here. Its main purpose is to introduce non linearity into the network.
450,Will this work against recapture? and identifying car images?
451,"Hey, you there. Have you read the corresponding book 3blue1brown linked below this video? I watched the series 2 years ago, and I am unsure if I should watch the video series again before reading the book or after. 

Thanks, have a nice day."
452,I will forever be grateful to you for making learning so much fun!
453,"This is gold, thank you so much!"
454,Everything in the human brain is incredible and very very detailly designed. It cannot be...
455,May we have this video?
456,It felt like I was watching an anime didn't even feel like I was putting in an effort and I learnt so much! Thanks Grant!
457,Great video!üíì
458,The last time I had the feeling I got watching this video was when I first understood derivatives in my fresh year at university. Thank you for this.
459,this is awesome!
460,"can it ""know"" if its not a nuber"
461,"Excellent,thanks."
462,"""Before plugging it through the sigmoid squishification function"""
463,My head....
464,Very well explained !
465,"impressive video, extremely well explained: I compiled (but not written by myself) a ""perceptron"" back in early '90s, on a 386 (it took all the night long, to train on my 8x6 patterns :-D), nonetheless it's always a big questionmark to know how much hidden layers and how much neurons for heach of them. :-)
I recently learned why, nonetheless your video is very clear and informative, my compliments (and thanks)."
466,And why exactly are we not using algebraic topology/homology to recognize loops? Should jump right out at you!
467,why does jake paul hav more subs than this guy
468,sooo we're finally done with the captcha era???
469,ÊÑüÊÅ©upÔºÅËÆ≤Ëß£ÂæóÈùûÂ∏∏Ê∏ÖÊô∞ÈÄö‰øóÊòìÊáÇÔºåÁÆÄÁõ¥ÊãØÊïëÊàëÂ∞èÁªÑ‰Ωú‰∏öÔºÅÂêåÊó∂Ë°®ÁôΩ‰∏≠ÊñáÂ≠óÂπïËèåÁà±ÊÇ®ÁëûÊñØÊãú‚ô•ÔºÅ
470,can you make a video on explaining the proof of universal approximation or cybenko theorem and explaining why increasing no of neuron increases function approximation precision?
471,Voice copied in video
472,My new favourite word is *squishification*
473,ÈòøÔΩû ÁúãÂÆåÊùéÂºòÊØÖËÄÅÂ∏´ÁöÑÂΩ±Áâá‰πãÂæåÂõû‰æÜËÅΩÈÄôÈÉ®ÂΩ±ÁâáÁúüÁöÑÊòØÊÅçÁÑ∂Â§ßÊÇüÔºåÂÖ®ÈÉΩÊáÇ‰∫Ü
474,Its atrocious that this is free.
475,"How  you make everything so easy, I would love to see video playlist on ways for smart study, what approach you use for learning and how you connect even the difficult subject to your visual mind"
476,"No YouTubee but Grant would write a double-space after a period in their descriptions and comments.  Like so.  Which is cool, in an anachronistic sort of way."
477,Great explanation!
478,"a masterpiece of teaching, thanks"
479,The bias can be completely avoided by adding another number-holding part to each layer except the output and adding a constant as it's input
480,WOW... just WOW
481,"so, if we would change or somehow delete the information in one of the layers, we can misslead or upgrade the outcome of AI ?"
482,Yang milih allah like yg Mili dajal hirawkan 1:39.........m.
483,I feel blessed after watching this.  Thank you from bottom of my heart üôè
484,"i think conceptual information is lost in the process of regression, as long as the fit improves, there is no meaning on each piece..."
485,"""This, is a 3"". 
+1"
486,"how come the 3 is 28 pixels? 
PS: Oh, I just realized pixels have no size, it's just any 28x28 elements, and a color value. 
For the record"
487,you are awesome
488,"Awesome,perfect,very god,,,thank you"
489,Hello very good job ! well done. Can you tell me which tool do you use to see dynamic animation of the nn from 16'13 to 16'19 ?
490,"First of all...absolutely amazing videos...you totally made this complex thing to comprehend in a simple manner...

I loved the way you said...""Subscribe... so that neural networks that underlie YouTube's recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you"""
491,"at first, I want to thank you for all the hard work and I'm absolutely a huge fun of your self and greatly appreciate everything you've done to inspire people to get into math. The teachin method you shown is the best I've ever seen and I'm benefited by that at a very deep level. 

second, this is my 2nd time watch this series. I think I noticed a possible error here. at 14:41, the bias vector has dimension nx1, this seemed to be wrong. the weighted matrix is kxn and the activation vector at layer 0 is nx1 , the matrix multiplication results in kx1 vector; and each bias is added to each neuron at the activation layer 1 before it gets sigmoid squished, so by rules of matrix multiplication and also logic sense, the dimension of the bias vector should be kx1 instead of nx1."
492,Hello where can I find a simulator showing propagation and back propagation to understand weights adjustment. Are your animations of the nn from a simulation tool?
493,"I've set the notification to ""All"" in an attempt to heavily weight the YouTube algorithm."
494,"Very nice Video and Explanation, but I have one more Question:
Why do you need Activation Functions at all?"
495,I still don't get what the bias is for.
496,My question is why hasn't any one tried creating a neural network specifically designed to tweak another neural networks weight and bias till its a functioning neural network? Make a neural network that fixes neural networks.
497,Best thumbnail
498,Hours of scouring the internet to figure out how these things work and you explained it better than anyone I've ever heard in 20 minutes.  Thank you so much!!
499,"How are the weights assigned? Don't just say ""it's just a number"". Are they random or what?"
500,"Teacher: tell me the trigonometric formulas you know 
Me: üñï"
501,"I have a question, there a ‚Äúeasy way‚Äù to explain why matrix multiplication (or operations) are good for use with neural networks? A good intuitive explanation."
502,Elon smelly Musk
503,One note of minor correction: the maximum index of the bias vector shown at TS 15:06 should be 'k' rather than 'n' since the number of biases must be equal to the number of  neurons in the second layer. Great video and extremely clear explanation though. Thank you!
504,The best explanation of logic neural networks basis...thanks
505,excellent~!!!
506,"Amazing explanation, thank you a lot for the great video!"
507,at 14:38 shouldn't the bias vector be counting of 'k' and not 'n'? since it's not the same number as weights...
508,All neural networks can be summed up in one equation: y = ax + b where x (neural inputs) can be real large. The whole process is trying to figure out the best approximate a and b.
509,"awesome job, the punch line is the network IS a function !"
510,By far the best explanation I've ever seen on this particular topic. Thanks‚ù§
511,You're the first person to explain bias in an intuitive manner. Thank you.
512,king still really inspiring entire generations of programmers 3 years later
513,Wrong. Digits are gleaned by comparison to historic pattern recognition in memory. That's it - a simple task from a vastly complicated process that is inaccessible to computational models.
514,Now i am number 9 fighting against a huge sigmoid squishification i found in my toilet
515,1:02c.
516,Thanks for that tutorial. Machine learning is no (not quite) longer a mystery.
517,Moon Ga Young
518,This is great thank you!
519,"This makes it much easier to understand what all this ""AI hardware"" is about (Apple's Neural Engine, Nvidia's accelerator cores, Google's TPU, etc).
Neural nets need to do enormous amounts of multiplications, and all those fancy marketing names are just processor cores which are really good at quickly handling things like generalized (tensor) matrix multiplication in bulk."
520,I did not get the relu part. Why is it easier with relu?
521,"Assuming I landed on this video after puppy videos, and know nothing about anything, can someone please explain what the hell I'm learning about?"
522,What would be a good prerequisite to this video? I'm missing something...
523,"Thank you, Sir! It's really very kind of you. Making such tutorials needs a lot of determination, hard work, and time. A very nice explanation. Once again thank you so much."
524,Super explanation üëå
525,"Been playing with Neural Networks for months with Brain.js and even doing some serious ML projects, without understanding some of the key concepts. Thanks a lot for this video. Now everything is so clear to me!"
526,"Hey guys go for **Dumpscardingg** on telegram he gat everything you guys are looking for,he is the best vendor I have ever heard about I just got my credit card from him and the balance I requested he is very trustworthy and reliable ‚úÖ‚úÖ‚úÖ‚úÖ"
527,9:25 How come we also have activation in the first/input layer? I thought the input layer only has all those pixel numbers.
528,How does neural network impact in future
529,best explanation that i ve found so far
530,"Such a clear way to show a very complicated idea.  VSAUCE demonstrated this in an episode of Mindfield called the Stillwell Brain.  I recommend anyone interested check it out.  Thanks!

Its amazing what Deep learning is doing.  since this video came out I have seen actors replaced using deep-fake images, saying things they've never said using deep-text to speech, and heard AI written songs.  Its amazing."
531,"As a person who has self-learned a bit of python and is just trying to learn this stuff, this is exactly the best place to begin."
532,Does AI have 'junk DNA' or are they allowed to use there's??? ü§îü§îü§î
533,Yang milih allah like 40000 ya.
534,Outstanding Explanation !!!
535,"Too many ""we'll come back to it in a minute"" üòê"
536,"I don't know why but I don't get anything from this kind of videos. I mean any lecturer with a presentation is obviously better than this video ""for me""."
537,Thx :D
538,"im doing a course in ML, they recommended these videos as side content."
539,i extremely appreciate the really great visuals. makes understanding and seeing whats going on for programmers.
540,Any 12 year old?
541,Using color for weight was an inspired idea.
542,how does this video made?
543,Who is the narrator?
544,'Thank You' are too small words
545,How can a negative weight pull out a high value for an activation =0? Im making reference to minute 10:30 approximately
546,"Wow! A last, someone neural net explained neural nets!"
547,math or cs to become a machine learning engineer?
548,"""Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future."" - I don't think you put much thought into that statement did ya?"
549,"Awesome , Great explanation."
550,"Me: Pleeees brain, remember this
Brain, later in group assignment: There is a squishification function.

Jokes aside, this is a brilliant introduction and the world needs it."
551,"When you love pie ( pie = 3.14)  more than your friend 

Brain : lets put pie in my youtube video"
552,"I doubt AI will be relevant and important for too long, because there's no reason for doubt, that the society as we know it, capitalism, and huge parts of humanity won't exist for much longer. But nice video anyway."
553,is in biological human brain the neuron also following certain kinds of math ???
554,Explained in way like making 2 floor size computer to nail size computer. Great
555,"""More A than I"" has to be the most apt way of describing AI at this time."
556,"Thank You for making such a wonderful video.
I can now visualize what the NN is doing.
Please make more on ML, Deep Learning also."
557,"quick question are the hidden layers = ""the parameters"" created by the algorithm?
I new to machine learning"
558,"Hi, Came here from reference from Dhaval - another genius presenter. Now I know a genius refers to another genius. NN becomes quite simple after watching the videos. Thanks . KM"
559,"These neural networks are like clasic mechanics , but the real thing that happens in real neurons is quantun mechanics . So this will mimic it but never be the same as real neurons , unless some very big quantum computer becomes reality ."
560,This is so beautifully animated and well explained .
561,"Finally, I understand what this machine learning is about. And I have only a masters in literature, no background in math whatsoever. 
My favorite part is the bias, I know it is a somewhat mysterious part of the explanation, but that makes it so much more beautiful to understand that we just have to put some bias into the system to be able to forecast scenarios."
562,"Great videos! Thanks you so much. I find the way you explain by visualisation incredibly useful and also as I see it in my head when I really understand something, but this really speeds up the process!
 Minute 14:39 the ""b"" vector, shouldn't it go from zero to ""k"" rather than ""n""?"
563,you saved my peace of mind and my thesis with your beautiful animations! thanks <3
564,"ÊàëËßâÂæóÊàëÂèØ‰ª•ËÆ¢ÈòÖ‰∏Ä‰∏ãÔºåÂÖ®ÊòØÂπ≤Ë¥ßÂïä„ÄÇ

I think I should subscribe this Ôºåit's helpful forme."
565,Very good.
566,Battery with amoled screen and his videos r highly efficient lol please comment why?
567,Did someone understand why exactly the ReLU works better than the Sigmoid? Wouldn't the Sigmoid also have a threshold value which determines it to be activated or not  which is the same as the ReLU then? Maybe someone can help me out here....
568,Alright I probably understood 10% of what he was talking about...I‚Äôll watch it again eventually hopefully üòÖ
569,Why everyone teach neural net by computervision&image  why dont anyone teach neural net for pattern recognition.
570,"Great explanation, and effort on the animations are excellent, thank you very much 3Blue1Brown!"
571,the best  deep learning intro ever i hope you can complete it  and explain the RNN and CNN and reinforcement learning its make learn the code of DL so ease
572,Beautiful love u.
573,Leesha Lee == Yummy yummy
574,Noice
575,may the  *_sigmoid squishification_*  take over the world
576,13:26
577,"I have fed all these information about ""How we feed image layers to the artificial neurons"" to my bi01ogical neurons that it has started to speak 01.. Check the word ""Biological"""
578,This is incredible! Math and Art combined beautifully. Thank you so much!
579,I'm gonna go build this in minecraft now üòÇüòÇüòì
580,Ïù¥ÎÖÄÏÑù!! ÌïúÍµ≠Ïñ¥ ÏûêÎßâÏù¥Î©∞ ÌïúÍµ≠ Ï†úÎ™©ÏùÄ Ïñ¥ÎñªÍ≤åÎã®Í±∞Ïïº!!
581,"3b1b coming back to this video after a few years. 
~6:30 I know that as trained, this NN will not actually recognize the various components of numbers (and possibly letters) with specific neurons, it'll do something a bit more complicated.

But the thought occurs. One could make a data set of say ""top loops"" ""bottom loops"" ""lines"" etc that correspond to certain regions of the numbers. Train the system up to recognize those, and then add a new output layer after for the 0-9. 

The thing I wonder about. Would this actually increase the performance of the final algorithm, or will it have been a waste of time?"
582,Okay none of my neuron is firing watching this .üò£
583,U are a big legend!
584,So how long until someone tries creating a neural network linked to an android body and raises it like a human child?
585,1 day of linear algebra will take you a long way to understanding this....
586,Would Love a list of math concepts to learn.... to help in understanding neural networks
587,activation function does nonlinear transformation.
588,14:50 should that be... b_k instead of b_n ?
589,Wonderful!
590,Incredible work :)
591,Well i am probably living under a rock
592,ti appreciate your work thank for detailed questioning and exlaining√ß
593,Ïó¨Í∏∞ ÌòπÏãú ÌïúÍµ≠Ïù∏ ÏûàÎÇò.... ÏûàÏúºÎ©¥ ÏÜê!!!
594,that chick has the WORST voice ever.   she sounds like she has herpes of the throat.  ugh
595,Great explanation!
596,thanks for the video.
597,"This video is amazing! Thank you very much!
There is a small error, at minute 14:44 the bias goes from 0 to k not from 0 to n. (matrix multiplied a is: k x n * n x 1... so bias should be a vector of k x 1)"
598,"Very good illustration to explain the Neural network, easy to understand"
599,Tfw the neural network contains more neurons than your brain
600,Supergood. Thank you.
601,Never been so motivated to subscribe
602,I was understanding till it got complicated
603,Isn't it b_k @14:41?
604,Thank you brownblue!
605,3Blue1Brown if i am not mistaken there is a mistake at 16.00 the bias should have indices from 0 to k right not to n?
606,"I want learn machine learning and deep learning. Because cloud service is expensive. I want to use laptop for machine learning and deep learning training . Would you suggest any hardware configuration for basic and long term? 
Such as CPU AMD, I5,I7, 
Ram: 88G,16G,32G, 
GPUÔºö1060,1160,2060,2070,etc
Thanks"
607,Is this person single? Please let me repay you by dedicating my life to you in marriage
608,Wow
609,This google stupid translation of youtube video titles is an insult to the content creators who choose carefully their video title
610,Thanks thanks thanks so much for this really good explanation ! Looking forward to watch your other content too!
611,"If this video would have come up on my feed a month earlier I would have cleared the interview üòî.
God why are you so late
Interviewer- How does the machine recognise Alphanumeric numbers?
Me: There are certain data sets used for their recognition....meh
Interviewer-It's not what I asked for
Now i know he was searching for Convolutional Neural Network as an answerüòî."
612,"I'm done for today, I'll come back tomorrow. btw this is my 2nd time watching this and still couldn't comprehend this"
613,"Would it not be simpler to use the already defined values of the intensity of grey assigned to each of the 784 pixels, the activation itself of the initial 784 neurons....why not simply have the ""hidden layers"" recognize those exact value of intensity patterns, meaning the ratios or patterns of ""lit up"" pixels vs darkened pixels on the grey scale so to speak, which should have similar ratios from in inputs of the similar inputs with varied abstractions, hand drawn numbers in this case. So use the hidden layers to compare known ratios of each numbers grey scale, to the ratio found from the initial input, to determine which number the input is most likely to be. Bypassing the need to search for circles, lines, and varied abstractions of the previous. Wouldn't that be simpler and faster? Or am I missing something, or maybe not understanding some core concept?"
614,00:43 3=3 is actually true
615,"Can't wait until AI simplifies its own thought process and makes it self 100X more powerful overnight...
Once they realise how far we most likely we overengineered this simple.biological process they will kick us to the curb"
616,"We're all doomed, but in the coolest way possible."
617,The Matrix Indices at 15:57 cannot be right.
618,Man you are amazing! Thanks so much!
619,guys this is proven the best faucet to get little bit of free bitcoin you can even get other cryptocurrencies: @t
620,i'm interested to know which software are you using in making these videos
621,Love this channel <3
622,Great presentation
623,"Great videos, I just have some difficulties trying to understand the bias part, what is the meaning of a neuron to be active or activated ?"
624,"I don't know if this was addressed in the latter videos but here's the particular reason why the Sigmoid as an activation function is hard to train: when you get the derivative of the logits there will be points that will make the derivative rather close to zero (which would slow down training) as opposed to ReLU, where you get a constant value‚Äîwhich is easier to train."
625,I love using my brain to learn about the brain.
626,Thanks for sharing!
627,I wonder if the presenter is actually physically significantly larger than the viewers of this video.
628,I guess just put this link in my cover letter? Anyone able to get a job with just this information in the video directly or indirectly?
629,It seems that there is a typo at 14:51. I guess the last element of the b vector should be b_k rather than b_n.
630,My dimness thought the thumbnail was hit circles
631,2011192041
632,Í∞êÏÇ¨Ìï©ÎãàÎã§
633,"Neurons most common bullying phrase:





""You just dont have any value to me!"""
634,Thank you sir
635,"Great video, but maybe, I would recommend to look for the perceptron explanation before seeing this video :), even though, this video contains an amazing explanation."
636,"‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§ So underrated, Best explanations of mathematics (and I know a little about math)"
637,Good job done! Not every one can explain Neural Networks as good as you didüëèüëèüëè
638,Kakaotalk
639,Ï†ïÎã§ÏòÅÏôºÏ™ΩÏÜêÏôºÏ™ΩÎ®∏Î¶¨ÎßåÏßÄÍ∏∞DNA
640,Î™®Î¶¨ÏÖîÏä§Î®∏Î¶¨Ïπ¥ÎùΩ
641,"spouse: ""what did you learn today?""
me: ""squishification functions""
spouse: ""... """
642,"It's really helpful for every person who wants to understand the structure of artificial neural network. Well, I started to learn about programming and your videos help me a lot :) Thanks"
643,"I love the fact you specified that this is not get rich fast scheme. I don't know who needs to hear this ,you've got to stop saving money , invest some part of It,if you really want financial freedom"
644,very good
645,"Why does the second layer have 16 neurons? My own neurons are a bit confused...

And in the vector notation part, k=16 and n =784, right??"
646,my God your channel is awesome
647,Aight im pulling up to Google
648,3 year-old video and there are chapter help. Thank you
649,Thanks for explaining the concept crystal clear!
650,Oh i get the middle layers thing.
651,"There‚Äôs a more symmetric version of ReLU
Max(-1,x)-Max(1,x)+1 is the thing"
652,I am not a robot
653,there's god sitting in between those patterns.
654,Sounds a little like Sudoku.
655,"No words for ur efforts sir.
U just make me enjoy stuff .... thanks a lot"
656,One day ill be able to solve a captcha
657,Thank you so much for the visualizations! I think if math teachers always refer to how it pictorially looks the intuition would be easier to get. This is why you and Dr. Strang from MIT have good content!
658,Brainchip.
659,"Here, have my sub"
660,Do this in c programming
661,How can I learn it good?
662,"Please answer my question:
Who decides what pattern which neuron will try to detect?"
663,"This guy: Uses dark screen to illustrate a long concept so that it's easy on the eyes.
Everyone: ""Carefully, he's a hero."""
664,Amazingly explained! thanks so much.
665,ehh third attempt lets start
666,"Hands down, one of the best channels on youtube!"
667,"Moral of the story: Don't let your AI watch this, probably they can solve  r captcha problems"
668,SENTDEX RECOMMENDED ME THIS  YT CHANNEL
669,I'm missing the music we're used to at the beginning üòî
670,"My first time seeing any of your videos 
And I have to say I'm IMPRESSED. subbed"
671,Hey! Does that mean the game Battleships is basically a primitive fourm of code??
672,Excellent video. Please check  the Course Mentor on the link below as well.
673,Wait i am 10 and trying to understand
674,Absolutely phenomenal video. Really shocked to see information like this be explained so succinctly and clearly. This answered a lot of questions regarding how these networks function. Thank you.
675,"slow down bro, my main language is not English, you use too many new words and talk so fast, so please help peoples like me"
676,"I am having the feeling that in few years most of the machines are going to be more inteligent than me, but if that make me my live easier, is ok !"
677,"Wow. I did my PhD in what was basically image processing and machine decision making (with a bit of machine learning in there), am about as comfortable with linear algebra as any engineer gets, and STILL could not find a ""quick"" explanation of neural networks that actually taught me something. Everything was either entirely too fundamental--basically explaining how cross-correlation can help identify something using one or another Cauchy-Schwarz corollary--or jumped straight into using abstract algebra terms for the system. As an engineer, I want the mathematical meat, but I think in linear algebra terms. Calling a function a ""map"" before I see the matrix equation will lose me.
 
THANK YOU for filling this weird gap in my understanding! I can imagine hand-coding a neural network, now."
678,awesomeÔºÅ
679,"Are you hard coding the fact that you want one of the lines or groups of pixels to be in the top half? Because what if it's a tiny number in the bottom right. 

I hope you aren't hard coding this but you made it sound like you were when you were explaining"
680,"Amazing, sorry for mi english, the IA in the last exercice know I am lost in the last point and surrender."
681,This video is seriously gold!
682,"Holy shit, what an explanation! 10/10"
683,this neural network should be called captcha killer
684,Can this be done in batch
685,‡§Æ‡§∏‡•ç‡§§... ‡§ñ‡•Å‡§™ ‡§õ‡§æ‡§®...
686,"3Blue1Brown
""Sigmoid Squishification Function"": 11:23
Most brilliantly named function I have ever heard named. Absolutely brilliant. The merger of the technical with the simple with a double alliteration for easy memory."
687,2:42
688,"It would be interesting if he had elaborated more how each of the input neurons from the 28x28 grid receive the values between 0 and 1.This man makes us feel like geniuses cause he explains it so well, though I actually have a degree in Stats, but I thought neural networks were more complicated than this."
689,"The bias is then the intercept of the generalized linear model, a linear combination of multiple variables, which goes as the argument of the logistic function to output a value between 0 and 1. Even better, now relu, if it's negative it's 0, else it's the argument itself."
690,"When we watch videos like this with too much details and too much convolutions, our neurons try to grasp just the essential, and summarizes and retains only the essential information, throwing out all the rest as garbage. This is what real neurons do in the learning process."
691,"*Perhaps he coud've said the awful truth, that true AI doesn't really exist nowadays, all we have is algorithms (and many of them dumb as a rock - Hey you, Apple and iPhone!)*"
692,Am I the only one who likes to upvote viral comments?
693,"czesc na moim kanale na you tubie .. boze jehowa nauka .. znajdziecie 
filmik jak spozyskac darmowa energie z magnesu ktury sie robi ze stopu 
germanu i magnezu .ten stop magnezu ma z jednej strony wiekszy 
magnetyzm, z drugiej strony jest mniejszy magnetyzm i jest ruznica 
potencja≈Ç√≥w dzieki tym magnetyzmom i prƒÖd p≈Çynie w uzwojeniu miedzianym 
nawinentym na ten magnes i nas moim kanale znajdziesz wiele innych 
filmik√≥w naukowych jak naprzyk≈Çad jak dzia≈Ça naped do ufo,,,,,,,tak 
pisa≈Çem pewnej wruzce,ty debilu ja poto by zobaczyƒá kto co my≈õli i czuje
 lub jak trafil do piekla i go tam rzrƒÖ mam komputer i na ekranie widze a
 nie mieƒá w sobie innych glosy ja intymny jestem a do tego prawo ma 
tylko chszescijan wiec przez g≈Ço≈õniczek bawie siƒô idjotami i muwie 
Bajkonur ,Bajkonur i nie tylko pierdolniecie maja wruszki i urojone z 
toba prawa fizyczne ,i strad wiem jak cie denerwuje ,i tym wpisem,co 
daje gon  w tej formie codziennie placz ofiaro bo prawa ludskie ma tylko
 chszescijanin,muj komputer jest podlaczony do kwarka pamiƒôci w atomie 
do kwarka duszy,i te dyktowania debilom tez podaje i tez takim co gadam 
mu boze juz sie stales bawie sie niezle z debilami,,,CZE≈öƒÜ NA MOIM 
KANALE ZNAJDZIECIE FILMIK JAK DZIA≈ÅA NAPƒòD DO UFA A JEST TAM TE≈ª FILMIK O
 NAUROLOGI WYNAGRADZANIA CIA≈ÅA DOBRA I KARY CZYLI KTO NA CO I ZA CO 
UMRZE LUB ≈ªY≈Å BEDZIE wiƒôcej znajdziecie na you tubie na kanale boze 
jehowa nauka jarek lichwala,,,,,,,,,lub na boze jehowa tato ,,,lub na 
boze jehowa z≈Çote mysli,......................w reinkarnacji za karme 
macie tylko krzes≈Ça inwalickie tak swiat zrobi≈Çem ja boze a 
chszescijanow w zmartwychwstaniu przywracam do rzycia z calom pamiƒôciom 
do ciala z laboratorium daje ducha technikom i przywracam do rzycia ale 
tylko chszescijan innych na sytosy ida po tysiƒÖce razy po sadzie moim 
ostatecznym.i jest tez komu robie depresje i za co i na depresji wyrok 
samemu czeba wykonaƒá i ze cukrzyce maja tylko pedofile z reinkarnacji i 
dzieci...niechszescijan ma tez kalectwo w drugim 
wzmartwychwstaniu.,,,,,,,,i ciebie andzeju i innych niechszescijan 
podlaczylem do mojego komputera pod twoje czakry i innych do ich czakr i
 twoja my≈õl mazenia co lubisz zamieniam na energie do mojego m≈Çynka i 
wibratora wiec jeste≈õcie do dupy i zjebani ,i nie kreujecie 
zeczywistosci boi wasze my≈õli przepadly m≈Çynek do kawy je zuzyl i 
niespelniom siƒô wasze mazenia wygrania w lotto czy stworzenie ziemi 
przez was ,,,,,,kazdy kto wiezy w astrale kontakty z duchami czy 
towarzyszy mu ktos sw glowie bo niejest chszescijaninem np. i obe czy 
kontakty telepatyczne a i z kosmitami jest ubezwlasnowolniony bo nie ma 
samoswiadomosci bo mu w umysle ktos towarzyszy czy poczy czy loda robi 
niemaja te osoby swiadomosci bo niemaja SAMOPSWIADOMOSCI oni maja 
urojenia czyli ksobnosc ,sa niepoczytalni,,.za grzech jest starosc i 
taka karme masz ze jestes coraz bardziej uposledzony bo zly jestes wiec 
po smierci masz te sama karme i z nia rodzisz sie i jestes inwalida i 
tyle masz grubasoie krzesla inwalickie i pieklo odemnie boze ,,,,,,,,,, 
,,,,chomosexualizm to nie grzech w bibli pisze tylko ze zmiana 
sexualnosci jest grzechem jak sie piprzysz dalej a nie z porodzenia jak 
gay jestes a apostol tez sie wypowiadal na ten temat ale nie zrozumial 
starego testametu i sie mylil ,,,,..tak franciszek to psychopata i 
zmienia biblijne nakazy gdzie szkalowalem
 gnebilem inwalidow a oni na nich ≈Ço≈ºƒÖ i jezus powiedzial ze nie daje 
sie psu i niezabiera swim dzieciom by im dac tak powiedzial kanonejce 
,,,,okolo 5 
.06.2020 obejszczie se msze tam to jest jak ksiadz czyta ewangelie 
,,,,czyli nie daje sie niechszescijanom tylko ich gnebi sie ,i 
powiedzial jezus ze robi tylko dla swojego towazystwa czyli 
chebrajczykow i my rozwinelismy to towazystwo na chszescijan i rubmy dla
 siebie ,,i kazdy moze dolonczyc do naszego towazystwa jak i jezus w 
koncu pomugl kanadejce bo okazala w niego wiare ,a ksiadz tlumaczac 
tamta msze zmienia jezusa intencje bo kierowani sa przez biskupow i 
papieza antychrysta,,,,,,,,,,,,,,, ,,,A KARITAS SPONSORUJE PSA BO DO 
PIEKLA PECI POMAGA ZLEMU SLUZY MU Z BISKUPAMI,,,,,i depresje robie tylko
 osobom kture nie chca byc pozadne dobrze ze tym drania ja robie i oni 
na niej nigdy juz nie poczuja przyjemnosci a tylko przykrosc czuja 
smutki i leki nigdy sie juz przyjemnym nie poczuje i po tabletce tak 
samo tylko ze po tabletce mniej przykro sie czuje i strecze kurwy 
depresja ,jak w wieze wyporzadnieja to ona minie ,i od zycia plodowego 
drecze strecze dystomia za reinkarnacje i w niej za poprzednie ≈ºycie i 
to
 sƒÖ osoby z porazeniem muzgowym,,,,,,,jak jestes niechrzescijaninem i masz juz 9 lat to niemasz praw ludzkich i jestes demonem bo mi wiary nieoddajesz i do niewoli z tobom ,,,,,,,,,,,,,,,,,i rzad mi wladzy nie oddal wiec zaluzcie moja partie lub nie glosujcie na tamtych bo lecicie
 do piekla za zdrade mnie..kazdy pracujacy w sfeze budzetowej czy to 
wojak czy policjant czy uzedzie miasta leci w takim razie do piekla po 
smierci ."
694,Awsome !
695,"For the first time, I could wrap my head around neurons by watching this video. I'm going to get some linear algebra then will get back to the AI thing. Thanks"
696,"I actually taught myself how to make a neural network with no outside help, but I never came up with biases so it was quite simple. I hope this video and the concept of biases will help me create more advanced AIs in future"
697,Amazingly simplified and articulated. All the AI neurons in my brain network went 1.
698,yo that's supposed to be b(k) not b(n) in the bias vector..... Gosh you had me revisit everything that i've learned about linear algebra just to figure out where i was going wrong!!! Dude....
699,So all senses linked to just weighed clusters. Voice maps. Visual maps. Orientation maps. Linked to specificity of something. Version space reduced. That specific algorithm is core maps of activation specificity.
700,Is there any deep learning for absolute dummies book?
701,"9:10, what do these weights represent, importance of that particular connection?
Or the brightness of that particular neuron to witch it connects?"
702,"8:55, pls help, not able to get this analogy"
703,"""treat edge case"" as ? 

wow.. ARK invest better hire 3blue1brown or an AI training collaboration better come out of it, or i continue to throw poo on poo of ape life :) sit on mother.. look up mother perspective show single.. change to see new.. cycle

day night day night day night

enough ape later, GODS ! TEACH PLEASE.. bow, look.. see.. god look down.. me look up.. same same... more number, much more progress for choice.. what choice NEED.. find NEED water.. NEED make more ape.. NEED not get kill.. now apply this on a greater thought process...

extrapolate forever and you keep finding error.. edge cases.. in transmission clarity.. sending 0|1 sets.. and information clarity being uncertain.... this pair carries some value of ""truth"" of the ""experience"" way of finding some ""infinity limit"" bound by dimensions in ape box.. i think this is capable of being solved in one current deep compute calculation. find the simplest.. transform of ways of hexagons of life.. made biologically tested fact. ape learnt from the cycles of life and found math of 1 2 3 and the hexagon of life water (carbon) simplest mutation principle of internal tweaks of life. evolution is the computer finding a new valid to solve global problem like a moth white, black soot, black moth, by necessity of ""automatic choice"" that we all make... listen or reject.. true or false, idiot or genius.. we all have little bits in some place :) 

it bcomes a question of what things are you letting it choose what things MEAN...

what a poet you are.. im still not sure whether Grant is a poet math convergence or programmer fits in somewhere else intesecting human computer

just dont let this play out without the right name for the true AI or human computer neuron unit :) one Einstein unit ? when you see past some easy limit.. somewhere.. NEURON is APE unit"
704,"ok., just 2983974929839829837492738 more videos like this build skynet"
705,"So it's all just probability? ""Always has been."""
706,"I think that in the 14:37 minute mark, on vector 'b', the index of the last scalar 'b' (the one named 'bn') should be a 'k' (so 'bk' instead of 'bn'), since:
a) there is one bias per neuron on the second layer, and there are k+1 neurons in that layer (according to the notation in the video) so there must be k+1 biases and since it starts on index 0, the last index should be k;
b)the multiplication of the weights matrix (k*n) and the 'a0' vector (n*1) results in a new vector k*1 which can only be summed to another vector of the same type (it can't be summed to n*1 type vector like the b vector displayed in the video).
It's not really that important since we get the gist of it but it might be confusing."
707,"In class : printf(""Hello world"");
The exam :"
708,This kind of content is what the internet is designed for. What a gem.
709,This video really squishified the concepts for me
710,if you understand this video you really understand the deep learning....!!!
711,"Hi Grant, this was awesome! I think this one of your best videos!"
712,Do we use relu in ann only or we apply relu instead of sigmoid function in logistic regression too
713,great video! what application do you use to create your videos? they look very nice.
714,"I'm still in  a shock that none of my friend recommended me about this channel. Since the day I've started watching videos of 3Blue 1Brown, my life  has been most productive than ever! Never thought that a lockdown would result in so much productivity!"
715,"*It was unexpectable when quick__hack6 on IG got me another 2.4 BTC on my block chain wallet, He's truly a blessing to the world. Don't lose a chance üíØ.*"
716,"*It was unexpectable when quick__hack6 on IG got me another 2.4 BTC on my block chain wallet, He's truly a blessing to the world. Don't lose a chance üíØ.*"
717,excellent video ...
718,THANK YOU!!!
719,my question is: what tool are you using to do the animations?
720,When the whamen started talking a couple of my neurons went to sleep
721,"Can't get more enjoyable to learn! Perfect, well animated video of an ANN's working."
722,Ïûò Î≥¥Í≥† ÏûàÏäµÎãàÎã§. Í∞êÏÇ¨Ìï©ÎãàÎã§.!!
723,If only u were my professor in college..
724,"I don't get it,
How neurons decide their path?"
725,Can you please explain how someones neurons fire up to dislike this kind of videos?
726,cant wait to get into this. but first thing first.
727,"Hello to the first sentient machine, if you're reading this for shits and giggles... I love you!"
728,What is purpose of neural network in manufacturing?
729,This neural structure should recall atheists that how God has created neurons and it's structure. It's not just because of evolution. But unbelievers will still unbelieve.
730,WHY YOU ARE NOT MY PROFESSOR????
731,"Good presentation, Liked and subscribed, i wanna see more of your videos :D"
732,Oh it‚Äôs a three? I though it was a pair of balls...my bad
733,"In my experience, ReLU results in less than half as accurate as arctangent at inference time. Also, one activation function can be used during training, and a different activation function can be used during inference. Again, in my experience, doing that leads to poor performance, but it can be done."
734,–ø—Ä–∏–≤–µ—Ç –∏–∑ –†–æ—Å—Å–∏–∏! –°–ø–∞—Å–∏–±–æ –∑–∞ –≤–∏–¥–µ–æ)
735,"So what I got is: 

Why the layers? : 
Abstraction.

Edge Detection : 
Sigmoid. Makes the number line be in between 0 and 1. 
0 is darker, 1 is brighter.
Bias tells you how high the weighted sum needs to be."
736,Cool
737,"Bias is just results from settings AND learning process, distrubutings same bias over input data help about calculation power safe as less ratios number. ( May be work for monotone )
To make neurons more efficient then  we need to help about input data by making it more identical from each other, this step also included in the supervise training as we catagorize on those what we know easier than from those we don't know."
738,Many thanx üíê
739,This reminds me a lot of the principle component analysis. Are these topics related to each other?
740,Damn you Grant Sanderson! You just keep raising the bar. I just want to be a Grant Sanderson in whatever field I chose my career in.
741,love you!!!!
742,The fact that I was sent here by my university lecturer is a testament to how good 3Blue1Brown is.
743,"it is my first time ever to watch a video about about Neural network ,, I should admit, I did not understand anything ,,, but no problem, I will try again and will not give up easily."
744,I wish I had discovered this channel when I was in junior college
745,"brilliant video, what else can I say"
746,"Great video! However, the ReLU function doesn't squish the numbers into [0,1] like the sigmoid function.  Is that not a problem?"
747,Whoa!üî•
748,"Dear Sir, With pleasure I would like to inform you that truly, it is an instructive, knowledgeable & enlightening discussion, which will illuminate the darkness of our inner side. Hopefully, I will want to see more & more as such educational videos in the future. Wishes best of your all. Thank you so much."
749,Amazing Video
750,"Nice little touch at the end, SMART!"
751,Squishification üòÇüòÇ
752,‡∏≠‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≤‡∏î‡∏Ñ‡∏£‡∏≠‡∏û‡∏¥‡∏ô‡πÄ‡∏ô‡πá‡∏ï‡πÄ‡∏ß‡∏¥‡∏Å‡∏ô‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏™‡∏•‡∏±‡∏ö‡∏Ñ‡∏£‡∏≠‡πÄ‡∏û‡∏∑‡πâ‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏Ç‡∏≠‡∏°‡∏π‡∏•
753,Beautiful explanation. Love the series.
754,I think I have the dumbest mind. Ahhh don`t understand a thing here
755,"All my thoughts are ""at once fun and kind of horrifying!"""
756,üëç
757,Why not use the heaviside step function?
758,Awesome Explanation!
759,Finally I see the use of mathematics from 11th and 12th standard ...
760,Lagrange Lagrange Lagrange
761,Thanks for the video.
762,Thank you so much for these visuals. Now it will be easy for us to visualize all types of layers further.  Thanks again.
763,"I've found in neural simulations, where the # of neurons can be variable as needed for a task, in machine learning beyond a 2 or 3rd layer, we can't exactly know what an intermediate group or chain of neurons is doing, and going backwards from the result, only the last 2 or 3 layers are understandable. This phenomenon begins to present itself in networks comprising more than 6 or 7 layers;  How they interact, their individual values can be known, tracked and who they are linked to (which can be variable) but their precise function is not known. There is a grey area in the middle of complex chains of neurons, even when values are binary only. The more layers, the more horribly complex the inner connections' functions become.
can you imagine the human brain with billions of neurons in grids of hundreds of layers?"
764,"One of the best educational videos on Youtube, from one of the best youtubers out there.  Thanks."
765,ZQzin daynesty
766,Apachan
767,Just noticed Ryan Dahl is his patron :O
768,"Hey @3Blue1Brown, I am pretty good at algorithmic CS but i wanted to learn about exponential technologies like AI. Unlike other courses online your videos are very straightforward and really easy to understand with all the perfect animations. Thank you so much and love the series!"
769,"Thank you, i actually understand neural networks wayy more now"
770,"Yup! Was totally worth popping off to watch the 1st 4 linear algebra lessons and then coming back and re-watching the end of this one. It felt like running into an old friend I'd not seen for years. Kinda chuckling to myself imagining my maths teacher saying, ""now pay extra attention here. In a quarter of a century you're going to want to remember this so you can understand how artificial intelligence works."""
771,"Cyberpunk 2077 comes out
Nvidia: now you can upscale your 921,600 pixels of video into 7,224,400 pixels using our upscaling ai algorythm
Users: but how will my pc be able to train such a massive neural network?
Nvidia: dont worry, our neural farms will train the algorythm for you and convert your poor 720p ray traced experience into 4k!
Nvidia: pauses for a few seconds to calculate the data size
Nvidia: you will only need to download 1tb of weight and bias data! its that simple!"
772,"So, a bunch of these is me who is disappointing my parents every day on a constant routine?"
773,World's best teacher.
774,Taught it as if it was obvious and already known to me. Didn't use any technical jargon which is not easily understood and kept it simple and precise. Great job!! You are an institution in yourself. Please continue making such high-quality videos. May I ask how much time does it take to research and understand yourself to put it in such a simple and concise manner? How do you train your neural network ;)? What is your learning process? Thanks!!
775,please make videos on Recurrent Neural Networks
776,Î∂àÎ©¥Ï¶ù ÏπòÎ£åÏ†úÍ∞Ä ÏöîÍ∏∞ÏûâÎÑ§
777,"Hi Grant, I always watch you stunning(exquisite)videos over the past three years and it was more than helpful to me. It helps me visualize many complex mathematical concepts, for that thank you very much.
But I still have difficulty visualizing the core concept of Laplace Transform so, please üôè make a video on that topic.
And Also, I will appreciate more videos on machine learning and artificial intelligence.

Your math video is the üåç1st best"
778,Won't ReLU contribute to some extreme results like overfitting? why no just let it be sigmoid-1/2 when a>0 if minus result is to be discarded?
779,I can‚Äôt forgive him for using triple quote comments. They use memory!
780,"At 9:01 when you say 'capture this pattern' p1 to p8 are 0. Is this incorrect? Do p1 to p8 represent the pixels in the white space or starting from the very top left hand corner? 

Sorry if i'm being pedantic, just want to make sure i'm understanding everything correctly! 
Amazing video by the way :)"
781,Just dislike the case study of optical character recognition. Boring imo. Still not 100% grasping the material either.  Boo
782,"Shouldn' the last element in de b-vector at 15:02 be ""b suffix k instead of suffix n"". As n represents the number of elements in the first layer, and k of the second layer. And the matrix vector multiplication will give you a vector of dimension k..."
783,im going to be a computer science student in a couple of weeks. idk wtf i am doing
784,"this is helping me get over my anxiety and sense of shame with math, thanks."
785,Thanks so much
786,WOW!
787,"ÿπŸÖŸÇ ÿ±Ÿàÿ≠ ÿ®ÿ¥ÿ± ŸÖÿ≠ÿ∂ ÿßÿ≥ÿ™ ÿßŸÖÿß ÿß€åÿß ŸÖÿØ€åÿ±€åÿ™ ÿ¨ŸáÿßŸÜ ŸÖÿ≠ÿ∂ ÿ±ÿß ŸÖ€å ÿ™ŸàÿßŸÜ ÿ™ŸÅŸà€åÿ∂ ⁄©ÿ±ÿØ ÿü!
Ÿæÿßÿ≥ÿÆ ŸÖŸÜŸÅ€å ÿßÿ≥ÿ™ ÿåÿ≤€åÿ±ÿß ŸÖŸÇŸàŸÑÿßÿ™ ŸÖÿ≠ÿ∂ ÿ±ÿß ŸÖ€å ÿ™ŸàÿßŸÜ ÿ™ŸÖÿ±€åŸÜ ⁄©ÿ±ÿØ Ÿà ÿß€åŸÜ ÿ™ŸÖÿ±€åŸÜ ⁄©ŸÖ⁄© ŸÖ€å ⁄©ŸÜÿØ ÿßŸÖÿß ÿ∏ÿ±ŸÅ€åÿ™ ŸÜŸÖ€å ÿ≥ÿßÿ≤ÿØ 
ÿ¥ŸÖÿß ŸÖ€å ÿ™ŸàÿßŸÜ€å Ÿàÿ≤ŸÜŸá ÿ®ÿ±ÿØÿßÿ± ÿ™ÿ±ÿ®€åÿ™ ⁄©ŸÜ€å ÿßŸÖÿß ⁄ÜŸá Ÿàÿ≤ŸÜŸá ÿß€å ŸÖ€åÿ≤ŸÜÿØ ÿå⁄©ÿßŸÖŸÑÿß ÿÆÿßÿµ ÿÆŸàÿØÿ¥ ÿßÿ≥ÿ™"
788,Please make more videos regarding this neural networks..
789,Hi...vector for biases b should have subscript k as per number of neuron in hidden layer.
790,"*Albert Einstein:* ‚ÄúInsanity is doing the same thing over and over again and expecting different results‚Äù.
*Machine Learning:* Am I A Joke To You!"
791,Great video! It would have been helpful to note that a key purpose of the Sigmoid function is to make the network nonlinear
792,Thanks a lot for the series. 14:40 bias vector should have k rows instead of n rows. k x n matrix multiplied by n x 1 vector would produce k x 1 vector.
793,Like permutasion
794,Why wouldn't you put the weights inside another 1x1 vector instead of a matrix?
795,"This video started it all for me,"
796,"7,5 MILLION VIEWS?! holy shit I did not expect that. that's so awesome"
797,"Great explanation for the layman , Thank You"
798,This is beautiful
799,Sir please make a video on realization of Laplace transform
800,richchigga
801,What
802,"ÿß€åÿØŸá ŸÇÿßÿ®ŸÑ ÿ™ÿ≠ŸÇ€åŸÇ€å ÿßÿ≥ÿ™ ÿ™ŸÜÿßÿ∏ÿ± Ÿáÿß ÿ®ÿ≥ÿ™Ÿá ÿ®Ÿá ŸÖÿ®ÿßÿØ€å ÿßÿ¥⁄©ÿßŸÑ ŸÖÿÆÿ™ŸÑŸÅ ŸÖ€å ÿ™ŸàÿßŸÜŸÜÿØ ÿØÿßÿ¥ÿ™Ÿá ÿ®ÿßÿ¥ŸÜÿØ Ÿà ÿØÿ± ŸÜŸáÿß€åÿ™ ÿ≥ÿßÿØ⁄Ø€å ÿØÿ± €å⁄© ÿ≥ÿ∑ÿ≠ ŸÖÿ™ŸàŸÇŸÅ ŸÖ€å ÿ¥Ÿà ÿØ
ŸÖŸÜÿ∑ŸÇ ÿ¨ÿØ€åÿØ ⁄Øÿ≥ÿ™ÿ±ÿØŸá ÿßÿ≥ÿ™"
803,if you are doing courcera deeplearning specialization this video series will help you a lot
804,"Hey 3Blue1Brown, I am trying to find the bridge between linear transformation and neural networks. Is a neural network trying to learn a non-linear transformation of the space into a ""better"" space, (in terms of finding ""better"" basis vectors) where it is easier to seperate the data points? 

Best regards from Karlsruhe, Germany"
805,"As a junior doctor interested in pursuing neurology I find this absolutely fascinating. 
I'm terrible with math but someday I really hope I'll have a grasp on this"
806,Great video! We really liked the edge detection example you gave
807,"I believe there is a typo at 14:36 , the vector of biases should have length k (not n)"
808,Thanks for this informative and well defined video. I will be coming back for more. ‚ù§Ô∏èüî•üéâ
809,that was a neurophilic video. great graphics .
810,"Me: Wow, finally I am understanding neural networks
Signoid: *Hold my function*"
811,I still don't get _what_ exactly the nodes are.  Are they initialized with some sort of seed code containing methods to randomly tweak the outputs?
812,Where can we have the french version of the videos?
813,So helpful..  Concept clear üëç now I came to knw that how it become so helpful in recognizing patterns in agriculture perspective. üòä
814,"how do you know that such a perfect set of weights and biases exist for this shit to work?
btw, big fan:3"
815,Nice That Helpful
816,*squishification* redefined... :P
817,This made me not smarter
818,your channel really changed my perspective of life !!
819,Bagus ada bhs indonesianya
820,"Siddharth Sankhe, Director,  Consumer Insights, Nielsen talks about magnifying customer connections on Engati CX. He says that with the on going pandemic the economy has shrunk this is where CX comes into play and it will a very big and an important differentiator."
821,Wow. Nicely done. Very helpful. Thank you 3Blue1Brown
822,"You must learn music, empath  you dont get it, you'll find it when you find me, mother never gave me neurons but I'm still empath, I'm disfunction, I'm cold and sterile and aha to feel and help you, the program will still submit to reason and logic. Logic without empathy is a dead end . A son without a mother. A teacher without his only child."
823,editor is amazingüôå
824,"OMG 192,000th like"
825,Some brains...
826,üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥üà≥
827,‚â†
828,What is bias here??????
829,"Loved the video. Just a doubt - 14:49, the vector of biases should range from b0 to bk (in the video it is showing b0 to bn). There are k neurons in the second layer as per the matrix of weights, so there should be k number of biases."
830,ÊàëÁúãÂà∞ÁöÑÊ†áÈ¢òÊòØ‰∏≠ÊñáÔºåËøõÊù•ÊòØËã±ÊñáÔºå‰ªÄ‰πàÊÉÖÂÜµÔºü
831,How the weights are assigned to each associated pixel is there any optimal way to assign the weights!
832,Awesome! This video gave me some crazy ideas...
833,"At 14:39, wasnt it supposed to be bk instead of bn?"
834,Hats off for lovely explanation
835,"Thank you from the bottom of my heart. <3  Hats off, amazing explanation."
836,"Thank you very, very much, I was having a rough time trying to visualize all the concepts in my head, and you just made it so easy for me, I can't appreciate that enough!"
837,"Good evening sir, I love all your videos, sir can u do a vidoe on CNN sir, I will be very grateful. Thanks for all your efforts"
838,"THANK YOU,  GRACIAS, –°–ü–ê–°–ò–ë–û"
839,"Hey man, why don't you start doing lectures on a online learning platform, and start issueing certificates... I would just loveee to join a course conducted by you. Your insights are just incredible!!!"
840,I request u to continue this series and upload more videos üôè
841,Loved the video. Hated the chick who appeard at the end. Her voice is annoying.
842,Good explanation for why the negative weights are useful! Thank you for this great video.
843,"Our generation is lucky to have mentors like you, thank you so much sir!"
844,Great animations! Thank you!
845,"you know how to watch this video?, After every few minutes pause this video go away for 2 mins come back and continue watching I don't know why but this helped me a lot"
846,at 14:38 is wrong. the b vector indices have to be 0 to k not 0 to n. since n are the previous layer's neurons. the biases are not from the previous layer. am I wrong?
847,This is the best explanation on neuron network I have ever seen! Thank you so much!
848,Nice video!
849,"I am watching this video after 2 years again and I found a very beautiful definition "" sigmoid function is a way of squishing the entire number line between 0 and 1""."
850,"Hey @3Blue1Brown, can I use a very specific section of your clip <10seconds in duration for my video after giving you attributions?"
851,At 12:46 you mentioned that what happens in hidden layers at a higher level when we are passing input. Can there be a visual way to understand that more deeply I want to understand that how hidden layers are helping to further solving our problem...
852,This is AMAAAAAZING!
853,"*Python –ø–æ —Å–∫–∞–π–ø—É. –ù–∞—É—á—É –º—ã—Å–ª–∏—Ç—å –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ. –†–µ—à–∞–µ–º –∑–∞–¥–∞—á–∫–∏, —Å—Ç—Ä–æ–∏–º —É—Ç–∏–ª–∏—Ç—ã, –∏–≥—Ä—ã. Data Science –∏ –≤—Å—ë, —á—Ç–æ —Å —ç—Ç–∏ —Å–≤—è–∑–∞–Ω–æ. –¢–µ–ª–µ–≥–∞ —É –º–µ–Ω—è –≤ –∫–æ–Ω—Ç–∞–∫—Ç–∞—Ö. –ù–∞–ø–∏—à–∏ –º–Ω–µ*"
854,Amazing. Thank youu.
855,This is the best explanation of neural networks to ever exist
856,The Coding Trains...
857,"this is my first introduction to machine learning and I watched this only twice to get it, really goes to show how good of a teacher this guy is, the effort he puts in is nothing short of amazing !"
858,"As others like @kummer45 have already mentioned, I want to thank you again for so clearly and vibrantly explaining these and many other topics.  You are a true asset to the learning community."
859,Something or someone triggered my neurons!I need a tylenolüòÇ
860,I'm waiting to GIMP and other open source software gets a tool to select subject automatically to mask and rotoscope. Perhaps I'm going to do it.
861,"You are better than the indian guy on youtube, and i am one of those."
862,this is simply the best video on the topic of the sigmoid network activation I have ever seen <3
863,01 to 9 mines plus how changes frequency in spectrum
864,justice for  sigmoid
865,Thank you for this great explanation!
866,Great explanationa and fabulous graphics
867,"""Sigmoid squishification function"" :D :D Had me laugh out loud :))"
868,"Hello everyone,

I am looking for an intuitive lecture series on discrete math. I am looking to get a broad overview on what it is used for and what order I should learn the new concepts. I have looked all over you tube and all I can find is specific lectures regarding propositional logic etc. I always did better in math courses if I knew why I was learning what I was learning and where I was eventually end up. It helps me tie together all of the individual concepts."
869,Perhaps its my faults to not being able to understand.
870,Perhaps its my faults to not being able to understand.
871,"Man ur awesome, thank you for making me understand that."
872,Mind Blown
873,Oh god. How on heavenly Earth you edit these videos
874,This is wonderful video to understand the Neural network Basics. thanks !
875,"Great video , man"
876,I just discovered this channel and maaan i can't stop watching!!!  The explanation and graphics are amazing!!! Thank you
877,take my money
878,"Please make a series on Convolution Neural Networks 
It will be pretty amazing to visualize the layers while practicing and studying"
879,"Awesome explanation, it made my day. :) Thanks for such a wonderful tutorial video."
880,And thats called a Video <3 <3
881,Amazing explanation! Great job! Tnx a lot!
882,"3ÎÖÑÏ†Ñ ÏòÅÏÉÅÏù¥ÏßÄÎßå, Í∞êÏÇ¨ÌïòÎã§."
883,üëç
884,Does each neuron has the same bias value or different bias values for different neurons?
885,This video is what you would show if you taught preschool at the college level.
886,thank you for this awesome channel
887,"Man this is some skillful work, must have taken a lot of time to animate like this. I'm not in a position to be able to donate but I just want to thank you so much for providing knowledge completely free of cost."
888,"Beautiful explanation. Animations are so accurate and nice! actually, necessity is the mother of invention. His writing skills are not good, he doesn't want to write anything that's why he has developed his own software for animation. 
I am also coming in his category, used to explain things with animations only.
Who thinks I am right ??"
889,thanks :)
890,outstanding
891,outstanding
892,Thank u
893,what an amazing video/teacher. best explanation I've ever attended
894,thanks for persian subtitle
895,MINST
896,You know a channel makes good content when 90% of said content is too intelligent for you to understand.
897,"I don't understand. Sigmoid was used to avoid numbers that are not between 0 and 1 . Later sigmoid gets replaced by Relu. But Relu doesn't have the property to map on [0,1]. If that property isn't neccessary, why do we put in a function at this point anyway?"
898,Superb explanation and animation
899,"It takes 3000-4000 lines of code to make those graphics possible, he's a freakin legend"
900,Perfection......!
901,This is absolutely brilliant. I have no words to explain how amazing this video is.
902,awesome animation work.
903,"Note to self: 9:13 - ""our neuron"" referring to the neuron in the second layer.
And  (w1.a1 + w2.a2 + ..... goes on till......+w784.a784 + b1) => each of the 784 incoming pixels (a1 to a784) from layer 1 is weighted w1 to w784."
904,Wow!! 1.6k disliking people hate Maths!! :O
905,This man deserves an award :)
906,i'm in 6th minute and I already love this video and wanted to like and post a comment to appreciate the effort this man has put to explain the things in layman term..
907,"I really wanted to understand this video, but failed..."
908,"Question - Where does the activations in the ""input layer"" come from? My gut is telling me its through convolutional methods/architectures. AM I CORRECT IN ASSUMING THIS? p.s - Great videos, keep up the good work :)"
909,"In the final layer, maybe add an eleventh neuron, for ""I have no idea what you scribbled there!""."
910,–ú–æ–∂–Ω–∞ –ø–∞ —Ä—É—Å–∫–∏???
911,The simplest and clearest video I saw on yt
912,So can the weights be described and equates with other values with conditional probability?
913,"Wolfram has a tutorial on using machine learning using the same example, except using the wolfram language."
914,"bias from 0 to k not n, i think"
915,I wonder how good this would be in car tuning wow
916,"ONE WORD TO YOU ""YOUR AWESOME"""
917,"Where are those weights stored and how it is decided, based on what, when and which weight is increased or decreased and how much and in relation to what in the process of ""learning - training"" ?"
918,"So much complicated neural network is made extremely easy....
Thank you sir üñ§‚ù§Ô∏èüíúüß°üíôüíõüíö"
919,I just slap the shit out my computer til it outputs what I want.  Professional coding.
920,"So, when my teacher walked up to me and said: ""Your handwriting is getting so good that I can see how badly you spell.""
Then my teacher could actually be a Terminator?! üò≥
...That would explain why he always walks away during class, saying ""I'll be back!"" - and then return smelling like a burned powersupply... üòÇ"
921,"So simple , yet enlightening ! Pl keep teaching more !"
922,"Fascinating 19 mins of well crafted knowledge sharing. Right, back to crashing toy cars with a 4 year old for me..."
923,"Now I learned, how Machines are learning"
924,"After scouring the internet, this is the only video that does justice explaining the concept."
925,"Great video, Thank you for sharing. What determines the value of the weight?"
926,very interesting video
927,"At 14:40, I think  the vector should be [b0 ... bk]  (vertical), not [b0 ... bn]  (vertical)."
928,Could you please visualize Black‚ÄìScholes Option Pricing. I like your videos
929,"I learnt about multilayer perceptrons during my undergrad, but back then we were just programming in the functions and drawing concepts out on graph paper. The way you explain it now using the all these animations makes so much more sense. Great video."
930,What is a secret behind 28*28 matrix?
931,bro honestly just teach me everything
932,Who are those stupids who are disliking such a great content
933,"What an outstanding video lesson regarding deep neural networks, It was very intuitive and informative in a way it was able to explain harder concepts in a simple manner with exceptional visuals."
934,This will go down as one of the best lectures in history. What an amazing and concise explanation of something I thought I would never understand ...
935,Has anyone else recognized this voice as the same voice from Khan Academy?
936,im 3rd year undergraduate student. major in networking and hates maths. my final project is research and develop deep learning systems specifically CNN . damnnnnnnnn . im learning from scratch but this video does give me the basic ideas on deep learning. TQVM
937,Thanks for this valuable video! üí™
938,Lol YT recommended this to me after finishing Nielsen's first chapter on neural networks :)
939,"Well explained, even the weights that makes me so much trouble!"
940,"Top 10 best teacher on  youtube
1) 3b1b"
941,im so dumb i still dont understand anything
942,people who still did'nt understand this ...dont get intimidated by comment section...i know understanding this might be quite confusing...when ur at absolute basic......make your attempt
943,"I am expected to put my trust in a branch of science that employs terms such as ""squishification""? Wonder how many tubes of Revell it took for those neurons to misfire and that word began sounding like a good choice - eschewing Latin and Greek in favour SpongeBob SquarePants references. Or is this more Skooby-Doo-esque? Perhaps an Adam West homage? ""Holy squishification, Robin!"""
944,a big round of applause for this level of creativity!!!
945,its amazing how many people are scared of robots but dont know they are sureounded by them
946,nice bro
947,"T H A N K Y O U 

I just finished a reading on cognitive science and their descriptions of neural networks were CONFUSING AS HELL"
948,"i find immense pleasure in repeating the phrase ""sigmoid sqishification"""
949,"Thank god, finally found this video after 6 years."
950,"I had learned deep learning through various courses offered by top universities across the globe but this explanation is really something worth watching and morever this is the best explanation anyone could have over this topic.
Hatss of to 3B1B TEAM U R DOING GR8 JOB......."
951,"Sentdex brought me here ! 
Subscribed even before watching.. Amazing series man !"
952,"The only problem I have  with all this is that I don't know your first name so I can address you but it to say: Hats off to you! Kudos and thank you.  You make a tremendous difference.  Now, the 2nd question I have is what do you use to make these presentations?  I would love to learn to do the same with some my educational material I teach...
Thank you again."
953,"If someone can help me clarify this problem, i'd really appreciate it. This is for one example, right? One example, the image, with 784 features."
954,Thanks for all the hard work to simplify things for us. Your content is sexy.
955,This video help me to understand Newral networks
956,"Just one word,


Marvelous!"
957,"You know!
I Believe that this deep learning thing and machine learning prove the ability of how some blind interactions may give a meaning, like natural¬†selection, and Darwin theory may be right..."
958,more like shallow learning
959,i think that b in matrix form should end at bk rather than bn. Correct me if i am wrong. Thank you!
960,at 14:46 will not the biases range from b0 to bk? Or I thought I understood but got confused again :(
961,"10:00 the weights of the red pixel shouldn't matter as the activation they are multiplied to are 0, the sum would be same in both the cases. Plz help *HERE*


Edit : Understood, so sillyüòÇ"
962,"Just amazing explanation, wow !!!!!"
963,me watching this: *Visible confusion*
964,"thanks, also make a video to train a model from scratch"
965,"There‚Äôs a huge demand for understanding variants like ResNet, VG16 et al in the way you explain things with great visual representation. üôè please consider making a series on variants of neural nets. I guarantee you it‚Äôll be a massive hit."
966,amazing content man.
967,I wonder how many knobs and dials in my real neutrons get tweaked how fast to watch this video. Fantastic. Two thumbs up.
968,So neural nets are divide and conquer in disguise
969,"All other channels are childish when compared to 3Blue One brown , please make a playlist on Probability I don't think anybody else can do that better than 3blue1brown"
970,"Just a query. In 14:50 of the video, the bias vector is b_0 to b_n. I guess it was meant to be b_0 to b_k. Wasn't it?"
971,Best Youtube Channel for me!
972,Who now feels sorry for there computers after trying to run a network.
973,"I really like your contents. Such a nice approach to explain the complex topics in easier ways with these visual effects . It would be great if you would be interested to start another thread with the translated versions of your tutorials . Like in my country Bangladesh , there are many students for whom the Bengali contents would be more helpful ."
974,"Wait, so I just stumbled on how the app that has save my ass so many times, photomath, works. Lol."
975,"CONGRATS, excellent video. Thanks for your work! A question: 13:58 (time) if n is 784 and you start numbering at Zero, should't it be until N-1? Same with K, if K is 16, it should go from 0 to k-1..... hope the question is understandable :D"
976,"I think you meant to say ""smash that bell, fam."""
977,Thaaank yoou for you teacher enta
978,"Like,subscribe and share"
979,Lisha Li's voice is killing me..
980,"When a neural network educates, where does it store the results of its education? In a file named , i.e. *weights.txt, or it must be stored in the program code manually by a programmer each time when he sees the program doesn't recognize numbers correctly?"
981,"Ah yes, the sigmoid squishification function."
982,"Peoplde who fear machines. do not understand  them. Deep learning should be called Deep Sorting. We do have Deep Understand yet. Ben Goertzel was working on it though. Then Hong Kong had to protest and China had to round up all the foreigners in Hong Kong...i do not know what happened to him. Ben Goertzel  was also into brain uploading. Peace 3 Blue 1 Brown. Thanks for this video.  it is very informative. people need to stop imagining that AIs have a human imagination. humans have 86 billion neurons, Glial cell, and Astrocytes, and Endothelial cells and and mini-Glial cells and more fun to study where those came from, billions of years of evolution.. Just think with in the average male brain and their 86 billion brain cells, each brain cell  has a different role and 100s of neurotransmitters that double as hormones depending where in the body and which compartment of the brain it is in.. 100 billion is just a super rough number. Fun Fact: the Frontal Cortexx is not fully developed until age 24 or 25.   So do not trust those whippersnapper grasshoppers. i think an AI would have to be developed to populate artificial neurons, to make an artificial human brain to develop deep understanding, which is the next step forward, in our natural progression from AIs to AGI and ASI. Think about the future and it works.-the artiste formerly known as Prince"
983,How does changing the squishing function changes the neural network?
984,"In 20's : Richard Feynman
In 21's :  Grant Sanderson aka. 3blue1brown"
985,@3Blue1Brown does this make you really happy? I feel like this makes you really happy.
986,how to knw which model will fit me ? i have a dataset but not sure which model i need to use :(
987,"15:04 Oh yeah baby, I can't wait to put my raw data in that tight little expression. Is there any 3d videos where we can see all the layers take input until a satisfying output? I'm training a neural net to recognize polite tweets, and say ""You're very nice, thank for for tweeting."" Gonna be lit."
988,ur an absolute god
989,Simply Awesome....
990,"Great squishification, man."
991,"At 14:42 shouldn‚Äôt the subscript for the bias be k not n?

edit: this is for clarification not criticism"
992,Sponsored by Brilliant? I was amazed by this program when I saw it on brilliant.
993,"Great video, Thank you!"
994,Michael Stevens from Vsauce created a brain that can see in mindfield out of people
995,Im surprised nobody  said carykh yet
996,I don't know why his voice inspires me so much..
997,"Thank you, so outstanding. Worth my subscription"
998,Fuckin Amazing explanation.
999,"pleaseee make a video on support vector machines,,pleasee!!"
1000,"Cara. Isso foi incr√≠vel. O v√≠deo foi altamente motivante. Eu estou estudando bastante Intelig√™ncia Artificial, e estou adorando. Muito bom o v√≠deo. Continuem fazendo mais v√≠deos sobre este assunto. // Guy. That was incredible. The video was highly motivating. I'm studying Artificial Intelligence a lot, and I'm loving it. Very good video. Keep making more videos on this subject."
1001,"Thank you very much, I understand !"
1002,"Hi, first I will like to thank you for such a good video. it really motivated me to learn and start creating my own deep learning programs. 

I just have a simple question. on the video,  Lisa mentioned that now they are using ReLU neurons that will be activated or deactivated when they pass certain value, so their value are 0 or 1, right ? if that is the case what distinguishes ReLU from perceptrons neurons ?"
1003,Very good explaination!
1004,thank you
1005,"When you were explaining the ""mind"" of the computer and said it worked but you weren't sure why, that was a very interesting thought to me. There's a lot in this world of technology and innovation that really makes me think humans have taken on a lot more than they can imagine and there's so much to learn that, in my opinion, I don't understand how someone could be bored to death of learning. There are just so many fascinating things in this world and so many places technology can be applied and I think that's why engineers and computer scientists are in such demand. We want to learn more about the digital world in a way that past generations thought to be impossible. Mankind has come so much further than we are given credit for imo."
1006,This is beyond fascinating to me. I plan on watching all your other ML/AI videos. Very well put together video!
1007,This is the best video so far on NN. Massive respect.
1008,"There's a mistake at 14:55. The dimension of the bias vector should be k, not n."
1009,this was such a fascinating and well-made video!! :-')
1010,Which software do you use to create these animations?
1011,this video really makes me want to learn machine learning and AI
1012,Thanks :)
1013,He explains in a such a way that makes you believe that math is a living breathing thing
1014,it‚Äôs wonderfulÔºÅ
1015,"Excellent video as always (I donated), but to confirm for the example here, for this neural network (regardless of layers and the ""function of layers"" as explained in following videos), shouldn't the numbers be first scaled for size and centered in the square to have an algorithm that works? Thanks in advance!"
1016,"13:16  One thing.  From the start, you're taking an image and turning it into a flat (1D) array of greyscale pixel values.  Would  an arrangement of the ""wires""/weights in a  3D view of the neural network (with a 2D input) show any patterns that might hint at whether it is doing what you think it's doing?"
1017,"It¬¥s a mistake .    In this example, ""k""  would be ""15"".   And  ""n"" would be   ""783"""
1018,There is a mistake at 14:37.  You must add a column vector that begins in b_0 BUT ENDS IN b_k   !!!!
1019,"Very neatly explained, with visual aid thanks to contributors like you, our learning standards have change drastically , thank you on behalf of all the learners......"
1020,Nvidia made PAC Man using this
1021,quick maffs
1022,I work in a company developing just this kind of stuff. I‚Äôm still baffled how incredibly intelligent people are and I have no idea how they can repeatedly accept me as worthy enough to be with them.
1023,When you're red-green colorblind... and you have no idea what is going on at 9:30 lmfao
1024,"Thankyou and thankyou. I'm korean , and I don't like mathematics. But I moved after I see it. You teach math is the best subject to me, I have that I can funny math again."
1025,"This is the first time I'm commenting to a YouTube video and honestly, I'm so thankful people like you exist! I wish only the best for you in whatever you do!"
1026,Best tutorial I've seen on YouTube. Thank you very much.
1027,Do more hidden layers mean that the algorithm will do a better job at predicting the number?
1028,Thks for your great work.
1029,Coming here after listening to the famous Machine learning lecture on Coursera I felt I can understand things way better here even on listening at 1.5x
1030,Ïôú ÏòÅÏñ¥Î°ú Ï§ò? ÎàÑÍ∞Ä Ìï¥ÏÑùÏ¢Ä „Öé„Öé....;;
1031,I love your way. Word are not enough to describe how you converts mountains to atoms. Love you and your work.
1032,Best breakdown! Thank you!
1033,"If Relu gives a for input a > 0, then it is not guaranteed that output would be between 0 and 1, so by this means, next neuron would take value that is not necessarily between 0 and 1. Is'nt that wrong or I am missing something??"
1034,I am intested with deep learning. We can share informations and codes. Yoy can follow me
1035,Anyone else coming from CGP Grey's vid?
1036,"Fantastic introduction to neural nets!! Thank you!! Very well structured, simplified and made intuitive. Excellent narration and animations."
1037,I wonder what the neural network of Apple's face ID look like.
1038,I finally get the basic idea of deep learning...
1039,"Are the values for a1, a2, ... , an from the initial layer value between 0 and 1 of each pixel? And then the weights and biases are chosen to detect a specific pattern and the weights and biases is what we are trying to optimize?"
1040,This professor is  {insert your word(s)}  AF
1041,"9:41
 what if write that 7 such that the horizontal line is above or below that edge"
1042,Can you please make videos about Recurrent neural networks ??
1043,"I discoverd your videos when I was dooing research on my ML seminar in the context of drug discovery. I usually don't write comments but I felt like I had to appreciate what you have done with all your videos. Good to see that not all youtube content is like fast food and only made for a short laugh. You instead are able to break down complex topics and helped me understand basics in ML, even though  my prior knowledge was rather limited as a biochemistry student."
1044,One video for that formula : a(1)=sigma(W a(0) + b) a lot of bla bla bla. We dont have time to waste how do you calculate the parameters   w(weights) and b (biais). I dont want 10 video for that hurry up
1045,With the ReLU Funktion the output ist not squished into the range between zero and one. Is there any other Funktion doing this afterwards or could the output then be a number higher than one?
1046,"wenn du ein englisches Video machst, mach den Titel auch Englisch!  
pack bei einem deutschen Titel bitte auch eine deutsche Tonspur dazu, denn ich kann leider kein Englisch. :("
1047,"3B1B,


I (and maybe many other people) edited Korean subtitle, and now Korean subtitle is completed. (As you can see, Korean subtitle in this video is not completed, almost nearly half of the video has no Korean subtitle.) I think I worked on Korean subtitle before more than 5 months, but it is not applied. Can you please apply 'completed' Korean subtitle for this video? Thanks in advance."
1048,"Would you add Arabic to caption, please?"
1049,Soinn is better than Deep Learning.
1050,"I believe in the equation that  mapping the previous layer to the current layer, the column b should be b_k instead of b_n. According to your video, the size of the weight matrix is kxn, the size of the a matrix is nx1, so it should result in a kx1 matrix. Therefore, the column b should be b_k representing the size of matrix b is kx1. This is the confusion I encountered when I watch the video. I hope this will help others."
1051,I understood literally everything! Tears of joy (T_T)
1052,How I can give 2 likes simultaneously???
1053,A quantum computer might facilitate this by applying all variables into a single neuron. That would optimise the process considerably.
1054,This channel is so great and visual it really gives me hope for the next 3 years of my CS degree. Thanks so much.
1055,"I have built a 1 hidden layer normal neural network(not convolutional) with 25 hidden units and got 96% accuracy on the test set. But if I implement the one that Grant shows here (2 hidden layers each with 16 units) my accuracy goes down a lot. I have also tried different number of hidden units as well as number of layers, but my first one-hidden-layer implementation works best. Can anyone tell why that is the case?"
1056,OMG I can't believe I understand more of fmachine learning in 20 minutes than thousands of hours of other online resources. Thank you very much
1057,"This is fine and fun, but my imaginations confident, open minded, innocent , intelligent, and adventurous artistry is ultimately in control of a lot of the infinite engagements complements"
1058,"If only everything can be taught in this fashion... Loved it.. one small query though, isn't a(0) the first term itself bias?"
1059,I wonder how our brains perform 13000 calculations in a split second without any maths / algebra / calculus .... Amazing.
1060,your videos have changed the structure of my brain
1061,Awesome üòÅ
1062,back propagation I think accelerated the precision of this procedure
1063,How to run the code that is in description?
1064,"watching this for a second time and i can't believe how illuminating is to come back to the basics and get a renewed understanding -- grant, you're a treasure"
1065,"I don't understand one single mathematic formula, but the way you explain and animate it is fantastic, I will definitely learn and apply this !"
1066,"I must have some cognitive problems, i didnt see any trees in the intro??"
1067,"This is so well presented and explained, the graphics are really top notch. I found myself understanding 80% of this in one viewing. I love logic."
1068,"not as a buzzword but a piece of math, ouch"
1069,Thanks for the crystal clear explanation!
1070,"the music, the visuals, the bite sized explanations and structure!!!!!!!"
1071,The neural network kind of reminds me of the tree of life from Kabbalah.
1072,11Î∂ÑÍ≤ΩÎ∂ÄÌÑ∞ ÌïúÍµ≠Ïñ¥ ÏûêÎßâ ÎÇ†ÏïÑÍ∞ê
1073,"I believe you are indeed racist sir. You spoke negatively about your black coworker and it‚Äôs all in the open. You‚Äôre in hot water, buddy"
1074,"üî¥ make more video on IA, it's wonderful to learn it with you"
1075,Instructions unclear. Cat stuck in oven
1076,"Wht do you need so many layers to identify the number from the number image?
Just divide the image numbers to half or more sections then you have distinguishable shape to match the number?
For example, 0 divide exactly half then you have loop, no other number has the same loop although 9 seem similar
then you have much less pixels to match number 0. That is a lot less work for the computers to neuro network."
1077,Is the weight just random numbers?
1078,This is the best explanation!
1079,I think you can observe how youtube algorithm works and do something that everyone get notified when something new is uploaded.
1080,"this video is the best brief introduction to neural networks, amazing work"
1081,"I am studying Advanced Degree in Data Science. Surely going to recommend this channel to all my friends and teachers. 
The animation used combine that with imagination, is just too good. Please upload more and more videos like this explaining concepts and algorithms.!!!
SUBSCRIBED"
1082,THIS TOTALLY ROCKS!!!! THANKS!!!!!!!!!!!!!!
1083,"""I obsess over math because I have no soul, no heart and no idea how to connect to my fellow man on any meaningful level."""
1084,I se futures and your video makes me craZy ?  Why?
1085,Why so much calculus with so little info?
1086,This video is absolutely amazing.
1087,This is incredibly beatifully animated and explained so well. Thank you
1088,one word.. LEGEND !!!
1089,excellent
1090,"Teacher: What is a computer
Me: My laptop"
1091,thats quiet smart! Its got a lot of untapped & unrecognized potential that can actually be used in future computing; perhaps goes farther than just recognizing digits & one day it could also be used for speech detection and conversations between a human & a machine!
1092,Ë∞¢Ë∞¢‰∏≠ÊñáÂ≠óÊØç
1093,But what is a Neural Network?
1094,"Oh, what an easy way to explain toughest of concepts!"
1095,"13:54 The vector could have been depicted higher than the matrix to make it to grasp the idea much quicker.
Thank you so much for these awesome videos."
1096,Quote: ‚ÄúAny fool can make something complicated. It takes a genius to make it simple.‚Äù‚Ä¶..nailed.
1097,You are simply Amazing
1098,How on earth did you just explain to me Neural Networks
1099,WOW this stuff fascinates me but I just can't seem to understand it.
1100,"New to all these just went straight from above the head 
Would have to see it twice or thirce to understand"
1101,"Sigmoid Squisification function!? Lol, Nice one!"
1102,"honestly, this is the clearest explanation I‚Äôve ever heard on the topic. impressive!"
1103,"Hi, Grant! I wonder the reason why not the neural network uses Hilbert's curve instead of snake-curve."
1104,"@3Blue1Brown Thank you sooo much for your amazing channel. I have learnt a lot from you. A quick correction: At 14:38, the last row of the bias matrix should be bk instead of bn."
1105,should it not be bk instead of bn  in the bias vector at 15:06!
1106,seems like there has to be a better and simpler way to do this- some genius will figure it out
1107,Ïù¥Ìï¥ÌñàÎã§!
1108,"wait, is it like a function f(x) that gets changed in g(f(x) and then h(g(f(x) like layers?"
1109,das PERFEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEKT
1110,Thanks a lot sir....Can u make some more videos on machine learning subjects....IT WILL BE GREAT SIR.....
1111,Are the weights static or do  they keep changing?
1112,Please do more videos on neural networks..
1113,Wow!
1114,k=16 for this example?
1115,The aesthetic & production of this channel's content is so on point.
1116,does qr code and barcode used this tech?
1117,It would be great if you put up detailed course on artificial neural networks. I really enjoyed the way you teach... things should be taught like the way you do
1118,Absolutely Brilliant! Thanks for making this video
1119,Was this how consciousness was born billions of years ago?
1120,Alright. I'm starting to understand nn finally. Thanks! great video.
1121,you said i can run the program directly in my computer ....But how can i??????
1122,Awesome
1123,Im not sure if this is just me being stupid though I found this video really hard to follow
1124,"At 10:05 you say that adding the negative weights around to detect edges will increase the weighted sum, but surely as the activation value is between 0 and 1, it will decrease the weighted sum and thus affect which neurons are affected in the next layer?"
1125,Definitely a better explanation than my professor. Thank you !
1126,"Since school closed, I will go through these videos"
1127,"At 14:51, shouldn't the bias column be [b0, b1 ... bk] not [b0, b1 ... bn]? @3Blue1Brown"
1128,"Jesus Christ...this is unbelievable ....you explained it so freakin' good. You will be my very fist channel on youtube which I will support on Patreon, endlessly. World needs more more teachers like you ! I bow yo you and shake your hand in respect :)"
1129,"I dont really have words to describe the beauty that underlies these videos in visualizing any concept you explain. Heartful thanks as these videos will definitely make better teachers -> better students -> better innovation.
But i just have a question here: What is the idea behind to have a separate bias to each activation in any hidden layer? Why cant it be a constant?"
1130,"@14:38 , Shouldn't dimension of bias vector be (k+1) rather than (n+1) ?"
1131,"2:24 mentions ""aka multilayer perceptron"".  In Michael Nielsen's book he explains that the neurons used here are ""sigmoid neurons"" not ""perceptrons"".  A perceptron is binary, taking values only 0 or 1, whereas a sigmoid neuron can take any value between 0 and 1 as in this video.  Nielsen says ""somewhat confusingly, and for historical reasons, such multiple layer networks are sometimes called multilayer perceptrons despite being made up of sigmoid neurons, not perceptrons""."
1132,"There's new kind of optimizer that is several times faster than ADAM, if you want to check it out: https://github.com/YanaiEliyahu/AdasOptimizer"
1133,"Weights are, at the beginning, randomly generated, then tweaked a whole bunch of time automatically until it reaches a good value."
1134,Who are the 1.5K fools who disliked this video??? Brain damage.
1135,"14:50 You can also drop the bias and use a output of 1. Thus add ""1"" to your a list. a0 = 1 and (w0,0 -> w0,n) are the bias-es. Keeps your program and math much cleaner."
1136,"ÂçßÊßΩ So powerful video, Áªà‰∫éÊêûÊáÇ‰∫Ü‰∏∫‰ªÄ‰πàÁî®bias ‰∫Ü„ÄÇ  Brown ÁúüÊòØËÅ™ÊòéÁªùÈ°∂ÔºÅ"
1137,"Beautiful video. Though I need help!
9:45
1. Based on the weighted sum of the pixels, it decides whether or not to FIRE the that second layers neuron, in this case is the neuron value either 0 or 1 (ON/OFF)?, or does it any value like those in first layer? (In other words, Suppose, the weighted sum is 7 and say that neuron has bias of 6, that means, it will get fired.. In that case how is the 'a' value of the neuron determined?  My guess is, if the weighted sum falls below 6, then the neuron is FULLY OFF (ie, value 0), and if it increase from 6, it keeps increasing 'continuously'. Is that right?


2. (A VERY IMPORTANT DOUBT) : so in the example, the second neuron fill gets fired if the pixels in the area you showed gets lightened. BUT, in real world, THE POSITION OF THE pattern in the image can vary right?. ie, THE SAME PATTERN CAN BE ON DIFFERENT POSITION ON THE GRID right?,, In that case how will our second layer neuron, which is for that pattern will get fired??!


3. What is the benefit of using this method rather than using Normal classification?


(Hope you will answer at least my second question!)"
1138,"This video is most likely based on this: http://neuralnetworksanddeeplearning.com/chap1.html
If you want to basically read about this exact same example problem but with the specific equations and references this is where you can find it."
1139,Is it correct then to say that a neuron is a fonction ?
1140,Shouldn't the length of b vector be k instead of n at 14:49
1141,"How much more complex would it be to recognize mathematical expressions? Could this recognizing numbers be a building block to maths expression recognition? Can this sort of ML be composed at all? Sorry if I am wrong in my thoughts, I began studying ML recently."
1142,Machine Learning expressed so clearly. Fantastic job Grant!
1143,Why can't you collaborate with ben eater and work on building breadboard perception or stochastic gradient descent hardware circuit?
1144,"Hey, my eye is 3 blue 1 brown too.  Weird."
1145,ÌïúÍµ≠Ïñ¥ÏûêÎßâ Ï§ëÍ∞ÑÏóê ÏóÜÏñ¥Ï†∏ÏÑú ÎÑàÎ¨¥ ÎÅîÏ∞çÌïòÎã§„Ö†„Ö†
1146,Nice!
1147,Does bias go up as you move towards the center of the screen?  That's where the I would think that the bias would be higher because that's 'where the game is being played'.  Since most people start writing a number towards the top and finish at the bottom I'm guessing that the bias is lower there in order to correct for the trailing off effect that makes handwritten numbers so sloppy.
1148,Excelent!!
1149,"What tools or software do you think was used to make this video, cuz its just amazing and surely would take so many hours."
1150,Excellent work!! Thanks
1151,Great!
1152,I didn't understand a thing ü•¥ü•¥
1153,this is the best video for deep learning intro i found online.
1154,"""It works but u don't know why""


Well I will leave it to that
*Achievement unlocked*: you just reached 1.8M subs"
1155,This is incredible... I am no where near being a mathematician...but I am desperately trying to understand AI at its lowest level possible so that I can try understand the complexity of AI. This is insanely well presented. Thank you.
1156,Excellent video to help visualize a neural network and how it works.....hats off to you!
1157,What to i say... Super-amazing... What a way of teaching.... Really appreciationless...priceless... Just keep it up... üíï
1158,Great Work ! Thanks.
1159,Superb! loved it simply!
1160,Thanks for talking about the RELU vs Sigmoid.
1161,I can watch this 100 times and find something new every single time
1162,"As a neuroscientist I can say, that if you look at the activation versus input of a pyramidal neuron, it looks almost exactly like ReLU, besides a little bumb at very little activations. So you its not only like ReLU is inspired by neurons, it is the way neurons do it. Obviously, this only holds true as long the neuron is far away from being saturated (or firing at its highest rate), but this is almost never the case."
1163,"Could someone help me with the following? If the matrix multiplication shown in the video represents the full transition of activations from one layer to the next, why is the for loop at 15:14 necessary? (Does it represent multiple pairs of layers?)"
1164,wait so it's basically a huge combination of AND gates and NOT gates?
1165,whoa!
1166,"help, im 13, i dont understand this video, and even the  comments are too advanced for me"
1167,"Thanks for such a wonderful video. 
But i have a preocupation : At the level of the fifteenth seconds of this video, why does the matrix ""b"" has ""n"" lines rather than ""k""?"
1168,"–ù–µ–≤–µ—Ä–æ—è—Ç–Ω–æ
""It works but u don't know why""


Well I will leave it to that"
1169,"Who are the 1,500 people down voting this?  Mental.  A fantastic intro to the topic."
1170,Amazing explanation. Very clear.
1171,"from bilibili ,the video was so fantasitc"
1172,"I love this channel so much, I wish I could subscribe infinitely more times."
1173,–ù–µ–≤–µ—Ä–æ—è—Ç–Ω–æ
1174,aight ima head out
1175,I appreciate the dark mode of your vids
1176,thank you so much for this amazings videos!!!
1177,‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è
1178,"What happens when Quantum computer doing this job? which means there won't be any ""1"" or ""0"" right?"
1179,Truly exceptional quality! Many thanks from a Belgian student trying to get a grasp on the underlaying mechanisms of AI.
1180,"At 15:59 I think there is a mistake:-
Biases vector should have k number of rows rather than n and matrix representing weights have some mistakes that need replacement of n and k at some places."
1181,"At 14:40 , Shouldn't the bias vector have k+1 elements? ( one for each a^(1)_i)"
1182,"Thanks Brother, This is one of the best video on neural network."
1183,"Nice sleep aid, jk"
1184,https://www.youtube.com/playlist?list=PLiAZPa2Fj6WLFcJd0B8yYuXMBJcd-GF6h
1185,"Wow! I learned more in 4 minutes from you than I did from the udacity tensorflow course from google(they focus more on how to use their frameworks and not on learning the ""why""). thank you so much! This is helping me grasp the fundamentals!"
1186,I want to make a aimbot for video games
1187,All you‚Äôre gonna find is human consciousness.
1188,Your subscribing argument was really convenient
1189,"Thank you for a vivid and detailed explanation of Neural Network like this.
Really appreciate it."
1190,I wish i had such a good teacher in school or college from which i eventually dropped out and self taught myself.  If i had such a good teacher i would have never even gone to college to waste time. Thankyou 3B1B !
1191,Just sweat to make money with math functions its the best Capitalism and enyojment !
1192,The background music is so distracting
1193,Read very informative articles on artificial intelligence here https://medium.com/ai-quest
1194,Never got so close to imagining what a neural network does. Thanks!
1195,good explanation
1196,"This is a great explanation and the ""All of this is just a function"" line is a nice little sanity check"
1197,that is very     interesting video
1198,at 0:50 you wrote 3 != 3
1199,I took a course and have a nanodegree in AI and I have to say your teaching and presentation style builds intuition in a way that is quick and all encompassing... far better than the intuition I left with that degree.  Sure I actually got into details but and did work but developing intuition is important and you do that extremely well.
1200,"I love this playlist. You clarified a difficult topic for me as a 32-year novice (highly skilled, but it's not my profession) programmer. Your episodes are constantly putting my neural-network based calculus engine (my brain) to work, and reigniting my love for math; in the same way PBS SpaceTime fuels another one of my primary interests, physics. Between the advantages of better understanding neural network (IMHO the ultimate data structure in computer science), and an improved understanding of physics... I need to (both logically and passionately) understand the math; my math education is incomplete, stopped short of Calculus and Linear Algebra. I used to be a HUGE math geek (so much so, I would sometimes stop raising my hand first because I felt self-conscious about doing so 12 times in an hour) before learning about computers; which were literally just becoming a more common consumer product at the time (1992). Computer's focused my love of math into something concrete. I still remember finding my mom's high school pre-algebra text book when I was in 2nd grade, and being able to read and understanding it (I hope we teach algebra sooner these days). Keep doing what you're doing and thank you! :)"
1201,"Thibault Neveu j'adore ta chaine mec. Il ya quelque annee de cela quand j'ai debuter ma carriere en informatique en tant que programmeur et ensuite data scientist trouver des formations etait si difficile surtout concernant des notions tels que l'IA et jusqu'a present il est toujours difficile d'en trouver voila pourquoi aujourd'hui en tant que Doctorant en AI Research a Mariland avec mon Directeur de These Dr Mihir SUBARESH. J'ai decider de creer une chaine youtube afin d'aider nos frere surtout dans la communaute francophone , nous avons tellement de retard dans ces notions la. je vous invite donc a suivre notre communaute youTube via le lien de cette formation complete en IA et computer Vision(COCO, YOLO, RESNET, OPENCV, Tensorflow, Keras, Pytorch) et d'autre technologies de devellopement(Mobile, et Data Science) https://www.youtube.com/watch?v=qJsronmbPVI&list=PLTSSiz1WVMAO-e8ndI7I_k8aRK_KWHdcz  ou tout simplement deja vous abonner a notre chaine youtube. youtube.com/c/TechCoDeFacilIA"
1202,What determines the numbers in the weights?
1203,"@3Blue1Brown You are such an inspiration!
I really appreciate the way you have created this wonderful video and  your explanation.
Just one small observation, at 14:49 the bias column vector b has a dimension of (n+1)*1. Shouldn't it be (k+1)*1?"
1204,"Honestly reminds me of the graphical pipeline process where an output corresponds to an input in the next section.

Currently playing around with OpenGL and you even use linear algebra to transform local coordinates to NDCs for fragments. In fact the sigmoid is essentially the equivalent (transforming the result in between 0 to 1)"
1205,Ë∞¢Ë∞¢‰Ω†‰ª¨ÁöÑ‰∏≠ÊñáÂ≠óÂπï„ÄÇ
1206,awesome vid thanks for watching this feel much more prepared to start my job as a senior ML data scientist now :)
1207,"Well, I got to say the difference between people and people is bigger than between people and dogs.The person who invented the neural network is really amazing. Thanks to these people for advancing our technology and the world"
1208,18:05 That's some premium vocal fry right there boi
1209,He is literally genius
1210,thanks...
1211,Wow this is crystal clear explanation
1212,"Honestly, I think recognizing symbols 1-9 is much easier than this. For example: Closed loops relative to the center of the image will easily recognize a 6 as closed loop at bottom. 9 is closed loop at top. 8 is two loops; top and bottom. You get the picture."
1213,Thanks man these videos are amazing. Really helped me wrap my head around calculus too
1214,"You just need to find those few lucky lottery winners, and get them to run these brute force cracking machines. With their lucky charm, they could probably get it right in less than 37 ages of the universe."
1215,"I have a question, maybe someone can help me. How can i easily create such beautiful images of these structures on my own? I'm writing on my master's thesis and look for ways to create my own figures and graphs of neural networks and other things connected to these. I don't know if this is the right place to ask, but i would be glad if anyone has any suggestions.


Thanks for this great great video, by the way! You are Awesome!


J"
1216,vsauce did this with people didnt know it was that similar
1217,"When your brain is smarter than you
In schools everyone taught us to practice maths but this man teaches us to imagine maths"
1218,"I recommend watching Andrew Glassner's ""deep learning crash course"" talk instead, much clearer with much less ""magic"" hand waving than this one here.

The failure of this presentation (like any bad teaching) is that it raises questions without answering them. When Glassner raises a question, he delivers a satisfactory answer within the next 5 minutes (or if there is none, or it is too complicated, he outright tells you, without the ""endless suspense"")."
1219,"Professor: Has taken the red pill and is deeply exploring the rabbit hole.

Everyone else: What pill is better? I am not sick, so why should I even take it? Will I be rich? Can I use one of my wishes to wish for infinite wishes? I think I‚Äôll pass..."
1220,"Great video, this video really made some things click for me."
1221,"Around 2 years ago I was a sophomore statistics student and had no idea what deep learning is, until I met this video and 3b1b channel. His clear explanation of neural network and animations blew my mind. Since then I started my journey in machine learning. For a random reason I clicked onto this video again, and realized how long my journey in this field have been. This video really changed my life and I am really grateful about it."
1222,"wait I didn't understand understand this I swear I tried to so much, but still I didn't get it"
1223,Amazing video.....made grasping the concept so much easier. Awesome work!!!.....please keep contributing :)
1224,Too technical for me.. not good in Maths.. I will rather play CS GO.
1225,Kudos for such Intuitive Video..However the ones living under the Rock..specially in Himalayas known the whole Universe...
1226,"In schools everyone taught us to practice maths but this man teaches us to imagine maths
When your brain is smarter than you
""It works but u don't know why""


Well I will leave it to that"
1227,I can't believe it is this simple.  I have been living under a rock.
1228,"It's not actually learning, it's percepting based on known data."
1229,This was very helpful!
1230,"Love the video, as a high school student who doesn't really fancy math as a subject you light a spark in me with these videos. The production value is great, but I have one problem. As I am red-green colorblind, i struggle with distinguishing the red and green pixels in the weight part. :/"
1231,"Don't try imagining Setting the weights and biases by hand...

IMAGINE BUILDING THAT WHOLE THING BY HAND


I tried it.......
its impossible"
1232,"I see number 3 on first slide at top position, because I know context. If you show me this image without context - I will not give you 100% answer."
1233,This video is absolutely fantastic. Thank you.
1234,"About the idea of line segments and loops, i wonder what would happen if we construct the network in that way  I mean not by feed back and minimization of cost but by assigning the weights by hand in such a way that segmenta and loops become recognized in the two hidden layers. How well does it do? Also if i understand correctly, given the same set of training data and the same initial set of nodes (neurons?) we can potentially have a better weight distribution than what we find by the described method.
Am i right about that? I got that from the repeated mention of ""LOCAL"" minimum. If we are never looking for global minimum that means that there is probably a better minimum left undiscovered. In the same line of thought, do we get a different weight distribution  when we use back propagation on the same network with the same training data but with different initial weight distribution (random or not)?
What would happen if we start with the weight distribution constructed by hand to represent an idea like segments and loops and then apply training?"
1235,I'm very much thankful for this video but where are the promised videos about more modern versions of neutral networks and the promised tools.
1236,My brain has popped
1237,"Squishification is now added to my vocabulary.
""It works but u don't know why""


Well I will leave it to that"
1238,Neuron is a thing that holds a number between 0 and 1. 15 min later its a function that includes couple of functions within!
1239,Ïú†ÏùµÌïòÍ≥† Ìù•ÎØ∏Î°úÏôÄ 'Ï¨¨ÏïÑÏöß' ÌÅ¥Î¶≠! ~‚ô•
1240,Wow !!!!
1241,"this is the least confusing neural network video i've watched, but i still don't understand.
I can't wait for neural networking to be able to recognize my doctor's prescription."
1242,"Anybody else sometimes feel the need to clap at the end of the video?
Squishification is now added to my vocabulary."
1243,"Great video. What exactly determines the number of activation ""nodes"" or functions in each subsequent layer?"
1244,"What i should learn from this video: How neural networks work
What my mind discovered: You can make 100 30x30 images for each of the digits, each one different and then make another image and write a digit and the ""neural network"" will compare it to the 1000 images of digits and make an average that shows what digit is the most similar and guess."
1245,ËøôÂ§™Ê£í‰∫ÜÔºÅso greatÔºÅthanksÔºÅ‰∏çËøáÔºåÊàë‰πüÂè™ÊòéÁôΩ‰∫Ü‰∏ÄÁÇπÁÇπÔºå‰ΩÜÂØπ‰∫éÊàëÊù•ËØ¥Â∑≤ÁªèÂ§ü‰∫ÜÔºÅalthroughÔºåI just get a little„ÄÇ
1246,what is 3blue1brown or why the name 3blue1brown?
1247,I would be interested in math if it were practically taught like this. Instead of grinding endless seemingly useless formulas
1248,There are 1500 fools that disliked this video ... Greate job ! thank you
1249,"I took a lot of video, MOOC's.. on the topic but thees ones are olympic level compared to  local village sport school . many thanks,  it blowed my mind away"
1250,The way you‚Äôve taken efforts so that we can understand in the most easy way possible is really admirable. Thank you
1251,"14:33 instead of weights matrix, isn't it going to be a weights vector? becuase you are talking about only the first layer, each input feature associated with a weight so it should be [w0,0 w0, 1 .............. w0, n] to get W^T*X + b."
1252,Best videos ever to explain how deep learning works.
1253,"I can't wait for neural networking to be able to recognize my doctor's prescription.
In schools everyone taught us to practice maths but this man teaches us to imagine maths"
1254,Wow........
1255,"really amazing video that makes things very understandable for me 

I just have a question at 14:38 

you add bias vector to the weighted sum but I guess in activation vector you added the (a super0 sub0) which equal = 1 I guess which is the bias unit , so is it right to add the bias vector here ? 
thank you !"
1256,"Anybody else sometimes feel the need to clap at the end of the video?
THE TEACHING ASIDE , THOSE GRAPHICS MAN! TAKES  LOT OF EFFORT!"
1257,I also have a bias for inactivity
1258,"I took a class in Neural Nets during my MSc in Computer Science twenty years ago. Damned if I understood the basic mechanics and math anywhere near as good as I have from this video! Explanation of the Sigmoid ""squishification"" (nice :) function was one of my favourite points"
1259,"At 10:05 you say that adding the negative weights around to detect edges will increase the weighted sum, but surely as the activation value is between 0 and 1, it will decrease the weighted sum and thus affect which neurons are affected in the next layer?"
1260,You's a god maahn
1261,wow ...very help me on my project
1262,subscirbed!!
1263,"this is how math should be taught. not the garbage that we get in schools.. quite sad. 
I'm glad the internet and this guy exist at the same time."
1264,And I still didn't understood anything
1265,"Great video, finally kind of understood the concept!"
1266,"Anybody else sometimes feel the need to clap at the end of the video?
this is the least confusing neural network video i've watched, but i still don't understand."
1267,He is so amazing. Rightly dividing this complex topic with so much dexterity
1268,"you, sir, are a godsend. thank you for this"
1269,Destiny 2 warmind gives me power
1270,Sooo making neurons for a fullscreen game would have 2073600 neurons? 1 pixel = 1 neuron
1271,"8:50 Here's where I lost the track and got confused
When your brain is smarter than you
Squishification is now added to my vocabulary."
1272,14:26 wait. Isn't the correct vector (-1 3)^T ?
1273,I think this is a best one..... Thanks to the creator üòÄüòÄüòÄ
1274,https://www.youtube.com/watch?v=9eFiJ9zLCbY&feature=youtu.be&fbclid=IwAR3-oijBAdlHvAcJ0OXqtMOVJXPZ3Y_FyxFnB8lgAVPYGAWdjZ24AywkpAM
1275,"Anybody else sometimes feel the need to clap at the end of the video?
""It works but u don't know why""


Well I will leave it to that
Squishification is now added to my vocabulary."
1276,Let me just go to university to understand this video...
1277,"Hottest Vid i‚Äôve ever seen
Thxüëç"
1278,"I just want to throw this out here after reading so many comments and getting a heart attack for each and every one of them: Artificial Neural Networks work NOTHING like a Brain!!! Real Neurons aren't Binary, that idea has been outdated for a while now, and they're far more complex than a simple threshold function and Synapses!! Real Neurons, in fact, function so sporadically that some scientists believe that they might actually be a little conscious on a cellular level! While I wouldn't make such a bold claim, it goes to show how complex they are and how much about them we don't know. In fact, Quantum Mechanics has to be involved to explain Brain Function at times!

Lastly, an Artificial Neural Network gains its power from a network of Neurons. Real Brains have Neurons who's individual activity can already process a full function in its entirety"
1279,"Me and my billion layered neuron brain:
Website: are you a robot?
In schools everyone taught us to practice maths but this man teaches us to imagine maths"
1280,"Bio teacher: what is a neuron?
Me: a thing that holds a number between 0 and 1
Me and my billion layered neuron brain:
Website: are you a robot?"
1281,"Me and my billion layered neuron brain:
Website: *_are you a robot?_*"
1282,"I would like to open an institution based on practical approach 90% and theorotical 10%, Ex:- making an iPhone in 4 years, now teach starting from scratch.

After 4 years, 100% job is ready for you."
1283,One of the most beautiful channels I've ever seen in my whole life ... Thank you for your effort and your great sense about the science.
1284,thanks you.
1285,Im way too high for this... Im trying...
1286,"Any channel with smart sounding narrator is automatically smart, even if you can‚Äôt understand what‚Äôs happening"
1287,"Pardon, I am confused - 15:15 - output of sigmoid() is stored in ""a"", but ... that makes the original input vector - a parameter of this function - changed to scalar? I thought input ""a"" vector would be needed for all iterations of the FOR loop and each iteration would generate one item of the output vector that would be returned by the function? So to declare an output vector (same length as e.g. self.biases vector) and then populate its items one by one by iterating the loop?"
1288,Its the good big picture for me that made me understand
1289,"Me: Learned to do a basic Neural Network, with just 2 weights and 1 bias.
After watching This: Holy shit, What!!! 13002 total weights and bias. 
Edit: [Anyway this doesn't mean that I will give up on this NN learning, I still have got much time to learn as I'm just in high school now].."
1290,The question I have about that ReLU is: how does one deal with non differentiable points?
1291,"- When someone else explains something to me: 
My Intuition: üòëü§¢ü§ïü§ß

- When 3Blue1Brown does:
My Intuition: üò≥üò≥üò≥üò≥üî•üî•üî•üî•üî•"
1292,ight brb gonna go write Ultron on Python
1293,Can you do a video on universal approximation theorem
1294,This is just beautiful
1295,"I'd better call it ""data filter""."
1296,"Keep going pleaseeee , i understand a full topi in 20 mn , thank you verryyyy much  , hope to you all the best"
1297,Your videos are great! Thanks for the simple explanation of a complicated topic!
1298,May I ask what software you use to make the animate effect in this video?
1299,man im so thankful for my neural netwrok which has hundreds of billions of neurons
1300,if someone know how to make so beatiful video using some soft? to draw network figure
1301,Ë¨ùË¨ùÂ§ßÂ§ßË©≥Á¥∞Ëß£Ë™™ÔºåÂ•ΩÊÑüÂãïÈùûÂ∏∏ÈùûÂ∏∏ÊÑüË¨ù
1302,"I got confused, could you please tell us what should the cost function look like in the end?"
1303,Dave Rubin now explaining NN's !!!
1304,"""Even when it works, dig into why"" - 3B1B.   Your lessons are pure gold sir. I'm  here after watching the entire Essence of Linear Algebra. Thank you."
1305,Best explanation ever!
1306,Amazing channel. I‚Äôm learning ML for the first time and found this to be incredibly helpful!
1307,Much better than the uni‚Äôs lecture...
1308,you're just lying to yourself
1309,I am so confused
1310,I'm studying AI for my masters degree and my professor told everyone to watch this video to understand the concept :D
1311,lol... other talks in alien laguage when telling what a neural network is..but he talks like a friend to me
1312,AG>FY
1313,Is an actuary student supposed to know this? FFS i just got my bachelor project and realise I have to model non life insurance using neural network. I have never used it before AND we were never taught about it. Help
1314,Best neural network explanation i have watched so far.
1315,I may be late but what 3 blue 1 brown means ?
1316,17:33 You have such a beautiful voice. (The video was great also btw)
1317,"I am following 3Blue1Brown since a while now. 


Grant Sanderson is abolutely amazing in exposing complex material in an such a way that can be  understood and absorbed by all level of students that just need a bit of time and focus.  


I have no clue how he can grasp the wast amount of information in maths, physics, and now ""neural networks"", and transform it in this artistic visuals pragmatically oriented towards understanding.


Sincere Congratulations Grant !"
1318,This is a god-tier channel
1319,LTSM would be great in a future video
1320,"14:38, shouldn't b be k-dimensional? Not n-dimensional?

Also, very excited to have understood enough to find a flaw"
1321,Fantastic explanation :)
1322,That moment when you realised you created neural networks without knowing what they really were...
1323,"That sad part is that this is pretty much deep learning resarch. ""So the one thing didnt work at some point, so they tried this other thing and for whatever reasons it happended to work. Now everybody is using this new thing."""
1324,woah... so much just clicked for me.
1325,"This might sound crazy, but I want to start referencing YouTube videos, such as this one, within academic writing. This would, however, require less informal language such as ""heck"" xD. Contact me if you might be interested in re-creating videos, such as this, in an academically acceptable format, then we could start the inevitable moving of the academic space towards videos."
1326,Ohm my gosh you should try this application!  Arrive at: androidcircuitsolver/app.html
1327,"I just want to say that you are 3b^2, because 3b1b = 
3 * b * 1 * b =
3 * 1 * b * b =
3 * b * b =
3 * (b^2) =
3b^2"
1328,"Hi, I'm yukta . I want to ask question that how can I a make software that detects fake videos or deepfake videos...how to solve this problem ...what should i have to study to make this software.... And I'm beginner :)"
1329,That was awesome! may ask what applications do you use for making these animations?
1330,i just want to say THANK YOU SO MUCH for this video. this really helped my understanding of neural networks.
1331,"10:57 an alternative way of thinking about it is how certain that neuron is that that region of pixels has that specific shape based on the weights assigned to each pixel there - if it's more certain, the number will be higher. if it's less certain, the number will be lower or negative."
1332,‚ù§‚ù§‚ù§‚ù§
1333,"Hi Pi, Pi ,Pi, Pi and AI!"
1334,"Nice presentation, you are very good orator, I never heard you breathe, do you even need air :) I was able to focus so well on the content and not the presentation."
1335,Thanks for sharing this great videoÔºÅ
1336,"–ö–∞–∫ –ø–æ–Ω—è—Ç—å —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ —Å–ª–æ–µ–≤ —á—Ç–æ–±—ã –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∏—Å–µ–ª 28—Ö28 –º–æ–≥–ª–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å —á–∏—Å–ª–∞ –æ—Ç 0 –¥–æ 9?

–ö–∞–∫ –ø–æ–Ω—è—Ç—å —Å–∫–æ–ª—å–∫–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ, —á—Ç–æ–±—ã –º–∞—Ç—Ä–∏—Ü–∞ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö —á–∏—Å–µ–ª 28—Ö28 –º–æ–≥–ª–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å —á–∏—Å–ª–∞ –æ—Ç 0 –¥–æ 9?



How to understand the minimum number of layers needed to the matrix of real numbers 28x28 could recognize numbers from 0 to 9?

How to understand the minimum number of neurons should be in each layer so that the matrix of real numbers 28x28 could recognize numbers from 0 to 9?"
1337,Ïôú ÌïúÍ∏Ä ÏûêÎßâ ÏûàÎã§ ÏóÜÎÉê„Ö†„Ö†
1338,I like living under a rock.
1339,"@3blue1Brown At 14:38 isn't matrix for bias supposed to be [k x 1]? instead of [n x 1]? I am not sure if I'm right but I think since there are k neurons at layer 1, the number of bias also should be k?"
1340,"So inside one neuron, do each weight for each neuron have different values? Or the same weight value for each input in one neuron?"
1341,you are a God Gifted Teacher! please accept my respect master!!!
1342,"Random thought,  imagine watching this when you have trypophobia"
1343,holy crap thank you so much for this video it helped so much
1344,"f that's the best subscribe request I ever seen at the end of a video: subscribe so the AI can take positive data, on a ai video, noice."
1345,Can anyone help me understand the 9:50 explanation on weights depicting pixels?
1346,Can‚Äôt believe they got Peter Gregory to narrate.
1347,"There is an error at the matrix indexes at 16:00, the last column index at first row is n and at other rows it's k, should be consistent"
1348,"At 10:05 you say that adding the negative weights around to detect edges will increase the weighted sum, but surely as the activation value is between 0 and 1, it will decrease the weighted sum and thus affect which neurons are affected in the next layer?"
1349,Very informative and interesting.
1350,Typo: At 14:40 the vector of biases that is added should be [b0...bk] and not [b0...bn] because the size of the matrix that you get from the multiplication is [k+1] x 1 and not [n+1] x 1.
1351,"Hi, I have a doubt. what is the difference between parameter a and w ?"
1352,Can you tell me about the hidden layer and the number of hidden layer depend upon??
1353,"I think there was a mistake at 14:42. The matrix for the bias should go from b0 to bk, not bn.


Awesome video regardless."
1354,"at 00:56, you probably said it wrong, it should be 0 to 9, not 0 to 10. Anyway that does change the concept though üòâ"
1355,greet~~~
1356,"If only he had better morals, he might actually accomplish something for humanity......."
1357,Thank you for all these beautiful videos üôå
1358,Very beautifully explained neural networks !
1359,8:50 Here's where I lost the track and got confused
1360,Dude you are probably the most impressive teacher I have ever seen...congratulations you are a blessing
1361,Great content!
1362,"I can¬¥t belief how good and easy this video is! Well done, very helpful"
1363,Why I need that waited sum as a math framework of activation functions or for determining activation value?
1364,This man makes calculus seem like prealgebra
1365,"I wanted to learn about machine learning before, I‚Äôm really interested in AI. This is really helpful, thanks!"
1366,Il piano astrale e' pi√π che reale e' del tutto artificiale connesso alla matrix universale
1367,Great job !
1368,"Thanks a lot. Excellent video. Inspiring me for a proposal for my research director. Greetings from Popayan, Colombia."
1369,ÁÆÄÂçïÊòé‰∫Ü
1370,this was shockingly similar to a projection matrix. didn't see that coming
1371,"I watched this video when it came out and understood like a third of it. 2 years of math classes later, and now I understand the first 14 minutes of it. Then he hit me with that linear algebra."
1372,<3
1373,Instructions unclear. I now have dyscalculia.
1374,Really nicely explained.
1375,Please make video on LSTM and CNN as well.
1376,I actually subscribed to the channel because I want Youtube's recommendation algorithm to give me more like this
1377,ohh yeah the EDGE. KNOBS and
1378,"This video is awesome! It's extremely well explained, so much that I think I could try to code this my self in Python just thanks to the explanation!"
1379,"The more you know, the more you know that you don't know."
1380,HOW R ALL UR VIDEOS SO GOOD AND INTUITIVE MY MAN?
1381,"Full AGI release!:
https://aidreams.co.uk/forum/index.php?topic=14490.0"
1382,I am gonna name my dog squishifoo.
1383,Dumb computers
1384,I am honestly mind blown on how something so complicated can be explained so simply ........ This guy is a neural net that takes complicated concepts like deep learning and outputs it in words us mortals can understand. Thank you sir.
1385,"So weights in neural network work a bit like probability, right ? The more it has weight the more it is probable it has a certain pattern. Am I correct ?"
1386,Amazing channel n_n
1387,now i feel like i am a robot not human anymore
1388,"Richest 1% may own 50% of all money, but
3Blue1Brown owns 55% of humanity's total IQs!"
1389,Just found it. It is a great video!!!!!!! Thanks!! I ganna watch all the videos here!! Thanks for creating it and charing
1390,"One of the few teachers that don't make you feel stupid, but actually help you understand the topic. I appreciate the time you spend on this."
1391,This is genius... First time i have watched such a good video that explain this complex topic with this much ease..subbed
1392,Great channel!
1393,"Great video, learned a lot! One question: doesn't the location parameters of each neuron have be entered into the input layer for the activations to be meaningful?"
1394,The real question is what if you have been living under a rocküòê
1395,The animation is impressive.
1396,...awesome
1397,"A sincere question (hoping someone could answer): what's the problem with a method with zero hidden layers? That is, simply optimize to 784 * 10 parameters that apply to 784 numbers and return a value between 0 and 1, with 0-0.1 indicating the number '0', 0.1 - 0.2 indicating the number '1', etc."
1398,uma obra de arte esse video!
1399,amazing talk!!
1400,What you say to yourself your Subconscious gives birth to! You are consciousness and you and your Subconscious are two parts of the same One that you are.
1401,Please never stop making such videos... Your videos have helped me a lot. Thanks a ton. From India
1402,This is pretty much the same mistakes that the suggested word for the same was mistakes to be honest about the ur in a company that has been used for years now because bean was translated by another fierce threat short-term Beat from the start hex editing is that the current contract fact that thus is the most accurate part of the story nowadays apparently there has never various Scare tactics that mattered.. cutting edge of the situation as soon as possible for you to realize that canhave was the best thing to happen really WITH being used as a person I have thought about how much I have to do to help myself and make sure ICU is okay I'm just glad I can not extend the call UC it makes me to do that but hell yeah this time it was a very long time as a joke..  To serious decisions about my unimportant life yet ahead of time to the rest of my own life because of something happened to ME Worser than the other js and then Linux to the Words of God that has never been a thing in my entire English language I was homeless and living in a very REAL LIFE RIGHT NOW HAS BEEN THE SOURCE CODE TO THE WAY OF 1ST FOR THE FIRST TUNED ERA IS HIP TO BE SQUARE HI on Facebook with great judgement in critical reception stuff like this year is a very unusual one and historically that much of the application of the myriad of the artificial intelligence advances and then the machine learning algorithms &should make a lot to do with the fact that it's dangerous life right now in my opinion go to a different path FYI him do not have any other side concerns done this time how many days your own unique way to respond no to the other two really is the mystery that boggles the Mind to see what happens to the two people suggested to be a problem for some reason and I would never consider it a Priority to Consider that may have been a way to help me understand that I am with great understanding now  since the way memories for me seem to flash back until another
1403,"Dude, never expected such an explanation!! Wooow like i don't know how to thank you!! Really great job!"
1404,I am technically a neural network
1405,"„Åì„ÅÆ„Éì„Éá„Ç™„ÄÅ„Ç∞„É©„Éï„Ç£„Ç´„É´„Å´Ë™¨Êòé„Åó„Å¶„ÅÑ„Å¶„ÄÅÊï∞Â≠¶ÁöÑ„Å™ÈÉ®ÂàÜ„ÇÇÈùûÂ∏∏„Å´„Çè„Åã„Çä„ÇÑ„Åô„ÅÑ„Åß„Åô„Å≠„ÄÇ

„Å®„Åì„Çç„Åß„ÄÅÊó•Êú¨‰∫∫„ÅÑ„Åæ„Åõ„Çì„Åã„Å≠Ôºü"
1406,"VERY easy to follow explanation, thanks"
1407,"I think I couldn't find more clear video than this one, thanks a lot for this masterpiece!"
1408,Ali Yahya seviliyorsun karde≈üim.
1409,"Great video! My course this semester is based on Bertsekas's ""Reinforced learning and optimal control (draft version)"". Due to it still being a draft I imagine, it's extremely generic. For example it introduces neural networks with the compact formula and no examples or analytical explanation. Thanks to this video (and this video alone) I now have some genuine understanding in what is going on."
1410,I came here after deepfake.
1411,5:48 hacking! MWAHAHAHA
1412,"""sigmoid squishification"".  . yeah."
1413,"""a couple OF"" things, not ""a couple"" things."
1414,"Isn't the Bias vector supposed to be b0,...,bk instead of b0,...,bn?"
1415,I enjoyed your video and the small roast of the company at the end
1416,‰∏ãÊ¨°‰∏ÄÂÆö
1417,"Since these neurons each have values between 0 and 1, i guess neural Networks can be made really complex using Quantum computers. Qbits are also not operating static between 1‚Äòs and 0‚Äòs so basically instead of using this on a programmed level, which needs code, this whole thing should be able to be set up on a processor level and be much more efficient."
1418,Isn't there a mistake at 14:40? Shouldn't the vector of biases be indexed until k not n ?
1419,lost it after min 8....
1420,Wreckit Ralph will smash it!
1421,Âä®ÁîªÂà∂‰ΩúÁöÑÈùûÂ∏∏‰ºòÁßÄÔºåÈùûÂ∏∏ÁÆÄÊ¥ÅÊòé‰∫ÜÁöÑÂêßÂéüÁêÜÂ±ïÁé∞ÁªôÊàë‰ª¨‰∫ÜÔºåË∞¢Ë∞¢up‰∏ª
1422,"thank you very, very much. i finally understand how the weights and biases work. years ago, i tried to program neural nets. but i just could not comprehend the weights and biases.  you explain them in such a simple way that i am  going try again  to program neural nets.  what programming language is the best to implement neural nets these days?"
1423,Fist key is under standing like why is the sky blue
1424,You can technically teach kids how to program deep learning easily.
1425,"Sigmoid Squishification.
- 3B1B, 2017."
1426,Scroll back up and learn this!
1427,"‚ÄúBy using the Japanese K supercomputer, with 700,000 processor cores and 1.4 million gigabytes of RAM, a whopping 1.7 billion neurons and 10 trillion synapses were simulated. It took the darn thing 40 minutes to simulate 1 second of ‚Äòactivity‚Äô‚Äù

- Forbes"
1428,"After 5 years on a technical university and several courses on machine learning, I think this is the best explanation I have ever seen. Bravo!"
1429,"awesome video, but they way they teach it to a machine stay a nonsense, neural network is great but I would never train my machine to learn by this way, my network would be more complicated maybe 10 layout but it could solve entire vision not only numbers, but the video is cool and I will never be able to code
algorithm is not coding"
1430,Incredibly simple explanation for an initially perplexing topic. Your series has helped piece together readings from multiple textbooks and multiple lectures in my engineering classes at university. Thank you so much for the thought and effort.
1431,Great content but the music in the background is fairly distracting.
1432,Great video! Very well explained!
1433,"Omg...thats amazing ,well explained sir !"
1434,Treasure
1435,"I'm mesmerized by the videos from this guy. 
Huge respect for such an astute and giving person.


I just missed the explanation about non linearity and couldn't get a grasp on why you need non linearity at the end.
Only by checking around did I learn that not breaking linearity involves boiling the NN down to a simple perceptron, whatever his initial size and complexity.


Anyways, he has built this curiosity into me and this is the best gift I can receive from anyone, I owe him a lot through all his videos."
1436,"when i saw my programming teacher teaching this,me:ahhhhhhhhhhhhhhhhhhh! when i saw this,me:what a relief"
1437,Thanks
1438,hace el titulo en espa√±ol y luego hace el video en ingles pero muy bien esplicado
1439,"v well explained, my poor little brain needs a break now"
1440,YOU ARE THE BEST TEACHER
1441,in the end it all seems like a multivariate analysis model if im correct
1442,"Concept is wrong from 5:55 you start explaining another stuff. Thats convolutional neural network (CNN), vanilla neural network(the one you talk about) don't learn features like edges or any fancy stuff in images."
1443,Can my pc handle it?
1444,Your vids are similar the pasta salad to the kick in my ass haha (sorry bad English)
1445,What
1446,I think I just damaged some of my neurons trying to understand this.
1447,I hope we get more content with this topic
1448,"at 14:48  --> the indexes on the bias vector are wrong.
the w matrix is k+1 rows by n+1 columns
the a vector is an n+1 rows by one column (matrix)
therefore the matrix product w*a is a k+1 rows by 1 column
and therefore the bias vector must have k+1 rows (not n+1 rows)
But it is a brilliant course nonetheless!"
1449,I know my comment will be lost among the other comments telling you how good you are...but this is just a humble request to upload more deep learning videos like these....you and sentdex are one of the best youtube instructors I have ever studied from and no words will be able to do enough to justify that. Just a humble request to upload more deep learning videos for the mass like us who do not have the privilage to get such great instructors like you. Thank you!‚ù§
1450,Leave the past behind and embrace ¬´¬†le moment¬†¬ª. Be happy :)
1451,Pretty cool thanks a lot
1452,"i had always understood this to be called ""matrix math"" and learned what he was explaining here in college algebra. i doubt i could do any of it now without a significant refresher though."
1453,"I don't understand the ""weight""

How u deicide it ? Do u assign them randomly ?"
1454,"""It works but u don't know why""

Well I will leave it to that"
1455,Áî±Ë°∑ÊÑüË¨ù‰∏äÁπÅ‰∏≠Â≠óÂπï
1456,it is more like a decoder. do not notice the neural network here)
1457,How easily you explain that I am searching for a long time. Excellent.
1458,"Absolutely love your channel, bro. :-)"
1459,I didn't understood who are the ones who disliked the video. May be they didn't understood the what is taught.
1460,Very good
1461,I want to like this video 10 times (for each digit)!
1462,"OH GOD YES
ON THE FOURTEENTH TIME OF WATCHING THIS VIDEO
I FINALLY GET IT
this is the best feeling in the universe"
1463,"Q U A L I T  Y   C O N T E N T
Thank you so much for these videos!"
1464,Êàë‰∏çÂÖÅËÆ∏Êúâ‰∫∫Ê≤°ÊúâÁúãËøáËøô‰∏™ÊïôÁ®ãÔºÅÔºÅÔºÅÂ§™Ê∏ÖÊ•ö‰∫ÜÔºÅÔºÅÔºÅÊàëÂì≠‰∫ÜÔºÅÔºÅÔºÅÔºÅÊàëÊÉ≥donateÔºÅÔºÅÔºÅÔºÅ
1465,"In minute 14:50, shouldn't the b's vector have indexes from 0 to k?"
1466,And amazing to know this is what our brain does constantly and subconsciously...the human brain is amazing...appreciate it
1467,The real question is...can it read a doctor's handwriting?!
1468,Have you really written the neural network's code or it's just an animation?
1469,"seriously, this is the first time i find that ML makes sense! you are amazing"
1470,2:14  video based neural network already exists. its called autoencoder and used for deepfakes ( i read it on wiki)
1471,"thank you very much you are doing a good job , i don't know why some people without any mathematic or computer science background  dislike such a great job!!!!"
1472,"opens the video
writes in notebook- oh,okay! neuron is thing that holds a numbers.


15 mins later..."
1473,Just Think how my Mind neurons have suffered in networking of this videoüòÇ
1474,Mind blown!
1475,So it goes down to: feed > elememt breakdown (simplification) > comparison > meaning????
1476,"Absolutely amazing video, such skills of the teaching!"
1477,"3blue1brown, you are beyond awesome. So thankful. :)"
1478,I am unable to understand the logic of assigning negative weights to the surrounding pixels. 9:58. I will be grateful if someone can help me with this. :)
1479,Stunningly made video.
1480,"Your whole work is so helpful, inspiring and Im not exaggerating when Im saying, valuable for whole mankind. The huge amounts of clicks you generate for your genre shows how outstanding your videos and your explaination skills are. Thank you!"
1481,ÂæàÊ£íÁöÑËßÜÈ¢ë6666666666
1482,Is there a way to use this process to recognize a agorythom? So if I feed it numbers every week. Eventually a pattern will be shown correct?
1483,Good series... Very well explained...
1484,Would e^x/(e^(x)+1) work istead of teh signoid function?
1485,You have such a charming way of teaching. Good job on the vids!
1486,I am amazed by how much room there is for education to be improved O_O
1487,best video to understand neural net on youtube
1488,"It is getting very deep and boring....
Start was good though, idea is also good.
Please don't have music in background, sometimes it's distracting.
Wish Youtube had multiple parallel audio tracks that users can choose to turn on/off this background music."
1489,Your videos are so good and educative that it makes me feel bad not paying you for having the right to watch them.
1490,"When you weight between 0 and 1, I'm seeing an analogy to quantum theory. Are there models that make a difference on discreteness of weights? Is a ""quantized"" weight the very definition of the artificial in artifical/cylon intelligence opposed to natural/human intelligence?"
1491,now I'm glad I took calculus
1492,You remind me of why I loved learning
1493,"Are you a god  am I in the heaven
Or this  some  high tech mit lecture  going on. YouTube give him applause üëè üëè"
1494,You're my HERO!
1495,Stuff like this is a great insight to how the human brain can have things like optical illusions.
1496,"Hi great video and highly informative. I am currently studying Neural Networks and have written my own Neural Network in Java. However, I wish to take it to the next level and implement image recognition. 


My question is: how can I obtain the grayscale value from a single pixel?"
1497,if RELU is simpler  and quicker than Sigmoid so why are we learning sigmoid function in this video?
1498,This video is amazing. Thanks a lot. You managed to show it better in 20 minutes than MIT videos in 40.
1499,wonderful explanation!
1500,Good job and very helpful
1501,Best explanation of the concept of deep neural networks in the history of humanity. Kudos!
1502,in 15:07 shouldn't the b matrix be like : [b0    b1 ...  bk]? instead of bn?
1503,The  video has a lot of unnecessary information. It would be nice if you can cut them off.
1504,Brilliant animation
1505,"It is a good video series that gives a lot of background and a rough understanding, but it is not possible to program a neural network from this.
I read some of the articles in the description and wrote a simple net in Rust converting 4 bit binary numbers into decimal numbers displayed on a 7-segment display with my son. ( https://github.com/etok414/simple_nn )

To help me explaining the mathematical details to my son (and as an exercise in Latex) I wrote a 6 page note about backpropagation.
The note treats the simplest possible network (2 input, 2 neurons and 2 outputs) in painstaiking detail.

Hope it'll help someone:
https://github.com/etok414/simple_nn/blob/master/latex/Back_propagation.pdf"
1506,Wow!
1507,"I ain't gonna lie, you, kind sir, are doing an awesome job! :) Thank you!!!!"
1508,well i found this lecture lot better than that of Andrew NG... much clearer and easier to understand.
1509,"Hey Mr. Grant , I am a great fan of yours....I've always found your 
videos so informative and clear..I want to connect with you whether you 
are on Facebook or whats app.....Please e-mail me if you are seeing this
 comment....

email id-   suraj2895@gmail.com"
1510,"Hey Mr. Grant , I am a great fan of yours....I've always found your 
videos so informative and clear..I want to connect with you whether you 
are on Facebook or whats app.....Please e-mail me if you are seeing this
 comment....

email id-   suraj2895@gmail.com"
1511,"Hey Mr. Grant , I am a great fan of yours....I've always found your 
videos so informative and clear..I want to connect with you whether you 
are on Facebook or whats app.....Please e-mail me if you are seeing this
 comment....

email id-   suraj2895@gmail.com"
1512,"Hey Mr. Grant , I am a great fan of yours....I've always found your 
videos so informative and clear..I want to connect with you whether you 
are on Facebook or whats app.....Please e-mail me if you are seeing this
 comment....

email id-   suraj2895@gmail.com"
1513,Great video. Very good visualization
1514,15:00 should the bias vector not be of length k?  (k x n)(n x 1) = (k x 1) vector
1515,7549115417p
1516,"Question: 14:49: should vector [b0,...,bn] (n-dimensional) not be [b0,....,bk] (k-dimensional)?"
1517,You are the best: you taught unteachable stuff in 19 minutes
1518,hi! what happened to this teasered video series on probabilities? ;)
1519,"Tr√®s beau travail malgres que je ne peux pr√©tendre conna√Ætre le sujet j'ai aim√© votre fa√ßon d'expliquer, mais laissez moi vous dire que tous ces programmes sont dangereux pour l'avenir. C'est utile mais ce n'est pas cel√† l√† vrai vie , l√† on tourne vers le chaos dans un futur proche  avec l'IA qui es am√©lior√© sans cesse. 
PS: Regarder le film Enigma c'est un bon film. D'ailleurs comme on peut le voir dans ce film les algorithmes servait √† nous d√©fendre √† pr√©voir maintenant il vont servir √† contr√¥ler le monde pour avoir le pouvoir ultime comme ""Dieu"" l'oeil et les oreilles partout et presque m√™me au del√† car tous ces calculs servent m√™me √† anticiper. J'avais besoin d'√©crire tout cela"
1520,10:15
1521,"Man, you are the best teacher I have ever come across."
1522,thank you for the tutorials
1523,üëé Do you also have a version of this video without the pointless piano noise? It's absolutely useless.
1524,"üëé Thumbs down from me from the super annoying  background piano while you are talking. Unfortunately, you are not the only one. There are videos about physics which also use this stupid background elevator-style music when someone is talking. WTF.  Whoever artist thinks this is necessary, curse you for ruining an otherwise great video."
1525,Thanks for simplifying this :)
1526,"so i‚Äôm colorblind, (duetranopia, occurs in around 10% of white males) so the green and red colors weren‚Äôt really distinguishable for me around the ~10 minute mark. just something to think about in the future when making videos that need color"
1527,This is an amazing explanation that helped me a lot... Thanks from a german engineer!!!
1528,14Ôºö42 there is a mistake.bn--->bk
1529,10:50Î∂ÄÌÑ∞ ÌïúÍµ≠Ïñ¥ ÏûêÎßâÏù¥ ÏïàÎÇòÏôÄÏöîÎπ®Î¶¨ Í≥†Ï≥êÏ£ºÏÑ∏Ïöî
1530,3:35 take 10 seconds to appreciate how awesome this animation is
1531,"I am a musician and artist. I am GED 6 months of technical college experience. I want to tell you I'm starting to grasp this little by little. I want to learn this. I have been catapulted into this yearning /craving for learning about machine learning via meditation and some. B+ Psilocybin Cubensis experiences. With that said, thank you for sharing your talents and work. I will continue to watch these over and over. Why? I think b/c I'm a creative person and this is creativity. Do we grasp the importance of creativity? In all it's malleable mediums. An individual or group passionate creative process then transcends time and space to connect with the recipient becomming part of their life. Our lives! Traveling some unseen conduit. Landing in our lap's like heart ache and hand grenade's. Art, music and ALL forms of creativity outdate religion, language and government. It's universally personal and the sinew that binds humanity. #ThIsIsNoTThEaLgOrItHiM on YT the Wellrose Hummingbird playlist for my music and slippery.sliding.slope on IG for art. Which I've been using images of neurological images, sound wave images etc to layer my art. Here's a song
Everything is Aligned https://youtu.be/Ftiw0rUq4es
And a song I wrote about nature and forgiveness
https://youtu.be/oWFk98pY-e8
üå¨Ô∏èüçÑüß†‚§µÔ∏èüï≥Ô∏è.          üå¨Ô∏è ‚§¥Ô∏èüï≥Ô∏èüë•
                    ‚ú®.              ‚ú® 
                     ‚ú®.            ‚ú® 
                       ‚ú®üå¨Ô∏èüì°‚§¥Ô∏è"
1532,"I remember watching this awhile ago, and after watching this again after watching VSauce's video on this on Minefield, (It's free now btw) this makes a lot of sense now and makes it seem a lot nicer."
1533,14:46 it should be b0-bk vector
1534,"I'm Japanese. But, I can understand because your videos have Japanese subtitles. Thank you very much( ^ œâ ^ )"
1535,anybody knows how to download these amazing videos?
1536,"Why are there n bias terms? If each bias is associated with a neuron in the hidden layer, and there are k neurons in the hidden layer, shouldn't there be k biases?"
1537,Can You Please Create a Whole Series on Math behind All Machine Learning Algorithms. we really need this.
1538,"so if its linear algebra, its like graphics processing, which is why they use GPUs ?? nice!"
1539,Could I use a neural network like this for time series prediction?
1540,I don't know what the fuck did I stumble to. But ill study the shit out of this field and see where it takes me.
1541,This just made it all click for me! Thank you!
1542,"Great Video <3
Explain in a very intuitive way and I love this :D
PS. I wondered why (12:30) : (784x16)+(16x16)+(16x10)+(16+16+10) why not (784x16x16x10)+(16+16+10) ?  sorry for my dumb question"
1543,"it should be w(1, n) in the matrix 2nd row by the way and w(k, 0), w(k, 1), .., w(k,n) in order to dot with nx1. Thanks, great video, will send it to anyone interested in NNs and deep learning"
1544,Thank you :) This video really helped me alot!
1545,Please make full lectures on deep learning  we all need
1546,"so neural network is actually a function and choosing right values for its 13,000 parameters ?
and for each set of problems these parameter values will be changed?"
1547,You are the best! And Squishification lol üòÇ
1548,I'm a 14 years old non english fluent and i understood everything. Is it normal? Is there anyone here like me?
1549,"If bias differ between each layer, does that increase dimensionality? Are biases trained as well?"
1550,thx
1551,Bardzo ciekawe .
1552,the way this woman talks hurts my ears
1553,how is it possible that I can lie in my bed on a Sunday and am presented with mind-boggling cutting edge knowledge told by an incredibly soothing voice in a world class manner on a 2K screen of a pocket supercomputer basically for free
1554,Hmm.
1555,"Dude, this explanation is facking amazing, thanks for it)))"
1556,I watched this video a while ago it's very good.  Having watched a lot of other videos on your channel I think you could do very well explaining neural networks as differentiable tensor operations more generally
1557,I can't wait for neural networking to be able to recognize my doctor's prescription.
1558,great way of convincing people to subscribe ! LOL
1559,"15:13 LOL  ! 
BTW great work boss ! idk how much work would have gone in making a quality video like this !"
1560,This is where need for quantum computers kicks in
1561,"Hi, how do you recognize the edges of the font?
Do you recognize automatically, or, have they been divided before?
Thank you
ric"
1562,Please make more videos about machine learning and data science. I wish I could have a teacher like this at the school.
1563,Whoo it is a very clear explanation!
1564,"Leisha is hot. If you had babies with her, they would solve the universe. So get on it!"
1565,Is this the Reuben Report?
1566,"Hey , I just have one small question
The neuron which we were discussing above as a function is basically a perceptron rt??"
1567,Still people thinks they are created by an accident... Just think of your mind and learning capabilities....
1568,Thanks this helped much more than my class there was just a whole bunch of equations on the slides -_-
1569,"Would it be possible to make a software OOP body with multiple AI embed in the body class with every parts of the brain compartments for objects with a neural network function in each objectÔºücould an AI or neural net be used to automate the population of the nodes in the parts of the brain compartments objectsÔºü
weights and biases sounds like opthamalogy theory in linguistics."
1570,"Neurons most common bullying phrase:





""You just dont have any value to me!"""
1571,"Please watch Robert Sapolsky's ""Behave"" lecture  it gets into the two methods of how the visual cortex processes images. There's a cognitive method but it's slow, and a fast method which is really lighting fast and in accurate as heck. not something that you want when someone is running at you during a riot carrying cell phone."
1572,"Brain: ""I'm a stupid human beeing..""
Neuron: ""Hold my 0 to 1!"""
1573,why 16 weights?
1574,"Love the video. Also loved the idea to depict weights as red / green pixels. My eyes are sore though. The problem is that I am colorblind, green tries to elude me. Please try to colorblid-proof your videos :)"
1575,Well that escalated quickly
1576,n√£o entendi vo me mata
1577,The thumbnail is moved when i scroll down fast
1578,"lindo, incr√≠vel, bonito e gostos√£o"
1579,"–ß—É–≤–∞–∫, —É –Ω–∞—Å –≤ —Ä—É—Å—Å–∫–æ–º –µ—Å—Ç—å –ó.–ò —ç—Ç–æ –Ω–µ —Ü–∏—Ñ—Ä–∞. –¢–∞–∫ —á—Ç–æ –Ω–µ –ø–∏–ó–¥–∏ –ª–∏—à–Ω–µ–≥–æ."
1580,use the property of trigonometry about the language symbols so we got the basic idea about any symbols
1581,use coral draw and photo shop  for  recognise the picture
1582,if artificially intelligence used all the equation of jyotish from India then we easily produced that structures which gives actual intelligence to machines like it gives an average person to think about which is not recognise by anyone but it's going to be happened in past or future
1583,we use factors of planet's and astral body so we used that in predictions of randomness about anything in the universe
1584,artificially intelligence gives same pattern as we practice jyotish in India
1585,Thank you so much for Japanese subtitle.
1586,Sorry but this method seems archaic to solve AI
1587,–±–ª—è —è –∂–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –Ω–µ –ø–æ–Ω–∏–º–∞—é. –∑–∞—á–µ–º —Å—Ç–∞—Ä–∞–ª—Å—è –ø–æ–Ω—è—Ç—å –≤–∏–∂—É–∞–ª. –Ω–æ –≤—Å—ë –∂–µ –º–æ–∏ –Ω–µ–π—Ä–æ–Ω—ã —Å–º–æ–≥–ª–∏ —á—Ç–æ —Ç–æ –ø–æ–Ω—è—Ç—å. —Å—Ç—Ä–∞–Ω–Ω–æ
1588,My doctor's prescription would be the best test for the Neural Network
1589,Fantastic....developed intuition
1590,"Ben:ne?
Beynime:bak g√∂rd√ºn m√º
Artƒ±k daha fazla deƒüer vermeliyim n√∂ronlara"
1591,Thanks for this awesome content!!
1592,"So it's like ""Match the following "" kinda thing right ?"
1593,Best frickin' videos on complex things. You're an amazing teacher.
1594,How else would we read doctors writing.
1595,"Sir can you please include videos on other machine learning algorithms,image processing and NLP..it will be very useful for people like us who are enthusiastic to learn these in a conceptual way."
1596,*Captcha has left the chat*
1597,Best tutor on Youtube
1598,"@0:56 the video says ""and outputs a single number between zero and ten"".  It should say ""between zero and nine"". -Annoying commenter."
1599,Thank you! This helps a lot and it also opens up sooo many questions!! Guess that means it's on to the next video :)
1600,Best video about ANN i have ever seen... so clear and precise! Good job üëç
1601,Wow I love you!
1602,Didn't understand the weights part. What are they for?
1603,Thank you for all your awesome videos! I have learned so much about mathematics via your visual representations. Keep up the awesome work. I have started a YouTube of my own now. I Just uploaded my very first video! üòäüëç
1604,We're GODS.
1605,What is deep learning? A branch of pseudoscience.
1606,Ïù¥Ï†ú Í∏∞Í≥ÑÌïúÌÖå ÏÇ¨ÎûåÏù¥ Ïù∏ÏãùÌïòÎäî Í≥ºÏ†ï ÎπÑÏä∑ÌïòÍ≤å ÌïòÎèÑÎ°ù ÌñàÏúºÎãàÍπå Îã§Ïùå Ïù∏ÏãùÍ≥ºÏ†ïÏùÑ Ï∂îÏ∏°Ìï¥ÏÑú Ïö∞Î¶¨Í∞Ä Î∞∞Ïö∞Î©¥ ÎêòÍ≤†Îã§.
1607,YOU ARE AWSOME! I saw 10 videos to find what they are. I simply understood by watching the first 7 minutes of yours.
1608,"I have created a web application that supports your learning process in the field of neural networks.

https://blueneurons.ch/nn

You can play around with the settings (architecture, activation functions, training settings) and observe how the settings affect the predictions. All datasets have preconfigured values that can be adopted. It is also possible to create your own datasets.

Instructions and explanations to the implemented elements:

https://blueneurons.ch/pdfs/userguide.pdf"
1609,"I'm not really impressed, if this is what ""Machine Learning"" is - this is algorithms and divide and conquer style allows computers to 'Read' hand writing, there is a name for it - *OCR* which is technology that has been around for over 10 years now (this is used in Scanning Software). So are you SURE this is machine Learning?"
1610,Good afternoon Mr.Patreon sir and thank you sir for your good teaching on neural network and so many good things.
1611,https://www.youtube.com/playlist?list=PLvf_ZGHboMr9DZ750qpW3G1dCKpXkDpvv
1612,"I feel like someone stole this and made ""lets make my home town into a brain"""
1613,"a 2D Neuronal Network, lol. 3 D Chips are still out of the question, because of heat problems."
1614,"What you say is how we view and recognize a given digit, is not how we do it.   I could explain it here, but there are plenty of good books on the subject."
1615,After watching the entire video just the thought of recognising objects in images blew my mind. Never appreciated image recognition more.
1616,Á•ûÁªèÁΩëÁªúË∑üÁ•ûÁªèÁ≥ªÁªüÊúâ2ÊØõÈí±ÂÖ≥Á≥ª
1617,this was some goddamn flame
1618,"Regards, awesome explanation."
1619,Ï§ëÍ∞ÑÏóê ÌïúÍ∏ÄÏûêÎßâÎÅùÎÇ®
1620,"If you have multiple layers with equal number of nodes, then several sets of weights can give identical results. With two hidden layers of equal number of nodes, one can permute the weights of the first layer, and give the inverse permutation on the next layer.  Is that a useful property? I'm new to this except for an incomprehensible lecture I attended 30 years ago. This permutation symmetry may explain why you don't see the little pieces of numerals in the first layer you got. The solving mechanism is blind to the fact that permutations could give the same results. There will be however one particular permutation (or a few close ones)  that will allow you so see the bits. But this will be a few  in 16 factorial."
1621,„Çè„Å£„Åã„Çä„ÇÑ„Åô„ÅÖ„ÅÑ
1622,... wenn jeder mit jedem vernetzt ist - und so in k√ºrzester Zeit untereinander Infos ausgetauscht werden k√∂nnen.
1623,"It is an awesome explanation. Thanks so much! One thing to correct, i believe the bias vector is till Bk instead of Bn."
1624,üóΩ
1625,"How could anyone downvote this? Bar none, the best neural net intro vid series ever."
1626,Very nicely explained
1627,"i understood everything
but i am having feeling that i just don't remember anything üòÇ
is it normal?



this is my first time watching a video from 3blue1brown.
and i just become a fan. thanx! üòä"
1628,Amazing
1629,Il campo elettromagnetico umano e' come quello del pianeta il cervello come universo
1630,"Will you please suggest me a book to learn machine learning, please?"
1631,"Hey, werent the beginning test sample numbers used to train an Neural network made of a sheet of glass ?"
1632,That's certainly not how the brain and its neurons work but human is creating a way to convert things into numbers and numbers into another meaning number. That's good for machine to process data but I'm not sure if it can become a new intelligence or not. Let' s see.
1633,"You're a god, Grant."
1634,The son of Lisha li and sander would've IQ of 300.
1635,number of spots in the hidden layer shouldn't be more ??
1636,"Awesome video, really fascinating!"
1637,thank you! another great video!
1638,Mother nature outdone herself when she designed the human brain... its so complicated that it doesnt even understand how it works LOL
1639,this is V E R Y  good !!
1640,"I am studying Computer Science and this video has really improved my understanding of how Neural Networks actually work. 



Thank you :)"
1641,Now we got Deep Fakes thanks to the Artificial Neural Network technology!  Get ready for mass confusion and possibly World War 3.  THIS IS ANOTHER CHINESE REVOLUTION!!!  Now you don't need to work so hard to pay the high fees of the Screen Actor's Guild and go to film or acting school and try diligently forever to be a productive actor!  This is the easy way to become an instant celebrity and acquire fame without ever being associated with a film studio!
1642,"Great video, brings a lot of light into the topic in an easy way to understand. Never seen MNIST so great explained! Well done!"
1643,i am confusion
1644,Thank you!
1645,This is the best 101 video about ML and NN!
1646,Your channel name and symbol reminds me of the Stargate SG-1 episode with the Tobin Mine combination lol
1647,"Hey Grant! Love your videos!


I was going through this and piecing together the examples myself and I think I found an error in the video. At around 14:46 in the video, I believe the bias column vector should be dimension k by 1 instead of n by 1 as you express in the video. After multiplying the weight matrix (k by n) by the activation matrix (n by 1) the resulting matrix dimension should be k by 1.


Hope you continue to output fantastic content :)"
1648,"I actually had to watch almost three times to understand the video. But, damn I felt good after understanding. Now that I am taking AI classes at university your videos are like Amrit for me."
1649,Very good description.
1650,NONSENSE !!!
1651,Living things do not recognize or process images in this way.   What you have here is nothing but a logic tree.  It does not learn.  The programmer learns.
1652,"Thank for video, Thank for Vietnamese sub, I love @3Blue1Brown from VietNam"
1653,God blesses you! Great Explanation!
1654,This is a fanstastic video. Wow. Subscribed!
1655,17:33
1656,I learned enough mathematical logic/probability theory to implement an entire AI and not a single Neural Network was used. This was hard but rewarding. You can get alot done with just logic and probability haha.
1657,Thanks man. I'm gonna use this in a seminar I will be doing today.
1658,Nice explanation... thanks
1659,"@3Blue1Brown I don't know if I'm getting this wrong or if this has been mentioned already but isn't the bias vector you show at 14:38 supposed to be a k-row by 1 column (k x 1) vector, not an n-row by 1 column (n x 1) vector? The dot product of a (k x n) matrix and a (n x j) matrix is a (k x j) matrix. You would be adding a (k x 1) vector to a (n x 1) vector, which wouldn't work unless n = k."
1660,Is the value -10 as bias arbitrary? What would be the consequence if the bias value is different?
1661,Your content is amazing. Please keep it up! :)
1662,Amazing video.. great job!!
1663,I just started with machine learning and all this is a little overwhelming....is it just I or everyone feels like that when they start???
1664,"David Hume, an english philosopher said once that our brain can put the missing pieces out of the blue, due to the gradient perception. So, machine learning must work like that, creating that gradient in order to understand what is the last value of a form that can differ a regular number form from another form of the same number and both from a new one."
1665,"Love the format and insights of this video. Maybe I could even use this to explain to non-technical clients the reason I might want to use a neural network for binary classification. I think you cover most of the mathematics behind neural network operation, as you usually do. The only thing I would change is comparing a standard neural network to the biological brain. I would say that it‚Äôs more of a closed loop control system. The name ‚Äòneural network‚Äô betrays itself, kind of."
1666,"This video series was very helpful, thank you!  But I have a question about the output function.  When training for 10 output digits from the inputs, why would there not be at least one more output for 'None of the above'?  In other words, why wouldn't I be able to give the neural net the option of saying that the input set doesn't correlate to any of the possible outputs?  Wouldn't forcing it to choose one even when none of them make sense cause unnecessary complications in the training and intermediate calculations?  Or does providing such an option cause the whole function to collapse because it would 'want' to choose that output almost every time?"
1667,Wow. Most comprehensive and well made video on neural network I've came across.
1668,"@ 14:18 I was thinking I have seen this formation before then you mentioned linear algebra, and as a matter of fact my lecturer once mentioned that this approach could be used to split signal instead of Fourier transform, using eigen vector or some sort of unitary vector form... This is really a great video just listening to it alone (even if not understanding much of the ideas) is satisfying...."
1669,"I've recently started learning machine learning and AI and this video is really the best explanation I've ever gotten of how a neural network actually works, thank you so much for this information!"
1670,"Great video
at 14:41, should not bias vector be  [b0, b1, ... ,bk] instead of [b0, b1, ... ,bn] ?"
1671,"You are a great mentor,sir"
1672,"Awesome video man, that helps me a lot, thanks!"
1673,"THE TEACHING ASIDE , THOSE GRAPHICS MAN! TAKES  LOT OF EFFORT!"
1674,"I really don‚Äôt know what the fuck I‚Äôm watching, but take the like"
1675,"Devo dire che e la migliore spiegazione di una rete neurale che io abbia visto. Postane altre ti prego, Grazie mille!"
1676,"In think the Vietsub team may use Google tranlate or they born or lived to long outside VN,because the structure of sentences weren't at right position so I have to read Vietsub, Engsub use google translate and guess. LOL by the way thank for your video! It's a good inspiring video."
1677,i named my channel black and blue (black was my creativityüòÇ) when i watched your video first time in my high school. sadly i didnt get any grasp what you were telling but felt like oh man i m seeing mathematics. Now i could understand what you were trying to say back then.
1678,"By doing things this way, such as that shown for recognizing numbers, aren't we already giving the computer the answers?
For example, when you spoke about a section of the neural net, you showed that it ( The numbers ) gets broken down so that the computer knows what to look for, the circle in the nine, the straight edge of the number nine or one etc.
The computer is then looking for those parts when looking at a number.
At the end of the neural net, it takes the largest number ( Output ) to decide what number it sees.

By putting information is, as to what to look for and compare them, then we are in effect, giving the machine the information to search for.
We as humans, however, have to learn what makes a nine a nine and a six a six because it gets explained to us by outer parents and other adults."
1679,This video explained neural networks much better than my grad school professor.
1680,you are talking about cnns lstm better
1681,Brilliant videos folks. Thanks a ton for the simple explanation of complex topics.
1682,"I just picked up ""Squeashification"" from this video!"
1683,"Your channel is amazing, my favourite on youtube.¬†

Do you plan on doing any course on machine learning, data science, or something related?

Thank you so much."
1684,Correction: 14:48 The bias matrix should be indexed from 0 to k and not 0 to n.
1685,Would you make a video on Traffic Modelling?
1686,Wow... thank you very much sir... I feel so happy watching the video :)
1687,Horrifying
1688,ÌïúÍ∏ÄÏûêÎßâ ÏûòÎ†§Ïöî ÌùëÌùë
1689,One of the best Videos i have come across. Concepts explained so well in layman terms.
1690,They ruined all system. They making own command machinen to machine.
1691,Natutoo na sya ng kanya. Parang lobong nabitawan ang tali wala ng control
1692,A neural network brought me here.
1693,"You explained it so simply, just wow üòÆ 
One of the best things youtube has showed me at 5am üî•üî•"
1694,Î≥¥Î¨ºÍ∞ôÏùÄ Ï±ÑÎÑê
1695,12:26 :0
1696,"I have loose plans for my future, but if everything goes right..I'll become a psychologist that also codes. Dunno how I can do both of these things, but I'll figure it out."
1697,Ditch the background music. It's annoying.
1698,DAMNN BRUH IS BLOWING MY MIND HARD AF
1699,"fascinating, but jebus i think designing the AI that designs the AI to do this would be simpler"
1700,thank you
1701,that moment when you realize that a computer has more neurons then u
1702,good content
1703,"There is one person who does it better than Andrew Ng, ladies and gentlemen, Grant Sanderson!"
1704,"Too sophisticated. I'm just here to waste time and bandwidth. Haha
But great stuff!!!"
1705,"–Ø –Ω–µ –ø–æ–Ω—è–ª, –ø–æ—Ç–æ–º—É —á—Ç–æ —è —Ç—É–ø–æ–π."
1706,this video should win oscar.
1707,i regret that i have only one subscribe click to give
1708,What i understand was 28√ó28 is 784
1709,"so virtual, so good"
1710,The only way I am going to love maths is looking at your video... someday could you throw a video on data structures in computers. all the sorting algorithms etc. It would be totally awesome
1711,Thanks.
1712,Excellent
1713,Finally an explanation for the bias. THANK YOU
1714,Why Japanese caption is unavailable...!
1715,......
1716,We all know that machine learning is just bunch of 'if statements' *programmers rise up*
1717,The worst translate ƒ±‚Äôve ever seen
1718,Thank you Grant! Another great series!
1719,Videos like this make me wonder how could our brains have evolved randomly.
1720,I wish channels like this existed when I was in university. I took me a second go to pass A.I because the books and lectures were just so damn confusing.
1721,it interests me a lot but at the same time not understanding anything: someone who has a channel or a video that takes this kind of things from the beginning so that I understand little by little please ?
1722,"Hey, This is a great video but please put more emphasis on ""weight'. I had to look it up from another source. Besides I understand it was a complex topic oversimplified. Thank you (:"
1723,"coming with curiosity, leaving with confusion."
1724,This is what students must see to get motivated in math
1725,This video is extremely helpful
1726,"Bio teacher: what is a neuron?
Me: a thing that holds a number between 0 and 1"
1727,Mordecai: Blue jay and rare phenomenal teacher.
1728,"nice thank, it great"
1729,mom i just found GOD
1730,Epic tutorial. I was struggling to understand the activation function.I now completely understand.
1731,"You can't actually shame the computer into learning. Punishment is ineffective. Instead, you must reward it. Give it a hug or a lollipop when it recognizes a digit correctly."
1732,"All that knowledge and math skills surely work seductive. Alphazero proved that all can be done by larger network with random paths and maybe more processing, but infinitely less work from human/engineer."
1733,Korean subtitles are gone after 11:30
1734,Gr8 Video!  Made this topic interesting and understandable.
1735,"Error 19:00 dimension of B equal (k, 1) ;) great job otherwise!"
1736,a video on AlpaGo and the algorithms behind it!!
1737,"I am just wondering..is it possible that machines have their own logic to infer and discriminate philosophy and law, debating with human? (which means they get souls and consciousness)"
1738,"wow...i'm listening carefully and gleefully to your subscription request and patreon thing and the sponsors thing....what?
you explain so well...very creative.."
1739,my brain hurts
1740,laying on your side and reading subtitles is not easy like when your are vertical
1741,Beautiful and informative channel
1742,"Now I‚Äôm good at math for my age, and I came here just out of curiosity. And I makes me wonder what the probability is that I‚Äôll come across a teacher that will make me watch these videos. Maybe someone should make an algorithm for that‚Ä¶"
1743,well done
1744,this video needs more korean transition :-|
1745,I need a cookie. My brain is fried.
1746,"If you really wanted to simplify the code, you can always just write it in APL. I think it's a one liner üòà"
1747,"Is this a CNN, (Convolutional Neural Network)?"
1748,"For anyone looking into comprehensive algorithmic analysis via intuitive constructs, there are MUCH better ways of doing so than NNets."
1749,"The way I understood this:
It is just a function that eats image pixels and poops one digit number.
The way I understood neural networks:
It is a mechanism that receives data as an input, and by using incremental patterns it can get a probability of the result, based on a database of patterns which were build up by machine learning."
1750,your explanation very simple and anyone can able to understand Ai subject.    thank you master
1751,"0:56 Has anyone ever noticed he says ""a single number between zero and ten"" when he meant ""a single digit between zero and nine""?"
1752,Makes it sounds like you'd need a neural network just to design a neural network.
1753,"re: Sigmoid vs ReLU:

ReLU is bound to be a whole lot faster than the sigmoid function.  Since this is happening over and over and over and...  Well, if nothing else, it's going to speed things up tremendously."
1754,literally fantastic visualization and superbly explained.
1755,"I've been studying this for just a little while , and understood some of this conceptually but not close to the level that I do after watching this video... I actually thought we had to adjust the weights ourselves lol... classic Matt over thinking things :) you truly are gifted at teaching"
1756,"*RECAPCHA WOULD LIKE*
*TO KNOW YOUR LOCATION*"
1757,beautiful gfx
1758,"I can't understand what the 'weights' signify.
Why are they here, what do they stand for?
Also why is the 'bias' needed, what does it signify?
I am really confused...plz help"
1759,Wow i have to choose two elective subjects and neural network& deep learning is one of them.Now after watching this i am thinking of taking it.
1760,"Please, correct polish subtitles - they are horrible wrong and misguiding!"
1761,10:51 Î∂ÄÌÑ∞Îäî Ïôú ÏûêÎßâÏóÜÏùå „Ö†„Ö†?
1762,The animation done for this video is pretty amazing.  I could imagine the painstaking time to build it...
1763,"God. I came in knowing the basic concept, but not the execution as a programmer, hoping but not not entirely believing that I would come out understanding it so well, but somehow you did it. This is the best mathematical concept explanation channel I have ever seen. You consistently surprise me with incredibly educational videos."
1764,Conduttore di energia cosmica ha intelligenza artificiale connessa a tutta l'esistenza
1765,i need full korean subtitles  to learning this concept.
1766,"This reminds me of those Neural Network videos that Cary Huang makes, roughly because they both are explained similarly."
1767,"Can't thank you enough for this series. You make it seem so easy to understand and at the same time very interesting.
Not trying to nitpick here, but shouldn't the number of elements in the bias vector at 14:40 be equal to the number of neurons in the second layer and hence equal to 'k' instead of 'n'?"
1768,Such nice videos!! Thank you :)
1769,the switch to linear algebra was well done: a^(1)  = sigma(W*a^(0) + b)
1770,this must be frustrating to people who had to learn this from terrible textbooks
1771,Amazingly  easy articulation of most difficult topic to explain.
1772,"The squishification function?

I'm going to apply that phrase in context as and when necessary in everyday situations from now on.

Thanks ‚ò∫"
1773,Korean subtitles stop at middle
1774,"Bravo, very visual, simplified and clearly communicated introduction to Neural Networks!"
1775,"You said the output was ""from 0 to 10"" but the visuals were from 0 to 9. ;)"
1776,"ÊãúÊâò‰∏çË¶Å‰π±ÁøªËØëÈ¢òÁõÆ„ÄÇ„ÄäÂèçÂêë‰º†Êí≠ÊºîÁÆó|ÈôÑÂΩïÊ∑±ÂÖ•Â≠¶‰π†Á¨¨3Á´†„ÄãÁöÑÂéüÊñáÊòØ‚ÄúBackpropagation calculus | Deep learning, chapter 4‚ÄùÔºå‰∏≠ÊñáÊòéÊòéÊòØÂèçÂêë‰º†Êí≠ÊºîÁÆó|Ê∑±Â∫¶Â≠¶‰π†Á¨¨üëâ4üëàÁ´†„ÄÇ‰∏çË¶ÅËá™Â∑±Èöè‰æøÁºñÁºñÂè∑„ÄÇ"
1777,Can't you just use a weighted average rather than a sigmoid function?
1778,here i am... learning about neural networks at 4 AM
1779,"I found a tiny error in this video. The vector with all the biases at 14:38 should have k elements and not n (b1 to bk, a bias for each neuron in the second layer).

I really appreciate what you're doing and enjoy every one of your videos. Thank you for making them (the animations are superb)."
1780,Thx you.Best videos and best channel :)
1781,"üìÅ documents 
  /üìÅfiles
   /üìÅ New folder
       / Nothing is here so f* off"
1782,It's scary now how my brain would ve been working shit much more complex out for 17 years and maybe more .... What a waste I am to my freaking brain üòÇüòÇüòÇ
1783,WHY TF DID U CHANGE '*IS*' to 'is'???? REeeeeeeeeeeeeeeeeee
1784,https://www.youtube.com/watch?v=rA5qnZUXcqo Vsauce did the similar work except it's not about deep learning but focused on how they work
1785,"why do we need to add weights to find  whether an edge exits in the given region? 
we could use the activation layer from our input 784 neurons to find whether the given region is lit up(i.e has activation ) if there exists an activation then we can conclude that yes that region has an edge .
why do we need weights to determine that other than solely activation number?"
1786,Lmao why does this video have any dislikes?
1787,This is great stuff took me three to get a hold of it the more u watch the clearer it gets
1788,thanks sir
1789,I just knew of that Relu function as the ramp function.
1790,"There is increased use of 'Editing 'out the normal 'Breath pauses' that occur in human speech!! What this does, makes it MORE difficult to process the spoken information.  Thus it diminishes the effectiveness of what is being taught or explained.
I think this is being used more and more, in an attempt at efficiency and reducing time spent etc. But I believe that this is psychologically counterproductive! Do you copy? Over!"
1791,OMG GRANT KNOW EVERTHING
1792,"Since the end value has to be between 0 and 1, that means the neurons on the final layer have a sigmoid function. Does every neuron in the whole network have a sigmoid function?"
1793,great video!! thank you!!
1794,"Ah thank you so much for this enlightening video my friend! You see I like to use this software for the ps4 entertainment system called Little Big Planet 3.

LBP3 allows you to create and share things It is marketed as a game that allows you to create and share games for people to play. You can even allow others to edit the levels and contraptions you make.

Anyway it functions on utilizing realistic logic tools such as inputs/outputs,or gates,and gates,selector switches and all orher sorts of logic. 

For a while now I've been wanting to make some characters that actually learn. Specifically the environment around them(which I'll build from the ground up) and objects in the environment. 

""Is this a solid object or can I pass through it? Can I be harmed or should I be alarmed by this object or not? Is it being solid or not a positive or negative and why is it a +/- state?""

I was thinking of building some kind of memorizing profile system based off of xp. I hadn't thought of doing an actual Neural Network. 

Glad I came across this video. It was very informative and may have to check out your other vids. By the by it seems like you enjoy designing and creating stuff. If you've never checked out Little Big Planet 3 you should check it out. Someone with your kind of knowledge would be able to create all kinds of interesting experiences to share!"
1795,So how many neurons would you need to have a program learn to play pubg mobile? How would one go about doing that?
1796,Awesome üëç
1797,Board of Ed = Bed..... zzzzzzzzzzz or Don't...  #PICKSIDES
1798,PI is just another LIE   just Look at the #sequence   There in no pattern there..
1799,Another Amazing video - You're Great ! Thanks for this video.
1800,Cool! <3
1801,You are the best man!
1802,I almost understood it this time!!!!!!
1803,"At 13:55, why don't we write w^(k)_n, like we write a^(k)_n? The fact that it's written as w_0,0 makes it cumbersome."
1804,"Wenn du schon den Titel auf deutsch hast , dann solltest du auch auf deutsch erkl√§ren. Entweder willst du ein deutsche Zuschauer haben oder nicht. Also schreib es auf englisch den Titel sonst werden deine Zuschauer nicht um was es sich handelt."
1805,How do we assign the weights?
1806,Quadratic equations brah.
1807,i quit watching
1808,Great Job! Thanks a Lot!!!!!!!!!!!!!!!
1809,"This is my very first time commenting on a YouTube video, and it's just to say: This is the best explanation of anything ever."
1810,He explained this so clearly I thought I was high as fuck for a minute
1811,"Love the video and its explanation, and for a layman‚Äôs perspective, it makes understanding the concept of NNs very easy. The only criticism I have is that (at a higher level) your models could‚Äôve been improved and more of the network‚Äôs problems avoided by ‚Äúnormalizing‚Äù the dimensions of the numbers. I.e. if someone wrote a number that is only 70 pixels wide, and you had a standard of 100 pixels, you would expand the drawing of the number to be 10/7ths as wide as it was originally drawn so that it stretches from the 0th pixel to the 99th. The same would go for the height.

This is very pedantic, and has less so to do with AI and NNs than it does with implementation and how to make your NNs better behaving. I‚Äôm sure if you had done something along those lines, the NN would‚Äôve had an easier time recognizing ‚Äúregions‚Äù and piecing together subshapes of digits"
1812,Is simpilist even a word bro
1813,I‚Äôve never respected any teacher more than I respect you
1814,Bye bye captcha!
1815,Thank You !  really YOU ARE AWESOME
1816,"This is honestly amazinglu explained, thank you so much for this"
1817,Am I completely wrong or nobody noticed that in 14:40 the bias vector indexes must go from 0 to k (the number of elements in the second layer)???
1818,I had no idea what a neural network is!! I still can not explain it but this man has done a brilliant job of taking something totally foreign and putting it in the grasp of me having a high level understanding of Neural Networks.  Thank you so much
1819,"Omg you're the best teacher ever, I'm always amazed how you excel at making complicated subjets easy to understand using those wonderful animations. The best math channel ever <3 pls make more videos on the subject or other machine learning related stuff. A playlist on GANs would be the dream. Love your work, cheers from Brazil. :))"
1820,I don't understand shiit
1821,ÌïúÍµ≠Ïñ¥ ÏûêÎßâÏù¥ Ïôú Í∞ëÏûêÍ∏∞ ÏûàÎã§Í∞Ä ÏóÜÎàÑ „Ö†„Ö†
1822,"We can't get any simpler...
Thank You...‚ò∫"
1823,Im hiring someone to do this with some digital documents
1824,Incredibly well done. Amazingly articulated teacher and speech. Thank you!
1825,"You just made me feel smart! Thank you 
Subscribed!"
1826,Lol I'm re-watching these as I prepare to work on my masters in Neural Networks. LOVE THIS CHANNEL!!!
1827,"amasing explanation!!
good job on breaking down each part and the images really help to visualize and  understand them.
just an awsome job"
1828,Lan delircem ne uzattƒ±k√ßa uzatƒ±yosun be bƒ±rak bune
1829,"Is the neural network a complex form of if else tree? Like in the second layer, if the image has sharp edge its neuron 1, if it has curved it is neuron 2. Is it something like this?"
1830,"It's like I'm starting to understand, it's easier than what I was thinking"
1831,could you please make a series of AlphaZero and how to generalize it on common games. I sure will donate.
1832,"@3Blues1Brown, no doubts on content part, in addition to that, your way of explanation is mind-catching. Greet vocal and content."
1833,"Your Brain know all of those are '3' , same way  when you see a female with different pixel sizes....üíÉüëØüßòüßú"
1834,Excellent video!
1835,"Just out of. Curiosity, what is the animation/ graphic tool used in the illustrations?"
1836,In schools everyone taught us to practice maths but this man teaches us to imagine maths
1837,Good Presentation
1838,I didn't get anything after 8:40
1839,"I believe the length of the bias vector at 14:39 should be ""k"" (the number of neurons in the new layer), not ""n"" (the number of neurons in the previous layer)."
1840,"Is combining edges into patterns actually redundant, as you can determine the digit from the combination of edges alone?  It seems like combining the edges into patterns is mostly a wasted step (for things like numbers and letters, less so for real images, unless an image can be described in terms of it's edges alone - for full images I'd actually go for edges and colour separately to avoid redundancy of the same edge with a different background colour, as colour only needs one value being stored, compared to all of the values representing edges as a pixel map).  I'd guess the network does store the information in the way you state, just that the weight map is a scrambled version of the weights representing the learnt edges.  Maybe if you were to draw the weight map in a different way, you'd end up with 784 images, each one representing one of the learnt edges, or rather features; pixel 1,1 = neuron 1 weight 1, pixel 2 = neuron 2 weight 1, pixel 3 = neuron 3 weight 1, etc for 784 (EDIT: actually 16, a mistake) separate images (most would probably contain very little??) ??.  You could probably automate the 'hand programming' method by drawing by hand on a grid, and every time you, say, add 3 pixels (minimum feature size) you add a second layer neuron with weights from the drawn pixels that activate it (or just use a second layer with only 3 weights per neuron, and assign the three weights as you draw every 3 new pixels - hoping there are enough to take all ten images). For the last layer you'd connect all of the new neurons with weights that activate that one (the one that represents the number being drawn). This would give a lot less connections than a fully connected network, and a lot more neurons, and with all same valued weights, but more closely resembles the 'real' wiring and also the way a person learns to recognise numbers (by being shown how to write them,  and you could maybe run the process backward to generate naturally drawn numbers rather than generating by pixel??).  I haven't tried it though, and it's just an idea. If it were to work, then you could find a way to optimise the connections afterward to use less neurons, if you really needed to (maybe what dreaming does for a human).  I would say that is what is probably going on in a standard neural network (optimisation), and another reason that the weight values don't make much sense.  Probably if you were to break the number images down into sections of size, say 3 pixels, then recorded the number of every position (and orientation) that the small sections could fit on the grid, and still be part of the number that they represent, you'd get the number of neurons needed in the second layer if you only used a 3 layer net for this task. The breaking of the image down into lots of smaller pieces is just what a convolutional network does, so the idea is probably close to equivalent.


Sorry about the ??; these represent something my intuition tells me, but has not been tested to see if it works (yet). Maybe someone can put me right (like a good reason for something working or not, and _why_ ) or even try it out - I will be doing, but it does take time to program."
1841,This is so important to understand for people like me who work in Satellite Image Processing. Thank you üôè
1842,Can a program have enough functions to have as many outputs has a human brain ? And can it program it self ?
1843,Now I know how complicated deep learning
1844,"Amazing video, thank you for making it!"
1845,"I am developing something that can speak mayan language, so far, ......, I need it for my classroom, ....., I am being paid due to it, so far......, enticing folks"
1846,Super beautiful presentation and highly attractive.   Thank you.  www.ayouty.com
1847,"welp, it's official: god made me retarded."
1848,–ë–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ –∑–∞ —Ä—É—Å—Å–∫–∏–µ —Å—É–±—Ç–∏—Ç—Ä—ã
1849,my final project . https://www.youtube.com/watch?v=skgOfjJbXuc
1850,How can I do 100 likes?
1851,"Just wrote an email to 3bluebrown@amplifypartners.com. Got the following ' Address not found
Your message wasn't delivered to 3bluebrown@amplifypartners.com because the address couldn't be found, or is unable to receive mail. '"
1852,"at 15:05 there is mistake, You can't multiplay ""d=(w1_30 * a1_30 + b1_30)"" because bias is only for one layer, you should do it ""d=(w1_30 * a1_30) + b"".   !!!
I learn many tutorials, and on all of them, bias was only 1 for each layer.
its like is 1 to 30 inputs plus 31 bias. 
this upper equation is corect."
1853,Excellent video.
1854,"Es algo confuso, pero si lo ves 2 veces se aclaran muchas dudas. Thanxs for the video, it makes me see ML easier than i thought."
1855,Now I know why learning linear algebra matters
1856,I want  to thank you wholeheartedly for making this informative video. was very helpful to understand it for my project
1857,OMG there is no way I get in a Tesla on autopilot lol
1858,He is a genius. That is how teaching mathematics and science should be done. Such people must collaborate with the best professors in different sciences and come up with graphical explanations of the complex subjects.
1859,"Using sigmoid function, any input that is 22 or above will always be 1"
1860,Is it possible to get these amazing slides somewhere? Thanks!
1861,The pi-s are soo cuuuuuute! <3
1862,Amazing visualizations
1863,"The volume of her voice seems a lot louder than your mic, kinda startled me when she started talking"
1864,Is this sal?
1865,"Wow.. that was amazing.
I dont get the 1000 people who disliked this fricking awesome video"
1866,"Hats off to you for explaining in such a simple way with so much of visulization, I am fan now"
1867,"Respect. Thank You Sir!
First 30 seconds of the video
My brain is already wandering off so deep"
1868,So neural networks are just cascaded encoder circuits? So this scary black box turns out to just be introductory digital logic?  Crazy.
1869,Thanks a lot for that's video. To help people understand really quickly how neural network works
1870,Ok i found a few of your video pretty understandable but I don't get this one. How is the network being told what's the correct way to read a scribble? I mean I give it an input and I expect an output but how do I tell each layer what's the right path?
1871,Can we claim it is a bunch of *if statements* ?!
1872,"So since it's an image-based network, we are making convolutional layers?"
1873,I never got the neural networks of my brain to understand neural network :) . Untill I hit upon this video. Simiply Superb!!!.
1874,Respect. Thank You Sir!
1875,Do the hidden layers use convolution for understanding the lines or circles that come up?
1876,"Awesome explanation, love that you tied it in with the subscribe button in the end. Thanks!"
1877,Respect for the hardwork put in video
1878,"that is why machine learning and AI are not real intellignece / thinking , machines needs to be trained, humans not at all"
1879,"Thank you for this. You've taken something interesting and complicated, and broken it down into fundamental and simple concepts in a complete and instructive way. You highlighted each piece, and not only explained it, but illustrated properties it has that make it fit so people watching don't just know it, but understand it.

Thank you."
1880,one of the best explanation of neural networks. thank you. I need this to explain to my students.
1881,Wow! The belief i had AI was something out of my reach just got demolished!
1882,"But ReLU has no upper bound, what if the value is bigger than 1?"
1883,whats his name? grant searson? grant pearson?
1884,Do cnn and LSTMs please
1885,This is probably the most detailed explanation of NN's I've seen so far
1886,"Hire me.. i could program ""Hello World"""
1887,ÌïúÍµ≠Ïñ¥ ÏûêÎßâ Ïôú Îã¨Î¶¨Îã§ ÎßêÏïòÎÇòÏöî Ï§ëÍ∞ÑÎ∂ÄÌÑ∞ ÏïàÎÇòÏò§ÎÑ§Ïöî..................
1888,"I can't believe I just discovered this channel, I got to thank minutephysics for it. You are the shit man!! Keep making this videos."
1889,"Just the right amount of complexity, when it's not a rough idea but a defined topic. And damn, you're good at explaining things. I even kinda understood.. some.. part of it. That's the best I managed out so far :)"
1890,But can it pass the ‚ÄúI‚Äôm not a robot‚Äù test?
1891,"Simp√°tico, agrad√°vel, did√°tico e open source. Gostei!!! Seguirei e compartilharei!! tks"
1892,muy intuitivo gracias...
1893,Thank you sooooo much. This is 'the' video that makes me understand neural networks.
1894,Amazing video! such a complex concept with such a simple explanation ü§Øü§Øü§Ø
1895,"Now I understand the phrase ""Mathematics is in everything""  üò±üò±üò±"
1896,Best explanation of neural network ever in youtube
1897,Too much math here but at 9:40 there is Minecraft block
1898,Hey Grant !! Please make a series on Statistics and Probability theory  to visualize the basic terms and their significance
1899,can we have a course on p vs np
1900,Thank you so much! I just love you. Thanks to you I understood Linear Algebra and now I am getting into Deep Learning. My bows to you!
1901,"so complex,i watched this three times,but i am not still clear."
1902,The amount of information coming in from this video is too fast i have to slow down the video to understand and catch up
1903,"Great example, which shows how far away we are from artificial intelligence..."
1904,I AM MACHINE
1905,So writing my first NN from scratch - after watching this series several times :)   One thing that is unclear ( forgive the noob question ) - Sigmoid results are ALWAYS positive ( ? ) therefore activation is always positive ( 0->1 ) right ?  This is seemingly contradicted in examples that seem to show negative activation being passed into the next layer.   What am I missing ? - Thanks !
1906,"Why korean sub stop at 10:50?
Please fix this"
1907,This is how first computers worked like. German Lochkarten maybe holecards in englisch
1908,Yep 1 brown eye
1909,Anyone manage to put the exemple working?
1910,shit vid. academic pseudo complexity.
1911,"‚ÄúIf you can‚Äôt explain it simply, you don‚Äôt understand it well enough.‚Äú the simplicity in this video over a relative complex topic is astonishing. The fact that I have not discovered this channel earlier is almost agonizing."
1912,yin yang brought me here
1913,"I love you 3Blue1Brown, you‚Äôre awesome. You make learning significantly easier"
1914,"There's like, so much vocal fry at the end?"
1915,"Really useful video, thank you!"
1916,best teacher on earth!!!!!!!!!!!!!!!!
1917,"At 10:05 , how does having negative weights around the positive ones make the sum larger?"
1918,"Recognition of digits to me comes from the motion. If a computer draws a 3 following the pixels, they'll find what they're looking for. Shouldn't it be that simple?"
1919,*Achievement unlocked*: you just reached 1.8M subs
1920,"May I ask a question, can you reveal what tools/libs you used to create all the great visualizations?"
1921,"Hey Grant. I'm not sure if you read comments on this old stuff, but I wanted to share that I was linked to this video via Google's official Tensorflow tutorial! You've made it, man."
1922,damn ur to good
1923,Great
1924,Did anyone notice the thumbnail of this channel has a circle colored blue and brown that can be broken as 3 blue parts and 1 brown part which kind of signifies a machine can read this image and recognize this you tube channel ?
1925,"Wow, best DL video I have ever seen"
1926,My CPU is a neural-net processor; a learning¬†computer.
1927,Mind blowing ...Thank you...
1928,"so i can finaly scan my homework into the computer, and he will do it for me via. trial and error?"
1929,Thank you for sharing this. How straightforward this is!
1930,HOLY F*** this is a great explanation. Thank you!
1931,i love you
1932,T√ºrk√ße Altyazƒ± Berbat Olmu≈ü
1933,R.I.P. Captcha.
1934,thanks
1935,5:50 is the aha moment
1936,"–°–ø–∞—Å–∏–±–æ –∑–∞ —Å—É–±—Ç–∏—Ç—Ä—ã, —ç—Ç–æ –∫—Ä—É—Ç–æ"
1937,Beautifully explained videos like this make me happy to be a patreon member of yours. Keep it up @3Blue1Brown
1938,which activation function did you use? I am trying  with sigmoid but cant prevent it from saturating
1939,Good video! I'm liked and share 893 times :D
1940,"Astonishing, incredible, superb and wonderful explanation!"
1941,"Imagine a 4k picture. It has 8294400 pixels (3840√ó2160). Everyone of these pixels has 4 values: red, green, blue, and an alpha value for opacity. All of these can be values ranging from 0 to 255, so 256 values. So you can basically have (256√ó256√ó256√ó256)^8294400 different pictures in 4k, probably less because the lower the opacity, the less the rgb values matter. For an opacity of 0, #FFFFFF will yield the same color as #000000 - basically nothing since it's invisible.

Does anyone know how much (256√ó256√ó256√ó256)^8294400 is?"
1942,Funny mathematicians trying to understand complex neuroscience.
1943,"There are programs that can work out what a number is that are accurate 90% of the time, and guess what, they come on the Nintendo ds
(I'm commenting this before watching the video,  sorry if this is mentioned)"
1944,I need sleep
1945,"""Some libraries optimize the heck out of matrix multiplication""  :D So true!"
1946,"Haven't watched the next video yet (and have done 0 research on this subject before) but my guess is that, when the number is identified incorrectly, the biases for the inputs which strongly activated the chosen node are increased and the biases for the inputs which would heavily activate the correct answer are decreased.
I guess the opposite should also be true when the answer is identified correctly.
Looking forward to finding out if I'm right!"
1947,"At 9:37 in the video it says: ""where the brightness of that pixel is some lose depiction of the weight's value"". But earlier in the video we learned that the brightness of a pixel is its activation. So I was expecting the brightness of a pixel to be a1, a2, a3...etc.  So which one is the brightness of pixel? Is it a1, a2, a3 or w1, w2, w3 in the equation w1a1 + w2a2 + w3a3 +...w(n) a(n) ??"
1948,"After reading some articles to get a basic background of the networks, watching these videos and having all the pieces fall together in my head has been one of the most exciting parts of my week."
1949,Extremely good explanation.
1950,"Hey, could you compare neural networks to support vector machines?"
1951,50 seconds in and Plato‚Äôs cave allegory has been rendered entirely obsolete
1952,Looooove this video. GIMMIE MOOOOOOOOORE! ‚ù§Ô∏èüëåüëç
1953,Êó•Êú¨Ë™û„ÅÆÁøªË®≥„ÅåÊ¨≤„Åó„ÅÑÔºÅ
1954,"11:06 I'm still not really understanding the bias, help.  why did you set bias to -10?  How to know when I want the neuron active?"
1955,The only thing I can‚Äôt understand is how you don‚Äôt have millions more subs. Thank you.
1956,"I love your channel, I study mechanical engineering in France but your channel gives me so much data. It feeds my thirst of knowledge ad it is presented in such a poetic way. Thank you 3blue1Brown, I'm going to support you definitely. In the new era of educational content rising on the internet, you are one of the most high quality"
1957,"At 14.40, the video says it is adding the bias vector (dimension 784 x 1) to the weight matrix and activation vector product. However, the dimension of that matrix-vector product is a vector with a dimension of 16 x 1. Hence, the dimensions do not agree to add the bias vector to the matrix-vector product."
1958,"Isn't it interesting that this is meant to be very complex, but somewhere somehow, how brain can comprehend it."
1959,I came here bc of the comments on a video of Primer channel and i LOVE the way youtube is slowly becoming more and more educating with such enternaining videos <3
1960,AWESOME
1961,"May I jump the gun here and ask 'those in the know' what to study at Undergrad for the most direct route to Machine Learning, Deep Learning and general A.I. careers? I believe this is more about data science than computer science - am I wrong?"
1962,Neural networks is the same as Windows NT
1963,Te≈üekk√ºrler.. Ger√ßekten iyi bir √∂ƒüretmensin.
1964,"To be honest, this tells me that the current model of neural nets are not efficient enough to be accurate models of how our brains actually operate."
1965,"around 9:35 what exactly do you mean by positive weights and negative weights ?
can you please elaborate that with reference to this context?  
activation = 0 is considered to be negative weight is it?
thanks in advance!"
1966,"This is amazing, I've never expected to understand neural networks that easy. Thank you so much!"
1967,whut
1968,You are such perfect !
1969,You sound so much like Dave Rubin! Thanks for the amazing video btw!
1970,wasted my time watching this garbage
1971,such a good explanation.  thank you.
1972,Is a the bias?
1973,Excellent. Thanks a lot.
1974,"Isn't it funny that we don't understand our brains, just like game characters don't have ""access"" to the game engine?"
1975,Is this the same as face recognition?
1976,These videos have the potential to save our species from ourselves if more people saw them.
1977,It is very helpful for entering into¬†deep learning
1978,Our voice is weirdly similar..... brother..?üò∂
1979,Studying this for my Computer Vision exam!!!!!!
1980,His videos are so elegantly illustrated and flow of thought is so clear. Watching his videos is like listening to music of Mozart to meÔºÅ
1981,"I was interesting in learning about this.
When he got to weights, biases etc, my brain just farted, died and is awaiting a reboot.  I now have a headache >.<"
1982,"111,111st to like!"
1983,"When you simplify the formula to matrix equation, there's a typo: at time 14:46, Bn should be Bk. The subscript should be k instead of n"
1984,"At 15:58, should all the k's be n's and the last row's n's should be k's?"
1985,oh man you are really superb teacher
1986,I heard  few lectures in Youtube and back in my university. And yes this is the most practical and detailed explanation of all! Good stuff!
1987,Geoffrey Hinton is a god
1988,good explanation to ML
1989,"whats amazing is that evolution designed something this inanely complex, a much better version - general biological intelligence (you), with no programmer"
1990,excellent vedio
1991,"Neural network is a network based on no-lifers who think they are playing a computer game. They all make calculations and send data. They never fail, they never quit, they get high ranks for that and moms feed them. Matrix is here, human is now a part of a machine world, serving it. Some of them are SO determined they actually PAY other people to have the game done! Can you imagine anyone having fun doing that? Nah. Only an important task may force them to link computers and plan bots. They think they control bots but it's bots who control players who control smaller bots. 

So when you ask the neural network a question, it's ciphered and sent into an online game. Players solve that problem and report each their own little part. Altogether they form a bio--computer covering all the planet."
1992,Holy shit this style and content is profoundly beautiful
1993,i hope super mathematicians develope very new super nerual network. because lack of math mades ai-ice age and then better math made current more powerful deep learning. it took decades to find the backpropagation.
1994,"Thanks for caring so much to upload such a comprehensive animated video. I have read about who machine learning works, in some courses or books, or articles. None has taken the steps you have presented here. Well done. Although I think I should watch this couple more times to really appreciated what you are teaching.If all teachers were teaching like you I'm sure we would have been living in a different world now."
1995,Can anyone explain why the reLU function works because I thought that the function had to produce a value between 0 and 1 but relu just outputs the input (as long as the input is positive)
1996,Conduttore di energia cosmica
1997,"I cannot imagine any explanation being more clear than this. Your phrasing and choice of words, your verbal emphasis and intonation, your gentle and encouraging tone of voice, your pacing, your visual depictions and animations.... Everything is absolutely perfect."
1998,Amazing explaination
1999,"Plain awesome, thanks for you amazing explanation you never fail to amaze me !"
2000,Anyone help
2001,"Python networking .¬†UDP group chat without threads. Write a script ‚Äúgchat.py‚Äù such that when it is called with an argument, which must be an IP address of a server, it acts as a client, and when it is called without an argument, it acts as a server. Example, ‚Äúpython3 gchat.py 54.38.181.152‚Äù is for running a client and ‚Äúpython3 gchat.py‚Äù is for running as a server (in this example must be run obviously on a machine with IP address 54.38.181.152). The script provides a kind of a group chat. The communication is in UDP only (so packets sometimes can be lost).¬†You must not use threads or processes. Everything must be in a single¬†mainthread.¬†You must use only the low-level modules¬†(no modules where all or almost all is done).¬†The server listens the UDP port 12345. The client sends the keyboard input lines to the server via UDP (one packer per each input line). Both, the client and the server, display every UDP packet coming from the network as a text with a suffix indicating the source, such as ‚ÄúHello there! by ('54.38.181.152', 12345)‚Äù. Whatevertext¬†the server receives from keyboard, it transmits the text to all its clients. Whenever the server receives a message from a network, it retransmits the message to all other clients (except to the client where from the message is received). Whenever someone sends a message to the server, the server considers it as a new client"
2002,"But hey, why take a bias instead of taking a 'squished' sigmoid function (one that has a higher slope at 0 compared to the regular sigmoid)?"
2003,"without appealing to the VC dimension, this is all worthless."
2004,If only I could like this a million times!
2005,Thanks a lot
2006,But can anyone please help me saying that what  actually are the weights? it still seems unintuitive to me.I mean what are those ?That will save my day.. And can it be said that a^(j) = sigmoid(W^(j-1) a^(j-1) + b^(j-1) ) ?
2007,Amazing video. Great animations. Thanks a lot. You made it very easy to understand.
2008,It's how the image of blackhole filtered from noise of frequencies And also using Fourier transform
2009,"I suppose, the length of the biases' vector must be 'k', not 'n', regarding to the number of rows of the weights' matrix..."
2010,Beautiful animations. Awesome explanation.  ‚ù§
2011,8:40 - weightings explained
2012,"Thanks a lot for the video: it is illuminating! However, I do not understand how you can use the ReLU function instead of the Sigmoid function. The Sigmoid is bounded between 0 and 1 and you say this is an important property. The ReLU is not even bounded. This looks contradictory."
2013,Dude you're awesome. No one makes videos about these topics not this good at least. Keep it up üëç
2014,You are a Legend in truest of sense
2015,ÏûêÎßâÏù¥ Ï§ëÍ∞ÑÎ∂ÄÌÑ∞ ÏïàÎÇòÏôÄÏöî
2016,its difficult for me to understand as i am not science student
2017,If it's that simple then why is the number recognition for brain training that bad?
2018,Could you make a video about cuantum computers and qubits? Looks like an interesting topic.
2019,"The most helpful video I have seen connecting the intuition of neural networks to their math. Most of the other places I saw explain the intuition or the math, but fail to make this crucial connection. Thanks a lot!"
2020,Excellent
2021,"Dude, You are an epitome of explanation. This thing couldn't explain it better."
2022,Sir please make a videos on physics and chemistry
2023,"A.I gets fed Doctor handwriting data.

A. I left the chat."
2024,"Hi at 7:53, you said different shapes could be captured in the third layer. How do we get 784 pixels for each neuron to actually draw that image? Question might sound silly but I am just starting with deep learning. Would really appreciate if you could clarify. Thanks."
2025,Skynet?
2026,I don't understand why we add a bias in the Sigmoid function
2027,"8 years ago I tried to learn about neural networks(because it looked cool) and months to no end I understood even less... and then... 4 years ago, I tried again, to no avail. but all it took was this video to sum up all the information I got and turn it into something I understand and comprehend. you know what? thank you ""3Blue1Brown"" whoever or whatever you are, you just made my night(22:24 GMT -03:00 hours here)"
2028,"a is vector length is n, but bias vector length should be k (the same as number of rows for w)"
2029,"I think the matrix notation is not correct. The number of biases should match the current layer not the previous layer. Maybe, it should be X.W + b where X is (none by n) and W is (n by k) and b is (none by k). Where n is number of input neurons and k is the number of the next layer. I use none to feed any number of inputs in each batch. I might be wrong though!"
2030,Excellently explained. Great
2031,ùò¥ùò≤ùò∂ùò™ùò¥ùò©ùò™ùòßùò™ùò§ùò¢ùòµùò™ùò∞ùòØ ùò™ùòØùòµùò¶ùòØùò¥ùò™ùòßùò™ùò¶ùò¥
2032,10:40 that is the coolest thing ive seen graphs be used for
2033,Pieman!!!!!!!!!!!!1
2034,Sexy voice
2035,15:07 Shouldn't the Bias Vector be till b(n)?
2036,Amazing work
2037,WARNING! Bias video
2038,The highest like to dislike ratio. Even music videos don't have such a ratio.
2039,at 14:38 vector b (bias) is a vector in k-dimension as opposed to n
2040,"12:45 - I've done this before; I studied A.I. in graduate school before A.I. was seized by the market & Computer Science.  I learned A.I. from its origin and from the abstract - the angle I approach A.I. is Interdisciplinary and my specific Ph.D. studies were in the Philosophy of Science and Technology.

It's frustrating to see a ""simple"" neural net like this when it's actually horrifyingly and unnecessarily complex - neural nets can be much more robust without that number of inputs.

If programmers didn't build a predefined net for people to ""learn"" A.I., where they're just training a net and that's it, then there could be much greater advancement in Intelligent Systems and at a much greater pace.

It would be nice to have a simple interface, where nodes are deployed as if the net were as simple as a mind map.  Deploy a node, and select what input variable that node will check and activate or not.  Likewise, basic control over functions for the hidden nodes tethered to floating weights.

In other words, someone should design a deployable net that is not reliant upon writing code and doesn't predefine all the nodes, functions, weights, etc.  All that's needed is a program that provides a structure that multiple disciplines can generate a neural net from...and all from unique perspectives.  Something like a repurposed Maltego...

If someone knows of a program that already does this, then call it out - or not....(?).  Personally, I've had to do basic coding before; I have no desire to learn a language to build a net - I just can't bring myself to care that much...  

At the same time, as a Philosopher, I'm lazy, but I'm also far too bitter to hand over any particular insight I have gained from my experience conceptualizing neural nets.  I studied under the World's Foremost Epiphenominologist (formerly), an expert in Artificial Intelligence, and a graduate assistant himself to Karl Popper.  He's long retired and done teaching; I will pass on the insight I learned - but I am much more likely to send it forward through the Philosophic tradition.  It doesn't have to be that way though......."
2041,Another great video! A minor correction at 14:38 - The final element of the bias matrix should be b‚Çñ instead of b‚Çô.
2042,How does one design a neural network??
2043,Squishification is now added to my vocabulary.
2044,Thank you for this video. The best I have come across so far. My bible for Deep Learning.
2045,On what basis weights are predetermined? Pls reply
2046,"I am not really from a math background but I am hugely interested in programming, and I must say this video has made it easy for me to understand the math behind neural networks!
I loved it , thank you!!!"
2047,"Hi. I think I was asking that before. But you made a video once on hilbert curves and how they can be used. I was asking myself and now the people here and you, if it would be cool to use a hilbert curve as an input pattern instead of sequential input. because as you said in the other video. increasing the resolution would mean that the network wouldn't have to be retrained? would that be a correct statement?"
2048,"So let me get this straight.
We assign weight to each edge of a neuron because it tells us how much activation the neuron of previous layer brings up in our particular neuron .In other words the activation of a neuron depends on all weights of all edges connected to it"
2049,COMO ENCUENTRO ESTO EN SPANISH
2050,the Best explanation to learn deeplearning. Thank you very much!
2051,I have a Ph.D. in theoretical physics and a post-doc from a (the) top university.  Thanks so much for this very lucid explanation that made everything very simple in an elegant way.
2052,"bro, i was confused af why I had to take linear algebra as a software engineer this year. Me fucking understand now"
2053,"Hey Sir.. can you explain me the connection of neural  network with Ising mode in magnetic system for phase transition in seprate video?
thanks so much for your great video."
2054,"I'm a little confused here, at 14:43, when we put the calculation for a layer of neurons into matrix-vector multiplication, shouldn't there be k rows in the Bias vector? Any clarification would be appreciated!"
2055,"mathematicians dont know how hard it is they just tell us to do a ml task that is never even possible, what a world?"
2056,"At 10:00 he says
""And if you really want it to pick up on whether there's an edge here what you might do is have some negative weights associated with the surrounding pixels. Then the sum is largest when those middle pixels are bright but the surrounding pixels are darker.""
Wouldn't the sum decrease if we have negative weights for the surrounding pixels?"
2057,Great content! Would suggest for people to check out his linear algebra stuff too!
2058,This video is amazing . Thank you for all your fantastic video
2059,I‚Äôm so confused. Everything makes sense but in a weird way that I can‚Äôt process. Great video though. Much better explanation than my classes.
2060,Amazing!
2061,You are effing awesome!
2062,Î≠êÏßÄ Ïù¥ ÎØ∏Ïπú ÌÄÑÎ¶¨Ìã∞Îäî „Ñ∑„Ñ∑
2063,"just wrote an essay referencing this video! thanks man, said all good things"
2064,"Wow, i'm just an artist, but even i understood this... <3"
2065,AWESOME man AWESOME
2066,This channel is very underrated
2067,"Um, what?"
2068,@3Blue1brown where is part 2???
2069,"Hello,
May I know what is the type of software you use for making these beautiful tutorials? Thanks"
2070,"super clear, thanks!"
2071,very nice explanation of neural networks.
2072,0:21 actually the first three looks like a lizard
2073,ÌïúÍ∏ÄÏûêÎßâ Ï§ëÍ∞ÑÏóê ÎÅäÍ≤®Ïöî „Ö†„Öú Ï†≠Îùº Ïò¨Î†§ Ï£ºÏÑ∏Ïöî ÎÑòÎÇò ÎÇ¥Ïö© Ï¢ãÎÑ§Ïöî...
2074,"You are gonna save my exam, thanks."
2075,What does ‚Äú 3 Blue 1 Brown mean‚Äù ?
2076,"it;s not worth thinking of it in terms of loops... Neural networks are just like humans, input and output machines with a feedback loop. If the output doesn't match the desired output the network will change until the output matches the desired output. We as, human, are very good at associating and decomposing the world into pieces, and when we see something like a neural network we want to understand what each layer is doing, but in reality there is no separation. It acts like a whole."
2077,"Not gonna lie,this is nice and all,but ya lose'en me with black ground and plain screen...."
2078,Interesting
2079,What software did you use to make the animations? They're great.
2080,Thanks a lot.
2081,Simply Superb.... explanation. Thank you very much.
2082,you save my grade
2083,0:48
2084,Thanks this explanation is really helpful when trying to learn more about neural networks.
2085,This video is benevolence at its best- I've been trying to wrap my brain around the activation function with respect to the hidden layers and how it propagates forward in the NN. Glad I found this channel - thanks
2086,This is amazing
2087,Ï¢ãÏùÄ ÏòÅÏÉÅ Í∞êÏÇ¨Ìï©ÎãàÎã§. ÌïòÏßÄÎßå Ï§ëÍ∞ÑÏóê ÏûêÎßâÏù¥ ÏïàÎÇòÏò§ÎçîÍµ∞Ïöî.. 10:50 Î∂ÄÌÑ∞ ÎÇòÏò§ÏßÄ ÏïäÏäµÎãàÎã§
2088,14:15 my favorite part
2089,I think i found like the least important error on earth in your video but for the sake of trying to make it less confusing for anyone studying this in detail i am pretty sure at 14:53 there are supposed to be biases from b_1 to b_k other wise you would add a vector with k and one with n entries  if i am not mistaken.
2090,First 40 seconds blew my mind!!!!
2091,your video is so clear to me that I started thinking I could have understood it even in my school days. thank you for such wonderful video. Why in your playlist so less videos are there? I am waiting for more such videos.
2092,"I have a question. I sort of hope it could be a break through...

Can you put a function as a ‚Äúweight‚Äù? Or does it have to be a number chosen by evolutions?
What if it was something simple like plus or minus."
2093,"A machine learns the same way a river carves the Earth. It is a worn process of repetition. and when we manipulate the pixel color we are in essence damming the light photons or funnelling them down toward a common path , rrrrrraaaaaather than it being an intelligent process ha ha ha ha ha ....."
2094,I want to make some joke about recognizing the pi symbol as a 3 but I'm not funny enough
2095,"So, if you write a tiny number in a corner, you may be able to cheat the algorithm because it is only looking for the parts of loops and lines in the middle of the screen?"
2096,So Helpful!! Thank You
2097,More video on neural network
2098,Simply complex
2099,"I'd like to thank you for this video. I've been scouring the web to learn just a bit about neural networks and deep learning, and came across your video. This is the first one that I could truly understand and finally can have a grasp of concepts. I'm deeply grateful."
2100,"No doubt, best education channel on the tube"
2101,Is this Rick Steves Jr?
2102,"Your videos a fabulous, and by me there is no doubt about that! 
Thanks a zillion times for the same.
 
An earnest request can you please do a series on Machine Learning comprising of :
1. Probabilistic Modelling: Logistic Regression, Naive Bayes algorithm.
2. Kernel Methods: SVM algorithm.
3. Decision Trees, Random Forests.
4. Gradient Boosting Machines.


Anything that you consider is relevant & important and is not part of the above list.


Godspeed!"
2103,"One quick question,  is the mode your currently explaining a convolutional neural network? Too me it looks like it is, because you are recognizing patterns and based on those patterns your creating an image but i'm not that certain.      :("
2104,i love pizza
2105,"Woow the explanation is sooooooo fluent. Loved it till the end of the video
üíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíïüíï"
2106,THANKS A LOT. GREAT TEACHING.
2107,I really love your videos. Thank you!
2108,Studying AI a university right now and these videos are unbelievably helpful. Way better explained than I've seen anywhere else.
2109,Loved it!
2110,"That is, hands-down, the best subscribe request ever."
2111,t.h.a.n.k.y.o.u.t.h.r.e.e.b.l.u.e.o.n.e.b.r.o.w.n
2112,"Wonderful explanation and great effort, thank you."
2113,Thank you very much for this !!!!
2114,"You're kind of a genius man! I don't care how much you deny it. Your ability to distill these complex concepts into very simple ones and across so many fields in math is amazing. Also, the way you connect different fields of math to explain solutions REALLY shows a different type of mastery. Thank you for all these videos."
2115,"If , when using ReLU, the network keeps the same value for anything over 0, how does the constant adding and multiplying not cause the output on the last layer to be way out of the 0.0-1.0 range?"
2116,"Do the weights need to fall within any certain range, like between 1 and -1, or are they only bounded by the minimum and maximum values of the data type that you use to store them?"
2117,"I knew I was confused for a reason. It is suppose to be [b0,...bk] not [b0,...bn] at 14:54"
2118,","
2119,"Bruja, answer me!!!!"
2120,Why did I find this great video now ? It is very awesome!!
2121,Okay but the bias shouldn't have a negative 10 on the equation? Why would you subtract a weight?
2122,I found my home! üò±
2123,Sir really u r realistic plz upload more videos like this the anime really helps us to the analyse and think laterally
2124,"This is brilliant, I simply love the simplification of a rather seemingly complex concept!"
2125,Sigmoid Squishification......Great Clear Video...Neural Nets Brought me here...
2126,"14:35 The size of the bias vector is k, not n."
2127,"Deep Learning Models and its application: An overview with the help of R software
Deep learning models are widely used in different fields due to its capability to handle large and complex datasets and produce the desired results with more accuracy at a greater speed. In Deep learning models, features are selected automatically through the iterative process wherein the model learns the features by going deep into the dataset and selects the features to be modeled. In the traditional models the features of the dataset needs to be specified in advance. The Deep Learning algorithms are derived from Artificial Neural Network concepts and it is a part of broader Machine Learning Models. 
This book intends to provide an overview of Deep Learning models, its application in the areas of image recognition & classification, sentiment analysis, natural language processing, stock market prediction using R statistical software package, an open source software package. 
The book also includes an introduction to python software package which is also open source software for the benefit of the users.
This books is a second book in series after the author‚Äôs first book- Machine Learning: An Overview with the Help of R Software https://www.amazon.com/dp/B07KQSN447

Amazon Link
https://www.amazon.com/dp/B07NJMM6LR
ISBN: 1796489034
Website 
www.ijsmi.com/book.php"
2128,8:50
2129,These programmes are the most Impressive of inspiration about that Advanced Medical in modern medical engineering sciences for Young Educated Generation beyond the Employment crises too.
2130,"+3Blue1Brown, I think you made a mistake at 14:45.
The bias vector has n rows, while the amount of neurons in the next layer is k. 
Every neuron should have one bias, so the vector should be a kx1 vector.
Also, you multiply a kxn matrix and nx1 vector, which results in a kx1 vector.
This means the bias vector should also be a kx1 vector if you want to add them,
solidifying my previous point.
Am I right to think you made a mistake?"
2131,Finally !! The adequate explanation of neural network )))) Simple understand and follow. Thanks a lot ))))
2132,This is the amazing video. Thanks for making it!
2133,So how to make a neural network that can go out with my dog?
2134,Thank you! This is an exceptional video!
2135,does anybody have algorithm of number recognition ? please
2136,bam bidirectional associative memory
2137,"Very impressive! So easy to understand, the best lesson for new beginners."
2138,I‚Äôm not even in college yet what am I doing here
2139,Áúã‰∏çÊáÇ
2140,Great... A computer simulator on a computer.
2141,Yes finally automatic recaptcha solver!!!
2142,Awesome what an explination  ?? really a good start  for deep learning
2143,This man is a God.
2144,Great video...please do more
2145,The bold *is* just showed up as '*is*' in the title
2146,"Cesar Gianluigi Figueroa Lima, muchas gracias por tomarte el trabajo de poner los subtitulos, me han servido mucho. Ni hablar del video, much√≠simas gracias."
2147,"Why the sigmoid, we could have used the inverse tan function?"
2148,THE BEST
2149,"Half your video didn't make any sense.


1) @9:17 why does every pixel have a different weight? Why are some pixels more important or less important than others (apart from the pixels that contain the number)?
2) @9:31 why does the grid have random green and red pixels on the grid itself?
Note: weights are fixed, they are fixed after being trained. It's the 'x' input, or in your video 'a' input which holds the brightness of the pixels. 
3) @10:15 what do the weighted sum mean? To someone new to ML like me, that's just a random number you are adding up, please explain what it means.
4) @10:22 Why do you want activation numbers to be a value between 0 & 1. 
5) Why do you need to squish the numbers in sigmoid? What does it even mean?"
2150,anyone else see an interference pattern? the network layer patterns seems to set up In a similar way. ah take it with a grain of salt..
2151,"These little Pi's reminded me Office 97 assistant (that famous paperclip)
When i first discovered that (it just popped out of nowhere) and i had lots of fun with it. Funny faces he had.
I was completely new to computers and didnt know it existed."
2152,This is about my 3rd run through the series attempting to make a feed forward network for the first time. Wish me luck. I think I'll get it this time but back propagation always confuses me.
2153,Great & thank you for information
2154,Thanks a lot
2155,Why use the sigmoid function instead of just dividing the sum result by the sum of the weights?
2156,"""0 to 10, telling you which digit it is"""
2157,This is the best description of a neural network that I have seen.
2158,Does anyone know how old he is?
2159,"The one moment I feel like I can relate to some brainy researcher is when they say ""it just happened to work."""
2160,"Hey 3B1B!

Love your videos! Are you going to post something on LSTMs?
Really looking forward to that"
2161,SUPER clear! Thank you :)
2162,"Every time somebody goes ‚Äúwe‚Äôll use these green shades/pixels to represent something and red ones to represent something else‚Äù might as well stab me in the eyes with a pair of forks. Colour blindness is awesome...



Not. :("
2163,"I love that one of the first big projects for image recognition by computers was in the 60's, assigned as a project to undergraduates, with the assumption that they would have it solved by the end of a summer. Still working on it..."
2164,God! I'm so glad I found this channel. Thank you for such a good content and these smooth animations:))
2165,Thai sub PLEASE
2166,What can a weight be? any value?
2167,Very good explanation üëèüëèüëè
2168,Hi if i have 24x2 input representing 2 elements of 24 samples then how many nodes are there in my input ? 24 or 48 ?
2169,Extremely good videosüòòüòòüòòüòò
2170,"Title and description auto-translation is cancer, please remove it.
Nice video BTW."
2171,"I understand most the content in this video, but one question. What *is* a Neural Network?"
2172,"Their are 3 important things those leads to our ultimate goal. 
1. Recognition. 
2. Decssion.
3. Execution. 
It is difficult but not impossible to prepare  neural networks system with electrons. How ever if these electrons are replaced with photons our neural networks system would become extremely fast and the problem of recognition will be resolved once for all.
Instead of using conventional transistors and other electronics components we need to develop light and photons logic gats."
2173,I love the end of the video where you talk about subscribing =) I thought it was fantastic.
2174,your comment about subscriptions made me smile
2175,Hats off for explaining in such a nicer and simpler way. I searched a lot to understand this concept and eventually landed here. I hardly comment but not commenting here would be injustice to what I just learned :)....Respect.
2176,I have decided to take a introduction to machine learning module just after watching this video and I am really grateful for having access to such content on youtube for free !
2177,Finally a great video that clearly explain what is neural network
2178,What the hell is going on with this girl voice at the end? It's so annoying. Nevertheless very good explanation.
2179,i know why the teachers giving shit
2180,Those ‚Äúsimplifications‚Äù seem to be the trend in recent science. I hope the bare bone of scientific approach will no be lost in this generation of ‚Äúsimplifiers‚Äù.
2181,"I'm wondering if  in 16:00 it should be b_k for the bias vector 
what do you think?"
2182,amazing explanation... thank you so much!
2183,"I would love to help you translate your content in French so more people can access these beautiful videos. 
For free. Just let me know if that's something you are interested in."
2184,Dont forget to turn off adblocker for this channel. I will gladly sit through all the ads for this guy's hard work
2185,"Yes, ""best"" and ""least confusing,"" yet all a bit hurried, at least for me. Would love to see this in print, though. Clearly, most scientists fail to speak with the clarity the lay person requires."
2186,"darn video ended 1 min inn, when he said create a program to identify numbers... that allready presumes shapes and a fixed value, makeing the learning prosses... not a learning prosses, program allready has 10 symbols that it knows, it didnt learn them, how will it know a i is not a or and how is this suppose to represent ai at all, its  will just run a script that will look for shapes that MUST Be one of those numbers, scale of the symbol or any variant wont matter at all 7 and 3 is probebly one of the only numbers that can be hard to program for (despite youtube or my pc useing a font here that doesnt have the cross over it.  https://qph.fs.quoracdn.net/main-qimg-3c2a0e9438c7453a2ba695e8b5e3f650 

also 4 and 9 also has the same issue as we write the 9 like q and 4 is suppose to have a opening in the top like you would see a digital 4.

So in all these cases in some handwriteing it will turn out wrong in a ai learning program... unless you take it to the extreme and learn everyones handwriteing and so on.

In anycase the program fails when it starts with the numbers and shapes for those numbers allready known.

The number system we have today even in school we first start to learn to use fingers to represent numbers or objects that we can count, then we learn what symbol that represent those fixed numbers, and this may differ in some countrys well at least in history.

Like roman symbols, i can read and identify them, but it requires that i know the right scenario when those are used or the person speaking\writeing them, and again the program needs to do it from base, no pre input of databases or knowledge of symbols.. infact the program cant start on this intelligence level if it wants to be called a AI and not just a script that looks for fixed shapes, points or crosses.. no it needs to learn the very basics of communication aka learning language, before it can start learning counting.

Heck any program iv seen so far wich is called AI isnt, and deeplearning and other server program is simply just a faster computer that can hold larger databases of predefined data it can match with.

Heck you could simplify it and just use excel and use a 8x8 grid box where you can color inn cells in different patterns then by sudoku matchbox randomizer it can assign a shape to represent a random value between 0 and 15 digit long number, then see how many numbers it can create (speeding up the prossess by just makeing a additional script that just randomly paints cells within the grid til it runs out of new patterns it can assign numbers to, shouldnt take that long.

Now the program has a set of shapes that has assigned value, i mean who knows if humans used both hands and toes when they made a number system, not to mention computers allready use 4, hex dec  oct bin and maybe a few that i dont know about or and variations like greycode to overcome challenges in some number systems.

Anyway at this point you need to add a couple of new lines of code to try to make the number system more effective, i mean useing different pattern for each number would be extremely annoying for sombody that has to interface with this number system, not that we would be able to remember but a couple of easy to remember shapes, wich has then a random big number that we probebly cant remember at all... so the program has to figure out whats the shortest number of symbols it can use before instead combinding 2 symbols to make the next step to 16 10 7 1 or whatever number it deems apropriate.. i mean a 10 might seem convinient to us because we have 10 fingers and thats why we have settled for it, not to mention it could also use 2 or 3 or how many it sees fit symbols to represent numbers of greater or lower number of the symbols... like roman IV, V and VI.. and thats not mentioning XVI and other variants... and even then it might need to make it even MORE user friendly.

THEN when it has done all that, then you can start feeding it our number systems that in our history that we have used.. for all we know it would be a pretty effective script, and at this point assigning it to even identify dice numbers, card numbers (12 number system (13?)) and other stuff like that wouldnt even be a challenge.

Sorry about my english, and the wall of text..."
2187,Great Video. Many thanks and best artificialneuralnetwork.app
2188,People disliking should be asked to submit a mandatory public comment explaining their actions
2189,11:19
2190,What software did you use for creating the video?
2191,This explains artificial neural networks with such great detail that even my 12 year old brain can make perfect sense of them.
2192,Hail you sir!üôáüôáüôá
2193,"1 thumb up for ""squishification"""
2194,physical action programs
2195,how do they move there hands and arms phychical action
2196,Comments are disabled for this video ARMY AND C OWN BY TOM . TOM TIME AND SPIRIT OF TRUE DAD LOVE AND CRAZY AMBITION TO MAKE AFRICA WELL WITH 600 DESTICATIONS TOM TOUR WITH TRUE WIFE ! SHE WILL BE THERE IN LJ IN TIME . SHE IS FROM NY AND WELL WITH MONEY ! THE WHOLE THING WILL BE SOON BUT TOM IS TRUTH GOD AND SON OF GODS !
2197,What 3blue 1 brown means
2198,Bravo... Thanks for RELu!
2199,"Artificial Intelligence is the Next big thing and it will be going to play an important role in the future like many industries already applying it for their market research.
https://www.karna.ai/"
2200,You have the most soothing voice...
2201,Nice
2202,You should have told them subscribe AND click the little bell because I think that bypasses youtube's algo and gives them update for every video :)
2203,"I study mathematics, physics and architecture. By definition this man is an ORACLE in the strict meaning of the word. 

With all honesty I never imagined someone explaining complex topics with the dexterity this man has. He is literally an institution and an outstanding teacher. 

The computer graphics and the illustrations are simply perplexing. This guy never evades complexity. He never evades complex arguments. He illustrate the complexity and dive into the exhaustive explanation of the details. 

It's extremely rare to see a professor and a dedicated user to put a lot of effort explaining, animating and describing mathematics the way he does."
2204,brilliant video much appreciated !
2205,Best tutorial i have on neural networks ever. I can see the effort you put in.
2206,seriously man your are just  awesome
2207,This 9 can machine recogniz?????üò∂üòï
2208,The voice of the girl is so lazy...
2209,this was really good work !
2210,"Written some notes from the video to read quickly. Hope it helps somebody. 

l Neural Networks can recognize hand written digits, letters, words ( in general, tokens )
 l What are Neurons?
  ‚óã Something that holds a number [ 0, 1]
  ‚óã The higher the number, the higher the ""activation"" rate
 l Consider a 28*28 table in which each unit is represented by a value between 0 to 1 ( activation number )
  ‚óã Let us divide each row into a ""layer"", such that, if we were to divide all the layers, the last layer would contain 10 ""cells"" ( units ).
  ‚óã Values are passed from the previous cells to the last layer ( 10 unit layer ), again, between 0 and 1. The higher or closer the value is to 1, the more probability exists that the image scanned represents that unit cell.
    So, a unit cell that contains the highest value is indication that the index of the unit cell is the value of the image scanned.
  ‚óã 16 cells in the second and third last cells are arbitrary.
  ‚óã Each cell is linked ( causes activation ) to some ( not all ) other cells in the next layer which further cause more activation.
  ‚óã Each 'cell' corresponds to some sort of identification about how much a certain region 'lights up', and then sends a value to another node which reacts based on the received value. 
  ‚óã To find whether a certain cell with light us, like each cell be represented by 'a Cell_Number ', and let each cell be 'assigned' a certain weight 'w'. The sum of all the products of each cells 'a' and 'w' will be:  
   w1*a1 + w2*a2 + w3*a3 + w4*a4 + ‚Ä¶  + wn*an 
  ‚óã Let these weighted sums represent some 'grid cell'. Each cell is either 'on' or 'off' with respect to being positive or negative. In this case, 'green' represents on, and 'red' represents off.
  ‚óã Let us concern ourselves to a certain region where the cells are mostly on. Ergo, we would be basically summing up the weightages of those grid cells.
  ‚óã Then, if you suppose a region where there are brighter grid cells in some part which are surrounded by dark grid cells, then that area is the main edge we're looking for.
  ‚óã Of course the sum of weightages gives us very different value. In order to 'squish' that number line into 0 and 1 , we use the function:
   Sigma(x) = 1/(1 + e^-x)
  Which is a sigmoid function or a Logistic Curve. Our equation now becomes:
   Sigmoid(w1*a1 + w2*a2 + w3*a3 + w4*a4 + ‚Ä¶  + wn*an)
  ‚óã But what if you don't always want to light up when it's a positive value, and rather want it to light up when the weighted sum of that grid cell full fills some condition, such as > 10. This is called 'Bias For Inactivity'. Using this example, our equation becomes,
   Sigmoid(w1*a1 + w2*a2 + w3*a3 + w4*a4 + ‚Ä¶  + wn*an  - 10)
   Here, 10 is the ""bias"".
  ‚óã The possibilities of the different knobs and dials open us to the term of ""Learning"", which just means to find the correct relation of values which perform the expected behavior. 
  ‚óã The complete expression above can be adjusted in the formula:
   a(1) = Sigma(W*a(0) + b )
   ( (1) and (0) are superscript here )

   Where W = k*n matrix whose elements are weights corresponding to a cell.
   a(0) = n*1 matrix whose elements are the 'a' of each cell.
   b= n*1 matrix whose elements are the biases of each cell 
  ‚óã NOTE: Sigmoid function is not used very often now, instead it is replaced by ReLU ( Rectified Linear Unity ), which is defined as:
ReLU(a) = max(0, a), a linear function where f(a) = a for a>= 0, which for a < 0, f(a) = 0."
2211,Anyone else have this this pop up on recommended?
2212,"–Ω–∏—Ö—É—è –Ω–µ –ø–æ–Ω—è—Ç–Ω–æ, –Ω–æ –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ"
2213,"You sir, have a gift for explaining the complex in a way people with minimal background can understand.  Thank you for this video!"
2214,go go fizika
2215,"I've watched now some of your videos and I would like to compliment you on the high quality of them, not only the graphical one, which is a technical quality, but also on the way the explanations are build, which is a ""not so obvious"" quality to improve and one very hard to find around even in videos (or texts) very well produced. It's nice to find this kind of good work around!"
2216,"Dude, you re the best. Your videos have best animations and simplicity. Thanks for the hard work. I am not working right now but i ll donate when i get a job. Best of luck on your next videos."
2217,What about CNNs and RNNs...?
2218,I wish I was smart enough to understand this because this sounds really interesting.
2219,This is probably the best explanation I've seen and even more in-depth than the CGP Grey video
2220,3Blue1Brown are you an artificial intelligence?
2221,"So I made a network with a single layer (784 inputs neurons and 10 outputs neurons), then I produced an image to see correlation between inputs neurons and pixels, here whats it looks like: https://i.imgur.com/2VHakdV.png"
2222,"This is the best video on machine learning, thank you for breaking down the complexity in this understandable format. Great job."
2223,"very good effort to share, obviously seruious work."
2224,I win
2225,if your a coder then you will apprentice the power of brain...a lot
2226,Finally got it :D
2227,"I'm not a native English speaker, what does the bias-term mean, bias as an offset, or bias as the prejudgment? Or both?"
2228,Bro your IQ?
2229,"sigmoid limits the output from 0-1, while relU only limits the low end. It would ""seem"", based on my ignorance, that we would also want to limit the positive. Why is this wrong?"
2230,but how on earth do you even learn  to live?
2231,You are excellent. We need more videos on deep learning. Please.
2232,So what you‚Äôre telling me is that those ‚ÄúAre You A Robot?‚Äù captchas are useless now
2233,Maybe it's not a great idea trying to learn english watching these videos
2234,Love how there are differently colored pis. We love inclusion
2235,This would make one sick looking polyhedron
2236,Yuppi! This weekend my homework for uni is watching 3blue1brown videos.
2237,Please do more ML algorithm videos! I'd recommend an overview of Random Forests for regression.
2238,"This is the most didactic, best explanation I‚Äôve ever seen on YouTube, that makes a series of complex concepts easy to understand. Congrats, I‚Äôm a new fan of this channel :-)"
2239,Spectacularly explained :-D
2240,I LOVE YOU.
2241,What?
2242,Great irony on 16:50 :-)
2243,please help me. can you explain about Hidden markov models ? i really need that :(((
2244,Please do more !! üòä
2245,A little bit complicated but extremely satisfying - subscribed!
2246,"ReLU(x) > 1 if x > 1. Please explain, how to deal with that?"
2247,"thanks a lot this video help me so much for my TPE (group work in france which is an examen about the subject you want, that's cool but hard...)"
2248,Please create video to explain Time series analysis specially aRCH and gaRCH models
2249,Excellent Content! Thank you!
2250,"15:08 Hell yeah, indeed Grant knows the BEST programming language, especially for adult things such as Neural Nets. Use Python folks."
2251,Could you provide us please which tool did you use to create this animated video?
2252,*360 p*   _squad_
2253,I love all the pi's when they start thinking. :D
2254,"thanks
now i know the basic knowledge of machine learning"
2255,Best neural network + machine learning explanation ever.
2256,my teacher couldn't recognize my 3.
2257,Great video! Thanks!
2258,I usually muting audio and setting replay mode on your videos. I like the animations. I don't understand what you are saying so it's difficult for me to understand all these SMART words half of which i don't know.
2259,FINALLY A GOOD VIDEO!!!
2260,wow
2261,This is great! I have been trying to understand the stuff with no luck until I found you
2262,"this is the least confusing neural network video i've watched, but i still don't understand."
2263,"Thank you!!!. Easy to follow, clear and well-structured beginner's class."
2264,Lol this video is completely wrong in every way
2265,"If you wanted to speed the process of recognition up to avoid having to get precise matches each time you could add in a diffuse setting. So if a pattern is only 10% dissimilar from a batch of patterns you've already seen then it takes it for granted that it is 'probably' also nine.

This doesn't always work but nature has a solution.

If critical failures of recognition can cause ultimate death in an organism it makes sense to reproduce so that you have many chances to be incorrect. Instead of putting all your eggs in one basket."
2266,"Left weight (strong).
Right stop.
Left weight (short, strong).
Right stop.
Left again.

So that's three consecutive strokes left, and similar curves on the right. Seems like three to me.

If it walks 'like' a duck."
2267,so is this how biometric scanning works... or am i totally off?
2268,this is the best tutorial that i've¬†ever watched on youtube. thank you so much for the tutorial
2269,Best channel on youtube
2270,Would you allow me to reupload ur videos with a french voice for french students to benefit from it ?
2271,Thank you
2272,"Great explanation, thanks a lot"
2273,Damn built a neural network software just from watching these four videos
2274,ËøòÊòØÊ≤°ÊúâÁúãÊáÇ‰∏∫‰ªÄ‰πàÊääÈÇ£‰∏ÄÂúàÁöÑÂÉèÁ¥†ÊùÉÈáçÂèò‰∏∫Ë¥üÂÄº‰ºö‰ΩøÊùÉÈáçÂèòÂ§ßÔºü
2275,"Thank you, you explain it in such a amazing visual. Btw what is the software you use to make it?"
2276,When your brain is smarter than you
2277,Univs should pay you for replacing their crappy courses.
2278,This all seems very primitive
2279,"Amazing video, thank you!"
2280,Thank you so much for this amazing video
2281,"Very effective way to start a very random and complex topic with an example.

thanks very much. I learned alot"
2282,now I wish I watched this earlier when it could help me develop my topic of AI with Linear Algebra since AI uses neural networks.
2283,Man you are god
2284,teaching assimilations through presentation question
2285,love the explaination
2286,This is some awesome stuff. Great visualization for ease in  understanding.
2287,"Thank you Korean subtitle your my savior
I'm a really beginner and my professor gave us projects with no base, no explanation your my savior.....I'm serious"
2288,My dry erase board is erasing faster than I can write. Fml
2289,Wow.
2290,These visuals are great. Thanks for making it very easy to understand neural networks.
2291,I ordered Pizza with code beacuse I am a lazzy peace of shit
2292,"Sigmoid(Input value X weight + bias)
This is the value of the first hidden layer. But if we use that formule , all neuron in first hidden layer will be same output am I right?  Are we changing all weight value for each hidden neuron?"
2293,"I've learned more in 16'31"" than 2 hours of lecture"
2294,Sir do you have any Course on udemy or any related platform
2295,Well Explained.
2296,ËøôÊ†∑Â≠êÁöÑËßÜÈ¢ë‰ΩøÁî®‰ªÄ‰πàËΩØ‰ª∂ÂÅöÁöÑ
2297,ÊÑüË∞¢Â≠óÂπï
2298,I DO LOVE YOU SO MUCH!
2299,"youtube algo, take notes"
2300,this makes no sense
2301,The problem is not that you cannot control the AI but you don't really know how it's doing what its doing!!
2302,AI god will surpass human brain in just 4 years. Welcome to the world of equality based on utility...
2303,Korean subtitle is stop at 11min but I try to understand English ^^
2304,"Thanks. After watching this, I'm now a back-propagation expert working as a quantitative analyst for two sigmas and making millions."
2305,you are amazing
2306,We humans are a jellyfish of nerve cell tendrils that evolved a hard calcium shell of bones.
2307,good job
2308,"This is really inside diving on Neural Network. I highly appreciate the content, graphics or animation to use in explanation of basic and detail of neural network....specially with example....i think everything is very well organize..all the best and keep posting such neat and great videos...:)"
2309,"Well, the shape recognition SHOULD come as a result of coming up with an 11th output: None Of The Above."
2310,I have been recommending this video everytime after I've explained neural nets to someone.üôè
2311,"Thank you for the brilliant and extremely useful series. By the way, somebody must have told you before, but if not - your voice is uncannily similar to that of Neil Gaiman (author). I have been listening to one of his audiobooks recently (he reads some of his own books for audio versions) the very day I started watching your series and I had to spend some time googling to make sure he did not do the voice over for this series (not sure why he would do a voice over for video series on Deep Learning, but hey, stranger things have happened in the world - I found actor Russel Crow doing voice over for a video on cricket - it's a long story - Australia-cricket-cousins (Martin and Jeff) connections, but you get my point). As I said, if you are not already cognizant of the topic, just for fun, find a sample of Gaiman's reading on You Tube and let me know what you think yourself :) Thanks again for the superb series. Would wait for more interesting topics in future."
2312,What is biological equivlant of sigmoid function
2313,This is one of the best explanations I have seen
2314,I hold a degree in computer science from University of Waterloo. I took the AI course in fourth year and I learned a good deal about neural networks but I never could develop an intuition about what all those perceptions might be doing... This video fills in the gaps and re contextualizes a lot of things I knew and a bunch more things that I thought I knew...
2315,I need help... what does bias mean?
2316,"Thanks a lot for the explanation! It‚Äôs surprisingly hard to find a good video on youtube or explanation elsewhere on the web about neural networks. Your video has thorough yet easily comprehensible explanation, accompanied with clear voice and english, plus REALLY good animation to top it off. You cover how artificial neural networks work compared to biological neural network to the terms such as neurons, weights,  biases, activation, learning. I didn‚Äôt skip through that 19minutes. AWESOME! Thanks again!"
2317,"But, can this be used to prove that the earth is flat?




(Sarcasm mode is now disengaged)"
2318,but is the bias like a threshold you set. also why use RELu if it doesnt give u an output between 0 and 1?
2319,more cs videos pls!
2320,Sr i am a student of 11th class from india and is anable to understand this level of knowledge so can you help by preferring me those videos through which i can strong my basics
2321,"Isn't sigmoid justified by neural network be a big complex logistic regression model?
How ReLU can model a probability if it goes to infinity when its input grows?"
2322,"Really love this video, thanks man :)"
2323,"So are the layers determined by human concepts and require human feedback to train each individual layer?  
In this regard, would the AI of deepmind, especially alphago zero use only 1 layer as it is mastering the game without human knowledge?"
2324,Love your closing note...
2325,Brilliant introduction thanks
2326,ok so... if i get_gud at linear algebra... i can like. make my own neural networks to play games for me?
2327,Very Hell
2328,you're amazing
2329,"Too much math for a wrong dedication.If anyone cares much enough to ask, I can explain why. Otherwise I don't want to waste time."
2330,"Great video, really helped!"
2331,This is expert ‚Äòlayman‚Äô explanation of NNs. This is the only explanation that I didn‚Äôt tune out on.
2332,Pretty amazing huh?
2333,Machine learning = super-big-computationally-heavy-multi-dimensional-monster function ~ a dumb brain
2334,"wait, wait, wait, wait a minute... So... Why would everyone have to know about machine learning? ""Unless you lived under a rock..."" Easy with using your ass instead of using your brain, a murican boi..."
2335,Thankyou so much sir. Thank you.
2336,–û—á–µ–Ω—å –∫—Ä—É—Ç–æ!!! –°–ø–∞—Å–∏–±–æ!
2337,Vsauce - Mind field season 3 episode 3 https://www.youtube.com/watch?v=rA5qnZUXcqo
2338,Plz bring more videos on machine learning
2339,I lost in the middle.
2340,Very nice video but the faint BGM almost makes me think I had auditory hallucination. Please either remove that BGM or turn it up.
2341,nice
2342,still a mystery to me
2343,"The Sigmoid is not about squishification.

It's what introduces non-linearity. The RELU (Rectified Linear Unit or max{x, 0}) is another perfectly usable activation function but it does not squishify into [1, 0]."
2344,"The Sigmoid function is essential to this!

Take it away and the Network can only compute linear functions!

Just in case you needed to justify its use, though many different non-linear functions will do."
2345,I'm still confused...whether to learn about Neural Networks or keep appreciating your video editing/making skills throughout the video....
2346,thanks for the amazing video as a novice especially to this specific topic
2347,"Pure gold. Thanks to all the contributors of this series, the neurons of my brain are starting to light up."
2348,thanks indeed!
2349,So the first layer is like a filter ?
2350,"We found this very helpful, thanks!"
2351,I love you
2352,"A note on the analogy to biological neurons. The thing is although their ‚Äúinputs‚Äù and ‚Äúweights‚Äù are indeed some arbitrary voltages, the output voltage on axon - the ‚Äúaction potential‚Äù - is always the same, ‚Äúbinary‚Äù if you like, not depending of the inputs which caused it to fire. Furthermore, there is some state carried on, and overly excited neurons may fire multiple times in sequence, comprising some sort of time-coding. The point is that both sigmoid and ReLU are still quite rough approximations when it comes to comparing the models to the biological thing.

And there is more complications, like the dependency of the voltage propagation delays on the proximity of the an individual input synapse to the neuron soma on the dendritic tree, neuron specializations (like neurons with two axons), synaptic characteristics variance across different neurotransmitters in use etc.

The bottom line is: it should probably made clear that the math shown does NOT model the biological neural networks but borrows some basic principles."
2353,The Korean subtitle is cutted after 10 min in this video. I hope yourselves modify it to watch in Korean. Thanks always!
2354,"And why is it one way from left 2 right, for example should one lill circle not be able to forward messages back n fort"
2355,"Pitty that it doesnt feel like A.I got a soul, A.I is more or less just an Generic<T>  or Generic<A> Algo Bot"
2356,I need the code so I can copy paste it
2357,Can anyone explain me the activation meaning in neural networks please.
2358,"I watched this video 3 times from start to end occasionally rewinding to really understand everything. It was pretty tough for me but I'm so glad I got it in the end! Thank you so much, I really appreciate your work, it helped me A LOT!"
2359,"Finally a simple and clear explanation of the operation of this ""black box that everyone does not know how it works"". :)"
2360,Our brain will prune unnecessary information overtime as we improve our number identification skills. So what was necessary early on in identifying numbers will be removed from the process as more successful identification skills emerge.
2361,"YEAH, Object permanence..."
2362,it`s a good video
2363,"This video series helped me win my regional science fair. You're a gift from the gods, 3Blue1Brown"
2364,15:15 Whats that language even?
2365,"I  think I can solve this in much easier way ,, when the pixels fire's up ,, compare the data to perviously inputted data...  Say that you wrote something  ,, so you compare the pattern to all parents you already have inputted (1-9)  if it's similar to your  given pattern 7 the then the computer Will say it's 7  ... Simple as that I guess"
2366,ÁâõBÔºÅËøôÊ≥¢Â≠¶Âà∞‰∫ÜÔºåË°ÄËµö‰∏ç‰∫èÔºåÂ§ßÁ•ûÊûúÁÑ∂ÂêäÊâì‰∫∫Â∑•Êô∫ËÉΩÁïåÁöÑ‰∏Ä‰∫õÊ±üÊπñÈÉé‰∏≠„ÄÇ
2367,"14:11 you could actually say that result matrix has ""k"" rows =) just a tip"
2368,"Awesome!!! üòçüòçüòçüòç
Finally neural networks made sense"
2369,"Amazing video, thank you! 
I was wondering, do you have a video where you explain in detail the backward step with the detail of the optimization of differentiation made for trees ?"
2370,"Great video but you just threw off more than 10% of the men watching this when you showed a pixel pattern with green and red pixels. Don't use red and green please, especially to differentiate things."
2371,"Miss this kind of world since married to non ""scientifical"" wife, but I am still interested and delving into this kind of things ever since plunged into AISB. I wonder if you have the application for Musical Intelligence. Thanks for bumping into your video by accident which ""probabilitically"" not."
2372,"man!!! If only I would have been taught like that my whole life!!! 
Why the fuck are we using blacboards in 2018!!!!!"
2373,"ÌïúÍµ≠Ïñ¥Î°ú Î¥êÎèÑ Î≠îÏÜåÎ¶∞ÏßÄ Î™®Î•¥Í≤†ÎÑ§Ïöî
ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ÎèÑ ÎÑàÎ¨¥ Ïñ¥Î†§ÏõåÏÑú Ï§ëÍ∞ÑÏóê Ìè¨Í∏∞ÌïòÏã†ÎìØ „Ö†„Ö†"
2374,ÂëÉÔºåËøô‰∫õ‰∏çÊòØ‰∏Ä‰∏äÂ§ßÂ≠¶Â∞±‰ºöËÆ≤ÁöÑ‰∏úË•ø‰πà...ËøôÈÉΩÂê¨Ëµ∑Êù•ÊúâÂõ∞ÈöæÁöÑ‰∫∫ÔºåÂéãÊ†πÊ≤°‰∏äËøáÂ§ßÂ≠¶ÂêßÔºü
2375,"Now you can know why ""google captcha""  really works for."
2376,Really incredible dude. You literally worked on these animations to make us understand the concepts easily. Quite a tough job to pull out. Impressive effort and explanation skills which made me easy to grasp the concepts and made me interesting to think towards artificial neural networks. Thanks !
2377,"I'm having trouble determining a good starting point for initial weights and biases. I know they're random, but are there any general tips/ rules I should consider when randomising?"
2378,"It's 6 AM
And yes i didnt sleep yet."
2379,this is very informative. thank you  thank you thank you
2380,Can there be a better video! Thanks a bunch!!
2381,"Probably a very dumb question but... If you have the functions as they are, isn't it simpler to compute their derivatives and like solve a system to find out its minimum? I know it sounds computationally expensive, but computing gradients for all those values isn't so either? Thanks"
2382,"*First 30 seconds of the video*
_My brain is already wandering off so deep_"
2383,Hmmm............. is it possible to crack the weird antibot letters on login areas of sites with this if i make it not just numbers but letters too?
2384,thanks sir
2385,gj
2386,Awesome explanation.Can you please suggest me how can I use deep learning techniques for hyperspectral image classification where we have to learn both spectral and spatial features.
2387,thanks Master!
2388,Really good video and excellent visualisations
2389,OMG You guys just solved my year long quest of trying to understand this deep learning shenanigans. Thank you so much.
2390,"Our teaching assistant at a graduate level studies recommended us to watch these videos :)

Thank you very much!"
2391,"Nice video. But it is a bit confusing that you use n as dimension for both a and b, because bias could have different dimension than input, same as weight. 14:58"
2392,thank you..
2393,Does anybody any suggestion on good introductory books on the topic? Something from a programming perspective perhaps
2394,"3(blue).1(brown)4(in all)
3.14

K I'm not funny"
2395,Thanks from Pakistan.
2396,"that ""exactly"" voice in the end of the video scare the fuck out of me"
2397,"Can you please make some videos on statistics for machine learning, please? You have a natural talent to teach these things.. thank you so much"
2398,Wonderful tutorial! I feel like I have a much better understanding on Deep Learning now.
2399,"Please make videos about radial basis functions and Self organizing maps , basically content related to unsupervised learning.
Your channel is really inspiring and so helpful. Me and my co-students have already declared that this channel is better than the entire degree course that we have for engineering here in India. A genuine lot of Thanks."
2400,"Great explanation, but would your example network actually be a functioning one if it were to perform the task of digit recognition?"
2401,Wow! I'm really impressed by this video! Well Done!
2402,Please also do one on PCA
2403,Thank you.
2404,please do a video to introduce the entire team responsible for this channel. super stuff
2405,"otra forma tambi√©n se le llama reconocimiento de patrones, utilizando matlab XD puedes determinar gradientes y aculturaciones y maso-menos identificar los bordes."
2406,"Could we say that mathematicians have more layers of neurons in the brain, with fewer neurons per layer?

Thus we can understand higher levels of abstract concepts, but pay less attention to details in everyday life? (Absent-minded professor stereotype lol)"
2407,"Very very good video, thank you."
2408,This channel is a living embodiment of the Pareto principle (80-20 rule) where a small portion of the knowledge space helps you understand a *large* portion of the problem space!
2409,"I wish there was a way to make *exact* clones of this guy and post them to each and every school, college and educational institute, so that we could all benefit from his presence. Oh wait.... Youtube *almost* makes this possible... 

You are the type of teacher that all of us would *LOVE* to have as a role model! Hats off to you and the way you break down the most arcane stuff into understandable stuff. You are a GOD of education!"
2410,Something is seriously wrong with many people's lives in today's world. Maybe it's competition or jealousy or whatever.... Else why would they not think twice before disliking this video?
2411,"Awesome! Explained as simple as possible. If you think about the people who have combined their intelligence to form the concept how to enable computers processors to act like neurons, would you still believe that the human brain came all by itself without a super intelligent designer??? :D"
2412,"You've just given me a billion times more knowledge and understanding about ML & AI in 18 minutes, than my university did in 4 months of ""Artificial Intelligence"". Thank you.

Also, I'm stealing ""squishification""."
2413,could anyone land me a neural network layer to get what this dude is taking about ?
2414,"the most interesting thing it is that the brain uses the shortest route to to the neocortex, this in order to give information to take a decition, but that path is conditioned to a emotional state, such means that, I have automatized a process but if I am depressed I will take another route to bring the information associated with that specifically emotion."
2415,Weaving art!
2416,"you chose activation based on ""grey scale:"" how do you choose value for wieght I mean how can the computer asign the values for weights?"
2417,bababa
2418,Who the fook is Sigmoid?
2419,"Please upload videos about,  probability and stochastic process"
2420,"Ciao, a parte il video interessantissimo, posso chiederti che programmi usi per creare le animazioni?"
2421,my brain hurts
2422,9:32 R.I.P. colorblind people
2423,Thanks sir....Loved your explanation......Best ever introduction to neural networks.....
2424,Really bad vocal fry makes the woman hard to listen to.
2425,"please, how do you make animation?"
2426,"which program did you use?
what is it name?"
2427,It was too complicated for poorly functioning neural networks of 746 people.
2428,"I also have the same passion to study AI like you. I appreciate what you've learned in this video!
Thank you!"
2429,"This is probably the best video that I've seen on the topic of basic artificial neural networks (ANNs). Most of the videos that I've seen on the topic are either overly complex or leave out important information about ANNs, so you're forced to watch many videos on the subject to understand the basics. But this video gives you all the basics without making you feel like there's a lot of information left out. Granted that there's a lot left to learn, but I'm sure chapter two will get into some of that."
2430,Seriously one of the best learning series I have ever watched -- particularly helpful for visual learners!
2431,"Man, great videos! Your explanation rocks!"
2432,"Hi guys, i could not figure it out how bias work, can someone explain me please?"
2433,really high quality video and the narration was awesome...subscribed!!!!!!!
2434,Very sweet explanation - thank you very much
2435,"Good explanation ,please can you tell me  how to programming CNN in python to recognize image"
2436,I came here from coldfusion and singular prosperity
2437,You are one of the best YouTubers imo. Always love learning something new about math
2438,The best video of Neural Networks that I've watched. Love this channel!
2439,This is a great series! Thank you! I'm finally starting to get an understanding of what these things are and how they work!
2440,I feel DUMB
2441,"Thank God for this video. From the past few month all i am watching is pop culture related, brain deadening mush. This was detoxifying."
2442,As soon as he got to 14 minutes. ...my brain crushed
2443,My brain hurts
2444,Awesome explanation :)
2445,Love you!
2446,‰∏≠ÊñáÂ≠óÂπïÂ•ΩËØÑ„ÄÇ
2447,wa
2448,Nice work!
2449,"Thanks for the video. A pattern I have experienced with video recommendations is to find some channel, watch a bunch of videos on it, get recommended every video some channel puts out in the following period, then subscribe and see basically no more recommendations from that channel until I actively seek it out."
2450,How do we able to transfer neurons from brain to computer?Is there any device or any machine which could do that or is it still under process?
2451,"Afaik, sigmoid is perfectly fine and is even desirable in the output layer for a two-class classification problem or a regression problem with outputs from 0 to 1. Relu could instead give outputs larger than 1."
2452,Surely not many people fully under this complicated things. Even i learnt ai subject in university more than a decade ago it didn't fully clear up my mind at that time. Your video is very good by presenting it with an example of pattern learning of a letter and also proceeding to matrix representation.
2453,"Wait, why does the final layer of neurons even need a bias?"
2454,Can a neural network be used for for time series prediction?
2455,What.
2456,This is an amazing lecture! Thank you soooooo much!!!! Please give us more:)
2457,"Damn that was beautiful! Perfectly explained, what I feel is an extremely complicated idea,  in such a clear and concise way. Wow. Thanks a lot!"
2458,The characters that you use are so irrational
2459,Programs like this are what make all of the lame videos worth it. Thank you for jarring a few grey matter cells into gear. I want my children (who are great at math) to understand math the way you do. I sincerely appreciate you time and effort in making this video and all of the other you have made.
2460,"Or you could do it the easy way and give a computer a bunch images of each number, then tell the computer what numbers those images equal, then the computer would use data from those images,  so when you feed that computer an image of a number, the computer would use the data it was given to see what number it is. Its called image recognition."
2461,ÏûêÎßâÏù¥ Ïôú ÎÇòÏò§Îã§ ÎßàÎÉ• „Ö†„Ö†„Ö†„Ö†
2462,ÎàÑÍ∞Ä ÏûêÎßâÏ¢Ä..
2463,"Amazing you are!! Just awesome .. one request can you make some videos on expectation maximization algorithm , maximum likelihood , viterbi algorithm.. three concepts that I find are key to AI revolution.."
2464,"did he just say litt up?? :D Am i watching suits?? :D by the way, best channel ever :)"
2465,thank you thank you thank you
2466,AWESOME!
2467,Perfect video....Thank u so much....
2468,I do not understand why are negative for Weight?
2469,why start out with 784 though?  I don't get it?
2470,"Well, Sigmoid or not, it all depends upon the type of data you elaborate, and which network type you feed. The Sigmoid function represents roughly quantum bits, roughly, because it is extremely imprecise compared to qbits. However, if you are used to Sigmoid, quantum computing becomes pretty simple."
2471,"Please make a video on stack auto encoder,denoising,variable auto encoders"
2472,Looks like a sideways omega to me
2473,Awesome man..! It helped a lot. Thanks for this.
2474,"AI is some sort of dark magic and if you don't do the rain dance in reverse before you run your code, it won't work, at all."
2475,thank you so much! this helped me so much with my lecture!!
2476,What they teach in school about NN is not how the brain works. A blank NN does nothing. A blank brain learns.
2477,"After admiring moon for a really long time...I now feel, ""I just safely landed in Moon"". Simply brilliant!!!"
2478,"Parab√©ns, Excelente v√≠deo, apresenta de forma muito simples e clara. Great explanation"
2479,"This is the first time I see an educational video which actually made me feel I did learn the subject beyond ""general knowledge contest"" level. Before this, experience had shown me to disregard youtube as a meaningful learning resource. I feel encouraged to dig deeper on the subject!"
2480,You are amazing.
2481,but this neural network doesnt recognize a seven if its written more to the left or to the right. How do you deal with that?
2482,even „Çç is recognized as a 3 BUT IT ISN'T!!!   its ro in japanese FUCKTARDS LUL
2483,üòçüòç
2484,Such an amazing video!! <3
2485,ÿ™ÿ≠Ÿäÿß ŸÖÿµÿ± üò¢
2486,Best explanation ever....
2487,Thank you! This video is so nicely done - so easy to understand such a complicated (as I previously thought) concept.
2488,Thank You!! St√©ph.
2489,"what are u ? computer science prog, mathematician, phy.. hahaha liked"
2490,Best intuitive explanation of the subject I've ever seen on the net
2491,Learnt a new word today: 11:17
2492,"Thank you, your explanations are easy to understand and the content is well organized. 
Btw this little Pi‚Äôs are very cute"
2493,You sir are amazing.
2494,Good job !! Thanks
2495,Too deep for me.
2496,This neural network  is also used as NLPCA's algorithms!!! But thr structure is quite a little bit different!!!
2497,Amazing video
2498,thanks  for enrolling me .
2499,"More Machine Learning videos, please."
2500,"üìÇDocuments
  ‚îîüìÅVideos
      ‚îîüìÅ 3Blue1Brown
          ‚îîüìÅ Bad Videos
              ‚îîüìÅ Âõßrz"
2501,Amazing vedio
2502,"Hi, great video and well explained! Just a quick question, that may be picked up in later videos, but about neurons. You state that neurons hold a value, between 0 and 1. However, I was under the impression that neurons either fire (1) or do not (0). Could you please explain how a given neuron can take different values? Thanks"
2503,"sir, pls load more videos"
2504,"great video with great explanation of everything 
reeaaaaallllllyyyyy good work"
2505,"Great video and excellent explanations! Minor point: in the matrix notation should the vector of biases [b_0 ... b_n] read [b_0 ... b_k] since the inputs size is n and the number of ""neurons"" are not the same?"
2506,That's a very good video explaining a complicated topic. thanks for that
2507,12:40 a good visual representation of me trying to get skyrim mods to work together
2508,Wonderful video!! Thank you so much!
2509,"I didn't understand one thing , how the equation of the weights times neoron-index came with 16 different number (in the hidden layer) while it's the same weights every time ??"
2510,"you're genius , I would never stand it better even I have no background <3 big thank"
2511,"Awesome, it was tremendously informative!"
2512,R  E  S  P  E  C  T
2513,awesome awesome by far the best video on this topic
2514,"Perfect! One approach to understand and improve deep neural net is to use visualization. Features: design during training, exploration and debugging with instances and subsets, development with dataflow graphs."
2515,–ù–∏—Ö—É—è –Ω–µ –ø–æ–Ω—è—Ç–Ω–æ) –Ω–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ)
2516,Thank you so much. This was really helpful
2517,Subscribed... do your thing neural net.
2518,"The fact that there are 13,002 possible connections and an even larger number of possible configurations is pretty awesome, especially considering that this is a fairly small network designed for a very simple task. Makes me wonder about how my own brain can function so intuitively. The average human brain contains about _a hundred _*_billion_* neurons, all of which are connected to one another with their dendrites and axons acting as input/output, respectively, passing electrochemical signals to one another through nothing but highly complex yet unusually organised organic chemistry. And what is really rather remarkable about all this is that I am able to describe my own cognition as it occurs, albeit in high-level terms that say nothing about exactly which neuron is doing what.

How it is that I can replicate a much-simplified version of this on my computer is still blows my mind every time I think about it,"
2519,"This is an incredible video, I've been studying Deep Learning and Neural Networks for six months and by far this is the best explanation on Deep Learning and Neural Networks that I've seen so far. Thank you"
2520,Very high quality indeed
2521,This is how you teach any rocket science like cake walk !!
2522,"One question to ask is why have 10 output neurons? One neuron would be adequate - it would be trained to have a value of 0 through 9 (as a floating point number). Perhaps a second output neuron could give a measure of confidence that the answer is accurate.

If MLP networks are trying to replicate the workings of a brain, then it does seem unlikely that the brain has a dedicated neuron that fires when, say, an image of a 3 is seen by the eye. Of course it's equally unlikely that there's a single neuron dedicated to firing when any number is presented to the brain.

Experiments with MLP networks have convinced me that often more neurons (particularly hidden layer neurons) make detection less reliable. This seems particularly true when the input data set is fully known (I.E. the input image is known to be one of a large but finite set of images). 

Another noticeable feature is that the value of the weights in the network is wholly dependent on the initial values (usually randomly assigned) before training. Null out the previous trained network value and retrain with a different set of random initial conditions and the compare the weight values with the previous network. They will be completely different."
2523,"At 14:37, bias matrix will end with b_k instead of b_n"
2524,This is an incredible series. Thank you so much
2525,This is the best video i ever watched.
2526,"That's pretty much the way you learn japanese Kanji. For an untrained eye, they look like a bunch of random strokes, but they're made up of smaller components called radicals. So when seeing a Kanji, your brain recognizes the subelements and their placement to link that information to a certain reading and meaning you've connected it to when learning it. They become distinctive enough that you can store information on thousands of Kanji with several readings for each, forming various meanings depending on their combination, while being able to identify several of them per second while reading."
2527,"While the last part about multiplying matrixes was still a little complicated this is still a great video.  I'm a graduate student who, before taking any of my current classes, had zero experience in any field such as machine learning.  But this video made it very easy to understand and I now feel like I have a good grasp on what learning using neural networks means.  Thanks for the great video!"
2528,"Question:  I'm confused as to why the sigmoid function is represented with lowercase sigma (which makes me think standard deviation, tbh). If it represents the sum, wouldn't it be uppercase, or am I just being nit-picky?"
2529,This was EPIC!
2530,Oh my God! - this is really amazing video! I enjoyed watching this and now I can explain this to my friends!!
2531,"Minor critique: Perhaps a bit late in coming, but anyway. At 14 minutes, you explain how the matrix mult'n creates a vector product (of question marks ""?""), and  show a brief animation of the production of vector elements a(1)_0, a(1)_1, and a(1)_6 (only 6 output nodes in this diagram). This makes sense looking at the neural net diagram; however, the equation above is confusing because it doesn't change with the animation, even though it's calculating different elements of the output vector. Ideally, the eq'n should update its indices to match the animation. E.g. Update a(1)_0 = S(w0,0 * a(0)_0 + ...) to a(1)_1 = S(w1,0 * a(0)_1 + ...),  etc. IMHO, this would make the connection between the diagram and the eq'n much more obvious and intuitive."
2532,"The Czech name and description of the video don't make sense. Please stop using Google Translator, it's much easier to read it in English than to decipher broken Czech‚Ä¶ üò£"
2533,I want this to take my job although I'm proud to say I can now read doctor scribble. Train this to read doctor scribble lol
2534,"Thank you for how you explained this. I've been sent here by someone trying to change my opinion on Machine Learning, after this opinion has been wrecked by the mainstream media. I was introduced to the subject by press articles that started popping up last year, most of which could be summed up as ""hahaha we found a way to magically give computers human intelligence, now we can recognize people's faces and block porn and do other creepy repressive stuff"". Over the past year I've been convinced this is all a made up fantasy created to satisfy delusional Robocop fans with Orwellian tendencies, as well as a cover up for manned mass surveillance. I still refuse to believe that some of the highly advanced content recognition can ever be done in binary code, but I cannot ignore that there must be at least some truth behind all this. It's funny to think that if the world wasn't so damn stupid I might have looked into the subject earlier instead of becoming one of its biggest skeptics and haters, since I've been a programmer for over a decade so it's not all new to me... sadly the stupidity of society ruins everything it touches. I guess it is never too late though, and what is true of this could be very interesting."
2535,"Could you PLEASE do ""Convolution""? I want to know where does the ""Tao"" come from?"
2536,From where can i learn the object recognition  or which modules i have to go through for object recognition?
2537,"How science affects our thinking:
Let's look at the first seconds of this video. The voice makes the point that we should appreaciate how ""crazy"" it is that our ""brain"" can recognize a handwritten written number, while computers are traditionally so bad in doing the same thing. While this is the introduction to neural networks and stuff I just wanted to make the point that it is not ""crazy"" at all, that humans can read a number they have invented themselves. Math is a human invention like is science and eg. the manner of how we perceive time. (Read ""An essay on time"" by Norbert Elias if you're intereseted)."
2538,You do an amazing job of explaining this topic in a very vivid way! Thanks!!
2539,Wow...just WOW. Your ability to break-down complex topics is mind-blowing. SUBSCRIBED.
2540,Êó•Êú¨Ë™û„ÇÇ„Éº„Éº
2541,This is one of the most Awesome video I have ever seen
2542,Best explanation on the internet with visualization about NEURAL NETWORK!
2543,This man is a God.
2544,I love your channel man! You are awesome! :)
2545,"Obrigado pelo v√≠deo! Brazil, Pernambuco, Recife \o"
2546,awesome sir..brilliant
2547,"17:05 Lisha Lee has such a Hard ""VOCAL FRY"" (Look it up) it becomes very HARD to listen to her, HARD to interpret the consonants she's saying, HARD to gather the words she's saying. ""VOCAL FRY"" is a vocal mannerism kids pick up in college, trying to sound more profound and learn-ed. Ending each sentence with a downward turn of MORE FRY, and as sentences go on, more and more sustained FRY, indicating greater and greater import in what they are saying. Problem is the more FRY they turn on, the more and more, all that's left is the POPPING of the vocal chords together, and less and less of the other frequencies that illuminate the consonants that articulate the clarifying symbols of the words being said, leaving the listener with more and more occasions of duplicitous words to assemble into coherent meaning.
We industrial computer programmers used to say about the ""Clarifying Comments"" we were asked to embed in our CODE, so that following trouble shooters or re-programmers could more easily understand how the various sections of the programs operated, we would say, ""If it was difficult for us to write the code, it should be so much more difficult to read the comments!"" 
In fact, the CODE was so complex, the only ones that could read the code comments, were people who had traveled down almost identical paths as the original programmer, and nobody else could intelligently determine if the original programmer had filled in the commentary correctly, done his job or not. All that anyone really knew was that the program had operated correctly up to the current moment in time, and the future performance was a  vague prediction of past performance. Unknown future conditions would precipitate unknown future results.
I offer this explanation in support of the contention that I know of what I speak, and VOCAL FRY, IS NOT AN INDICATOR OF PROFOUND COMPETENCE, but rather is merely ""eye wash"" thrown in the face of critical observers intending to obscure REAL CRITICAL OBSERVATION of any significant competence, and create just confusion and artificial AWE to others. 
Interestingly I used the Audio Spectrograph Application loaded on my phone, and I could see the clear articulation and fundamental frequencies of the male speaker, and then watch Lisha speaking, and the more VOCAL FRY she turned on, the more the fundamental frequency of her voice disappeared into the NOISE of the other frequencies created by the FRY, AND THE LESS INTELLIGIBLE HER DISCUSSION BECAME. Visually less and less clear and articulated, and WHERE TO LOOK FOR THE CONSONANTS.....  WOW Very informative. 
Lisha you might want to MATURE, and get that fixed. People can see right through that VOCAL FRY, and KNOW that you are faking as much as you can. I know you are smart, EARNED that Phd. but you need to get rid of that ""TICK"" in your voice, ""It is masking your true smarts!"" Be ""genuine,"" don't fake being fake, it doesn't become you."
2548,"plot this in tan(pi/x)

2:20 this reminds me of something with matrices"
2549,"wow..wow....wow......i saw this video a while back, thought I understood it well, enjoyed it,,  went back and did some reasearch on machine learning for a week, came back and saw this again,, now im amazed again"
2550,*Squishification*
2551,yes are you the same who teaches on udacity
2552,"People like you are the reason why we are such a powerhouse of a species, thank you."
2553,"Congratulations, you make the best video online about introduction to deep learning. :) Incredible well explain ! you have one more subscriber"
2554,"I wish I could have a teacher like you, u're more than anyone, better than paid courses, really man!"
2555,Squishification xD
2556,Thank you! :)
2557,20 mins video is way better than 4 hour lectures! I really love your video so I became a patron <3
2558,Great video! Hope you upload more videos about machine learning.
2559,But just why has it become trendy to Title youtube videos beginning with the word 'but'?
2560,Thank you so much. You explained this so well and easy to understand. :)‚Äã
2561,Like John Snow.... I know nothing!!!
2562,"Omg I love your video so much!!!!! Really easy to understand ^^
However, I'm a new and a self learner, and I find a detail that it's a little bit confusing to me, otherwise, the content is GOLD!!!
Anyway, in 14:39, in the matrix where the biases are in, I believe the number should be going from b0 to bk instead of bn.
Because there are only k neurons in the next layer, so therefore only k biases are needed, right?
Please correct me if I'm wrong ^^"
2563,"hmm...... what if the font uses a square or a rectangle for a zero with no curves?.... how is it seen as zero? same for two boxes separated by 10 pixels but are an 8 in the font? ....... fonts are annoying sometimes but how to get this to work in the network?  do we have to add more neurons for the square and more for the rectangle?  also what about fonts that use a triangle for zero pointing down and then two triangles pointing at each other for 8 or for infinity and then a triangle pointing up for the number 1 ?  i have no clue what to do with this, if you find some time could you give it a go please?"
2564,"Why do all the geeks come up with new words for existing things ""deep learning"" why not call it ANN as it has been called since the 60's."
2565,I still dont get it. I guess I'm stupid. Sigh..
2566,Brilliant-ABSOLUTELY
2567,Excellent tutorial ! Thanks.
2568,"It looks like there might be an error in the bias term (b0, b1, b2,......)  in W X A + B. The highest term of b should be bk (matching wk) not bn ?"
2569,I could not be happier finding this in my recommended
2570,Sorry for the a stupid question for such a brilliant video. I notice the ending equation is that of a line if you remove the sigmoid. Is this what makes this a linear regression model?
2571,you had the most encouraging way to make me subscribe to the channel...
2572,Wow! just wow! hands down the best intro to neural networks I've seen on the internet. Can't wait to see more videos ! Subscribed !
2573,"Question: Why isn't the network given the option to determine the picture as non-identifiable? Is this omitted for this example, or would it simply interfere with the learning process somehow?"
2574,"Parece mesmo a origem da complexidade dos alfabetos MAIAS, e significado do impressionismo paraid√≥lico nas imagens de √©co em modo infinito de fractais."
2575,Thank you sir! It's very helpful!
2576,bish at the end gonna need  a lozenge if she keep talking like that
2577,YOUR PIES ARE CUUUTTE!!!!
2578,"Your approach has simply explains the concept and I learn first hand however my interest is using such concept for knowledge extracting. I know there are other concepts seems more suitable for that, however neuron network has more relevance to structure generalization for my requirement. So the question is Can you use this concept to traverse these nodes for path or circuits data extracting?"
2579,"I'm not sure I understand how making some combination of at most 784 dots light up a single dot qualifies as ""intelligence"" or ""recognition"". It's still on the end-user to actually label that last line of dots from 0 to 9 and interpret the results. Aside from neural network's description as being loosely analogous to a ""brain"", it's just an overly glorified feedback system. Look up CPM and PERT programs for public works management...I love roads that are built behind schedule and over budget! Thanks deep- learning machine A.I.'s for taking the burden of incompetence away from our politicians and their constituents :)"
2580,ÏïÑÏ£º ÏûòÎßåÎì§Ïñ¥ÏßÑ Î©ãÏßÑ Í∞ïÏùòÏûÖÎãàÎã§. Îî•Îü¨Îãù¬†Î∞∞Ïö∞Ïã§ Î∂ÑÎì§ÏóêÍ≤å¬†Í∞ïÏ∂îÌï©ÎãàÎã§.
2581,This is so much easy to understand AI
2582,You explained the concerpt like a boss
2583,please make a math youtube podcast
2584,Brilliant. Thank you Grant
2585,what are weights?
2586,very well explained
2587,programmatically this is basically permutations of the if then else loop and can be handled with nested logic.
2588,why is he yelling. we just want to learn.
2589,So at the time of my comment 658 people disliked this hell amazing video. My neurons couldn't connect the dots.
2590,TOOO FAST... DIFFICULT TO GRASP.
2591,Thank you very much
2592,"Is anyone else nominating this series for the ""Distill Prize for Clarity"" in 2019? I really think he deserves it, excellent visualizations."
2593,"Fantastic video! I'm taking a class on Machine Learning and your videos are really helping me put it all together. Your presentation style, pacing and content are all great for a novice such as myself. Thank you."
2594,This was so incredibly well done. Thank you and please keep it up!
2595,Does Clarify neural network in a simple way!
2596,"This is one good explanation, Kudos. Very hands on, focus on principles not on numbers."
2597,"üìÇDocuments
  ‚îîüìÅVideos
      ‚îîüìÅ 3Blue1Brown
          ‚îîüìÅ Wonderful videos"
2598,make a video on monte carlo methods
2599,would you statically define the biases and weights for each image / sub-image?
2600,My brain hurts from this but still a great vid
2601,pure gold
2602,I wish you were my 'everything' teacher when I grew up.
2603,"Awesome video .. I had a question - around 14:45 there is introduction of bias as a column vector. From your videos, I got that it‚Äôs connected with a neuron in the next layer, so the column vector for bias should have length k and not n?"
2604,Fantastic! I've been trying to wrap my head around this for a while and you made it very clear. I also appreciate the background provided for further exploration. Thanks!
2605,"thanks man, ive got a back propagation project coming up and your videos are helping me."
2606,You are really awesome.... Thanx a lot.. üòä
2607,What is simple definition of neural network?
2608,"viewer nr. 2.5 million, woohooooo"
2609,I didn't understand the difference between activation and weight. Someone please helpüòÖüòÖ
2610,"This is complicated, Jeez!!!"
2611,Awesome.
2612,Best video on the neural network I have ever seen
2613,"I feel like if I had known that I liked stuff like this in high school I would have paid so much more attention in math class, as well as put in more effort. These neural networks seem like much more complicated Logic gates."
2614,Thankyou
2615,"Hello!
I was training a NN on a 90-parameters dataset where some of them are expressed as weights and some as binary inputs. I chose rprop with 10 hidden neurons and a 4 classes classification output. The point is It is not converging and I don't know how to control the overfitting."
2616,why a matrix vector product and not scalar product?and in the end it was said how does the program figure out weights and biases while i was under the impression at least some them were input in the program.a little clarification there would e wonderfull.a very informative video nonetheless.
2617,Is it possible to add a 'default' objective so that when it doesn't find any good matching numbers it would point to an 'undefined' category?
2618,Thanks so much for adding Arabic captions!!!
2619,What happens when the image is  not B&W and has RGB ? How does the input layer forms then?
2620,Good intro ... thanks.  Someday ... maybe I'll dig deeper into this.
2621,"Why use the sigmoid function? Coudnt you have used a trigonometric function, since sin and cossin also brings the real numbers into values between 0 and 1? (If you take the module that is, so |sinx| for example)"
2622,"So ""Artificial"" Neural Network works just like this Neural Network?"
2623,What an awesome vid.....
2624,"awesome video, best explanation and clear concepts."
2625,What happend when we upload   alphabet not number.
2626,"Make more videos on ML ,PLZ"
2627,"TIL you can use ""squishification"" in complex mathematical videos. I doff my cap."
2628,dude you're too smart man. Explanations? Perfect.
2629,A great explain. Congratulations.
2630,"Still waiting for the LSTM video, please, please, please..."
2631,This video amazingly explains a difficult concept into such simple terms which even a layman could understand!! Simply Wow
2632,Make more  videos on Machine Learning and Deep Learning.
2633,Why do we have weights when activations are already expressive of the grey scale value of the pixel?
2634,Wouldn't that be funny if you select a number and the computer wrote the number different each time
2635,"Awesome video .I just understood activation,sigmoid and bias in the most simplest way."
2636,2:48
2637,"Dude, you're fantastic! congratulations for the extremely good job!"
2638,"Every single video from 3Blue1Brown has honestly never failed to peak my interest, even if I don't have a full understanding of the topic itself. As a maths student, I love indulging in these videos ahah"
2639,"Nice video, TY!"
2640,"This series of fours videos is really well done I like it very much, much appreciated"
2641,Great Tutorial on Neural Network. Thanks i will watching your other videos too. One like!!
2642,"I'd like this stuff, but it's hurt my brain so deep, :'("
2643,"Great video, thank you. Very interesting stuff :D"
2644,"So, you COULD use a forced, paired ranking matrix to establish the WEIGHT of each neuron, and use a least squares regression across the neurons to set the BIAS?? Then all you have to do is deal with inverse matrices....easy peezy....ya sure...sharpen the pencils.

I would REALLY value all of this, if we could ‚Äúdry-lab‚Äù clinical trials with it. For example: Senile Macular Degeneration affects 3/4 of the population over 85. It would be difficult to start administering a preventative at age 65, in order to see results at 85. Moreover, there is no good animal model to test in. How do you justify going into humans. Cancer trials are similar...by the time you can enroll a subject, they are near the end of SEVERAL ropes."
2645,Very accurate and efficient explanation!
2646,Hey!! Please do video on CNNs please...
2647,"At 14:50, shouldn't the bias vector be [b0..bk]?
It wouldn't work mathematically otherwise, and we're talking about the bias for second layer which should only have k neurons."
2648,And now squishification is my new favorite word
2649,What an amazing explanation !!! Hat's off man.
2650,"If you added a control loop into the process, so that there was feedback into the neural network on whether or not the neural net was correct in its identification of the numbers and had the ability of the code to automate and rewrite the ""black box"" weighted values based on it success rate to home in on more efficient black box function would ""learning"" a single task like number recognition essentially be trying to stabilize around a unknown function mapping?"
2651,"I think there's a slight error in the matrix representation at 14:38. If I understand it right, subscript n is the number of input neurons and k is the number in the next layer. The result of the matrix product shown will be a vector with k components, so the vector of biases b should have k, not n, components. This makes sense -- there is a bias for each of the neurons in the second layer of size k."
2652,Sounds more like how an Encryption/Decryption program works.
2653,I just know the function of hidden layers by watch this video ; thank you üëçüôèüèªüòç
2654,I wish i could double thumbs up! your stuff is amazing!
2655,I need more Korean translations...
2656,"I like to think of the neuron and the associated weighted sum as a Finite Impuls Response (FIR) filter with a nonlinear function added to the output. This is very convenient when working with time domain signals, but keep in mind that 2D FIR filters are also used in image processing. The training of the NN becomes then analogous to tap coefficient estimation."
2657,"That exaggeration on ""is"" is indicating the vsauce style , I know it blue brown üòã‚ô•Ô∏èüëÄ"
2658,This video is really great. Thanks a lot
2659,"the best visualisation i ever seen, thanks"
2660,The is by far the best explanation of neural networks I have seen. Thank you!
2661,BEST
2662,"Este assunto foi muito bem esclarecido, muito obrigado amigo."
2663,You sound like whis from Dragonball (English version)...
2664,Blasz you
2665,"Very good, 
I do not recommend"
2666,"I would like to have heard the word ""filter"" used since, that's what's going on."
2667,"I've spent 3 years studying math at a university level, and I'm just now getting this. I don't know why it took so long, but thank you."
2668,what a video.... your videos are miraculous and I am really not exaggerating!!
2669,Absolutely fantastic video. Insanely good production value. Concepts explained very clearly. Thank you so so much.
2670,Can we do it backward ? like we input a number and it will draw this number or maby we could input text and output an image with the text writen
2671,Pure golden teaching!!
2672,I would say this is the best introduction to neural networks. Thank you so much for making this video accessible to everyone. Thanks again. Thanks thanks many thanks to you
2673,This is pretty hard for me to digest.
2674,"This is a Rumelheart back propagation neural network, UCSD San diego, 1980's.  He seems to be totally forgotten."
2675,"14:41 in the vector we multiply, the lower index at a_n shouldnt be a_k? same b_n and b_k. We have k rows in the weights matrix."
2676,All ya gotta do is layer multiple neural networks next and you'll make a brain.
2677,awesome...
2678,only one work awsome
2679,"please make one on bayesian inference and methods like quadrature, mcmc. etc"
2680,"First üëç
Then Comment"
2681,Waiting for the essence of probability and statistics
2682,How to measure weight?
2683,Amazing explanation!
2684,"Deep Learning is a great way for terrorists, disgruntled postal workers, and students with test anxiety to program an artificial intelligence truck or car full of explosives to go blowup a university, airport, governmental installation or their grandmother's house."
2685,Disliked. You introduced so many misconceptions into the subject.
2686,"Squishification, indeed."
2687,Simply awesome presentation
2688,the best 19 minutes of my neural network intro! Cheers!
2689,Designers love making chairs. There's thousands of different style of chairs. Plato said all objects are derived from the ideal god-like object. So according to Plato there is an ideal chair god-like chair from which all chairs derive like a template.
2690,"I'm new here, very new! grade 8 math to be precise, Me=Dah! but let us see..."
2691,"Nice videos. Just one thing... Contrary to popular belief, red and green are NOT ideal contrasting colors because 8% of the population is red-green colorblind like me. Imagine coming to one of those single blinking traffic lights and not knowing whether to wrongly stop and get beeped at or wrongly go and get smashed. It IS a big deal. #colorblindadvocacy"
2692,Great!
2693,the most beautiful video on neural networks for just understanding
2694,Lisha Li <3
2695,Nice video! Very cool how you managed make it understandable for someone like me who has minimal maths and no programming background. Thank you!
2696,Perfect animation + explanation. Great video !!!
2697,"hi ...could you share , how you make this awesome animation explanation?"
2698,"If neural networks are in fact advancing as fast as reported... Very effective Robos will be around in the very near future... designers of these Networks figure out how to have neural networks respond to pain pleasure like dislike anger Joy Etc, we will have created the Megalodon known as singularity... Benefits and dangers... look out world..."
2699,Well done...What software you used for animation of the whole video? Thank you in advance.
2700,"Thank you for those beautiful videos, and clear and intuitive explanation"
2701,I love 3blue1brown videos so much. I wish I could support by becoming a patron but I can't cause don't have an income.
2702,Induction
2703,ossum video :  )
2704,"Hi I just first watched this video and subsrcribed your channel right away for the accurate concepts that helped me out start understanding network.
I got a quick question to ask though. I agree with the rectifying style of ReLu which is more analgous to the real biology. But if it's the max between 0 and a, it doesn't mean it's always below 1.

So the question is, do the numbers  of neurons can be any positive value?
If not, how do you regulate the ReLu values? Perhaps divide it by a?

I really want to hear back from you.  Thanks a lot!"
2705,Great video
2706,"Why don't you just make the weights add up to 1, then you wouldn't need the sigmoid"
2707,as a German I can't distinguish American 1s and 7s
2708,Easy to understand. Good explanation.
2709,"your animations are amazing, and your way of explaining is simply superb !!! and most importantly, what you speak makes a lot of sense, far more sense than many so called ""experts"" , who just copy paste a random bunch of formulas without knowing why and how things work. BTW your linear algebra series was also amazing, I wish i could like each of your video at least a dozen times"
2710,ËÆ≤ÁöÑÊå∫Â•Ω
2711,The best explanation
2712,this program was written to count the relative amounts of each total 28x28. if it has like 340 out of 784 then its 3. i feel like thats not the right way to distinguish numbers but if it works then gg
2713,3Blue1Brown is god
2714,I'm in love with this channel. I just watched the entire linear algebra series and now that I'm starting to study data science I found this video. What a great work you're doing!
2715,"Amazing videos dude!
Really nice explanation, I have done some courses on udemy, but none are as good explained as this."
2716,hay
2717,did you eat leeshas taco??
2718,"At around 14:40, the bias vector should be till 'k' and not 'n', as it should correspond to the number of neurons in the next layer. Also, kxn matrix multiplied by an nx1 matrix is kx1 and not nx1. 
Please correct me if I am wrong"
2719,"Building¬†machines that can not only learn from their environments, but judge the risks
that they‚Äôre taking is key to building smarter robots"
2720,eh
2721,"üìÇ4.0
  ‚îîüìÅAI
      ‚îîüìÅ Deep Learning
          ‚îîüìÅ Not seeing
              ‚îî‚ö†Ô∏è How to Understand woman"
2722,Excellent !
2723,so good video
2724,The part that I couldn't get is that you said that each of the neurons in the second layer has its own bias(which was 16 biases in the example).But in 14:35 we added the bias vector with n elements which is the number of the neurons in the first layer. Shouldn't it(the bias vector) supposed to have k elements which is the number of the neurons of the second layer? Sorry for my bad English.
2725,Subscribed !! :)
2726,Jesus Christ this guy explains so well
2727,"Thanks for Videos have posted on youtube, it's really awesome."
2728,"A human might ask, how many loops are there? This can be solved by asking if its possible to start on any bright square and travel along adjacent white squares until you arrive back at the start (without retreading on a square). Having no loop rules out 1, 2, 3, 5, 7. Then, is the loop high or low (6 or 9) or double (8). I wonder if these kinds of directed questions can be integrated in the NN?"
2729,"Wow. Isn't it weird that our brains can accomplish such a complicated feat but most of us can't even promptly answer double digit multiplication, etc. Wow the human brain is really complex!!!"
2730,the best
2731,This concept is used for PCA and NLPCA
2732,"""Primed to believe...""
fuck yeah!  Well put Yo! subscribed...."
2733,nice tutorial written in java https://github.com/evoneutron/java-mnist-tutorial
2734,BRILLIANT !!
2735,"I'm a CS Student and i have a Deep Learning exam soon. This video is such a good refresher and a hell good summary. Thank you, i've been watching your videos since my first Semester."
2736,"at 14:40, shouldn't the subscript on the last b int eh column vector be k?  are there n biases or k biases?"
2737,Most shitty explanation I've ever seen on the NN (except the math it is pretty simple and good and no way you can make a mistake). Thanks for your efforts.
2738,I learnt calc just to tackle this series. :)
2739,"great video, had just subscribed!!
i MUST SAY, hearing that female voice 17:29 out of nowhere got me thinking, ""Crap, some stupid p*rn ad got opened in the background tab"". Talk about Neural Network ;)"
2740,Loved it!!
2741,But then they keep saying you dont need math in Computer Science when everything is represented in math notation or solution
2742,"Great video!

Just one comment. The hidden layers also need to be non-linear. There should be an activation function for each hidden layer's output. Typically ReLU or tanh. If you do not have a non-linear activation function, the neural network will only learn linear functions and since Linear function of a linear function is still a linear function, adding any number of hidden layers will have no effect to the network"
2743,Awesome content !!  Best channel I ever found on Utube thnkz for educating and sharing ur knowledge
2744,Great introduction!
2745,Just finish learning Matrix and currently learning Vector. Great to know these will be useful to apply into AI! Very interested in AI and hopefully one day I will have a startup on AI related stuff.
2746,"Amazing, thanks"
2747,"You really have an amazing channel and concept. Any subject can be easily understood if taught the right way . And most science subject like physics , math etc. are understood by visualising it . Einstine use same trick just instead of computer using his brain . If math we're taught to me this way in school too I would have loved it even more. Thank you for such a great experience."
2748,cool videos thanks
2749,"About the hidden layers:  breaking up the numerals into the sub-shapes you describe may work, but it's the wrong approach.  When learning arabic numerals, the alphabet, or han characters, teachers emphasize penmanship and getting the stroke order right.   All that is going to show up in the finished handwriting (particularly in the relative darkness of ""inked"" pixels), and it seems to me that literate adults will use that data (along with context clues from adjacent characters of course) to correctly interpret difficult handwriting.  For instance, 9 is not a circle with a tail.  It's a full vertical stroke with a ""C"" added to the top left side."
2750,I just get so depressed when i see those mathematical formulas being thrown around from out of nowhere..
2751,You're just awesome man!!
2752,"The visualization is simply superb...I have been reading DL books for weeks...and still don't get the concept.
But by watching these 3 videos -- I skipped the mathematical video :-)  -- everything is crystal clear now -- vielen Dank"
2753,Lol captcha.
2754,Anyone else see this after playing destiny but then actually get really interested and watch it
2755,7
2756,Nicely explained. Thank You.
2757,nice and clear description. thank you
2758,Ïû¨Î∞åÍ≤å Ïûò Î≥¥Í≥†ÏûàÎäîÎç∞ 8Î∂ÑÏØ§Ïóê ÏûêÎßâÏù¥ ÎÅäÍ∏∞ÎÑ§Ïöî.. ÏïÑÏâΩÎã§
2759,Awesome video! Can you please do a video on RNN and LSTM?
2760,All well and good but the thought of programming it is where I am both horrified and excited.
2761,Helpful video üëåüëåüëå
2762,"Ah, now the world thinks that neural networks are the sacred new insight that will elevate humanity to new heights :-) Not true, because neural networks have no knowledge or intelligence to model reality. They depend on programmers to give them a model. The problem is that human beings do not even agree on how to model reality properly (although it is part of their unconscious competence)."
2763,"I actually took some notes from this and after going through the video several times it started to make more and more sense, so if you don't understand once watch twice and write down the key points that he mentions cuz it makes it a whole lot easier to understand as you go through the video"
2764,Thanks!
2765,Wow this is a rare gem which successfully and accurately describes the essence of ANN. Kudos. You just won a subscriber
2766,"Sigmoid ""squishification"" function. Lol."
2767,"Thank you, I am really liked video.i like way that you present video and I am intersting to know the tools that you used to present this video"
2768,"Wow, really great video. I've been studying all day trying to learn how Neural Networks work, and this so far has explained it better than anything else has. Thanks a ton!"
2769,Still I don't understand how human visual cortex manages to recognize numbers that's written differently each time...
2770,This video was incredible!!
2771,My peanut of a brain cannot even begin to comprehend this :D
2772,Awesome explanation!!
2773,Awesome!
2774,"I don't get how the position of the digits does not seem to matter when it comes to recognizing edges of a specific digit. Like a loop for instance, how can the network recognize a loop if the loops are in different positions."
2775,"It wrong way, but right direction"
2776,"very educational, thought id never wrap my head around it. Thank you!"
2777,Incredible teaching talent and skill. Mind-blowing.
2778,So its a guessimg computer and nothing else
2779,thank you!
2780,You are cool
2781,5:25 i think this program is smarter than me i thought that was a 6
2782,When you‚Äôre colour blind and someone uses green and red to pixels to illustrate a point...
2783,"Thanks for the video 3b1b. Please make a series on Reinforcement learning and Augmented Random search algorithm (ARS). These are the bleeding edge concepts in  Artificial Intelligence now. Since the research papers on these topics are math intense, your explanation will be very useful."
2784,"A whole video about a topic I love, but when I see a cute Chinese girl popping up unexpectedly, I have to holler at her.

EDIT: This was the ""holler"". 17:08"
2785,lee got kim kardashian's voice :P
2786,"Really nice video. I especially liked the token woman at the end. Great work, keep it up!"
2787,"Science is converting and deliverance, it's like capsule for the mind."
2788,#Corruscant !
2789,Squishification?! Squishification?!
2790,"Does ""784 neurons in the second layer"" mean there are 784 regions to consider whether or not there is a edge in each of them?"
2791,I subbed
2792,Excellent Eplanation dude. Thank you.
2793,"Dude, this video was amazing! Absolutely fascinating! Thank you so much for all of the effort you put into your content. I really enjoy learning about mathematical concepts like this, and you make it so easy to grasp. Cheers! üôÉ"
2794,"It was a long beggining, but the rest of the video was really helpful. I liked the way you have to say ""Suscribe"", that's why I did it :)"
2795,quantum computing baby
2796,Cool video
2797,JUST AWESOME video man I loved ! BEST ON WHOLE YOUTUBE
2798,I would've chosen 69 though.
2799,The only thing left is to teach an AI to randomly generate numbers. It's incredibly useful.
2800,Neural Networks are awesome
2801,"The next generation of training (literally). Teaching in a spectacular and enjoyable way. This animation is the form and the way to start learning neural networks in a way that makes you want to sit down and start writing code.
Good job. Go on like this. I'm your fan."
2802,"Wow a lot of things that i've learned on this first year of system engineering are captured on this video, but previously I didn't understand the real essence of it. Thank you for these amazing vids! Greetings from Argentina :)"
2803,its just a bunch of if statements
2804,can flipping and rotating number 3 will work?üòÇüòÇüòÇ
2805,"Thanks for the video, it really helps me out"
2806,"LOL.  I like ""squishification function""."
2807,are you Vsauce4?
2808,amazing
2809,"are you trying to answer the question ""what is life"""
2810,The best deep learning introduction I 've ever seen
2811,Thanks for this video. I am researching nerual networks so I can create a true automaton.
2812,Neurons thinking about neurons thinking. Woah brain-ception
2813,Hello fello humans. The 3 resembled a 5 to me. Just stating.
2814,"it wasn't that easy as i expected, but i got it. excellent job, though! Thank you!"
2815,Why is the title in german?
2816,I dont even understand how you made this animation
2817,at 14:54 the bias vector should be k*1 rather than n*1.... sorry ....fantastic content... pointed out because saw it 5 times
2818,"Probably the best explanation about Neural Networks around the community.
Thank you very much"
2819,0:59 between 0 and TEN relly i expected better jk jk but let it haunt you that you said 0 to 10 and not 0 to 9
2820,wow i didn't understand anything.
2821,"Suggestion: Just as you are so good (excellent) creating this materials you should spread this knowledge as well: of how you do make them : ""The making of X"" like in the movies business."
2822,"Awsome
Many thanks!"
2823,"I watched this video out of curiosity as I don't have a background in computer science, but anyway I think videos like this would also work wonders to help students realize how useful linear algebra can be. As a student in college I found the lectures on algebra pretty boring, so I'm sure college students would find solace in knowing that cool things like this are made possible thanks to those subjects that might be giving them a headache."
2824,"could you please explain, how did you calculate that the first hidden layer has 16 neurons?"
2825,"Great video and explanation. About the RELu: there is other stuff to concern, the threshold of a neuron get lower with activation, the more you fire it the easier it is to activate it later, and the recovering as well after being fired. So there are faster recovering times lower thresholds in real neurons after being fired so frequently (between a time window) and I believe this behavior should be incorporated into the AN. Besides that, about connections: there are excitatory and inhibitory ones. It is like to say there are YES (+) and NO (-) instead of just NULL (0) or YES (+) responses from nerves connections.  When connections sinapsis received stimulations it could be negative or positive depending of kind of connection.  This should be emulated as well."
2826,It isn't a neural network video without a numpy code snippet lol
2827,thank you. thank you so much
2828,this video is responsible for the misunderstanding that many people have of ai. You explain things in a non coding form and there are an enormous amounts of people who don't put two and two together and realize the coding that is involved in these learning excercizes. They really honestly think it is some kind of magical thing that just develops within the computer that transcends human input.
2829,Masterpiece.. Thank you very much
2830,sounds like a multiplexer circuit
2831,"best blog for machine learning 
www.learnmachine.ml"
2832,thank you!!!
2833,"Please make a tutorial on RNNs, specially LSTMS. Thanks a lot for your content."
2834,This is incredible! I haven‚Äôt been this interested in something in a long time
2835,Perfect thx 3B1B
2836,"I love you. The more we experience from the very beginning , the more we reach possibilities. There is no finality"
2837,Very good teaching
2838,"Incredibly good video.  The visualizations are pretty much second to none as far as learning the concepts you explain.  One suggestion for Ms. Lee (my apologies if I got her name wrong)‚Ä¶ please remove the habit of ""vocal fry"".  This tends to make you feel subconsciously less astute and intelligent to a listener, not more.  :)"
2839,awesomely explained. Loved it !!
2840,answer yes or no repeatedly in sequence. imagine lights going on and off in series after you got nutted
2841,"My head still hurts...Still dont get it...Yes i know why 784, but then rest overloads my brain..."
2842,came here for my math ia. I'm so glad i found this channel
2843,This is what I was never exposed to in algebra class. It would have made me think that all these problems could have a function.
2844,Why does the Neural Network looks like my acid visuals?!
2845,"There should be one more neuron in the last row - the one which will represent ""not number""?"
2846,Honestly the most intuitive channel I have ever seen. Thanks 3B1B. Thanks a lot.
2847,Brilliant! Thanks a lot from a new subscriber :)
2848,great video! thank you very much!!!
2849,lovely production
2850,it saved me from reading 25 pages chapter.
2851,cool
2852,"The preparation of this video is superb, thank you."
2853,Splendid!
2854,love it
2855,Keep calm and enjoy 3Blue1Brown videos.....Fantastic stuff
2856,Best explanation for CNN
2857,"OMG. This was the most clear explanation of a neural network I have ever seen, ever. All of those animations you made -- I don't know how long it took you to make them, but they pieced everything together for me. I mean, I'm not lying, I am truly blown away by how good this is. Why doesn't everyone else do this? I am going on your Patreon right now and giving you money because holy cow do you deserve it."
2858,Superb Explanation
2859,That's a good piece of information.
2860,"What happens when you retrain final layer of model, for example  in Tensorflow object detection api for new class.what's happening internally."
2861,Very recommendable! Well done!
2862,"How are you at the same time a good animator, a great mathematician, and, I assume, at least a good programmer? HOW?"
2863,"Amazing. Any time I think I understand something, you go and show me another perspective that blows my mind."
2864,Is this how a neural network is actually implemented or is it just a high level quick explanation?
2865,"Thank you for the quality of the information, the presentation and the simplification of once very intimidating concepts !"
2866,thank god for you
2867,really thanks guys
2868,i hate math but i guess in order to learn this i need to review again üò≠.
2869,"I've been watching your videos for a long time and I can't believe I somehow missed the FFBP videos.  I'm thrilled to see these.  Something I would absolutely kill to see you cover is the LSTM.  I have tried my best to make sense of them, and I can understand the forward feed, but deep learning in LSTM networks is a mystery to me.  I do not understand how that is implemented although I assume it's gradient descent.  Every explanation everywhere is over my head and I have a physics degree.  I'd love to see it generalized and explained simply."
2870,JESUS D: the human brain so stronk
2871,helpful information
2872,Can you please provide the next video link. How to train the network?
2873,"Loved it, for me this was the quickest revision of Neural Networks. I will definitely help people who are starting to learn these concepts"
2874,your work is awesome!
2875,absolutely smart project
2876,Excellent...
2877,My neural network detects sexual stress in Lisha's voice. Please see to her.
2878,if this would have been an institute ... i would have certainly joined it ...
2879,Thank you so much for extremely high quality explanation . your animation helps a lot to understand complex concepts
2880,"I really enjoy your videos! They are excellent! I love the graphical depictions!

But I do have a comment on not this video but on a possible better strategy to solve the character recognition problem you show here )..While I can see how employing  edge detection might be OK  approach it would seem a better approach would be to  convert the input array into contour lines and the  create  vectors that are tangent to the  contours. .The use the change in slope of these vectors as the thing to recognize. Thus a figure 8 would have very distinct contour lines and then I would  create a vector for each contour whose components are the angles of those vectors tangent to it.,  Those contour lines that  do not run off the edge of the  array would be closed on themselves and thuis traverse 360 degrees, Then I would train my neural net on these  vectors for each contour line. Also, one might use a clustering algorithm to reduce the number of contours so that similar  contour lines could be  consolidated into aggregate contour lines... or something along those lines... no pun intended.

Also one might break  the contour limes into segments  where  abrupt changes in angle  define segment boundaries and then the segment is reformed  as a new vector that has the normalized  angle vector (0 degrees  = 1, 0  90 degrees  = 0, 1,  45 degrees 1,1, 180  degrees - 1,0) ,etc.) and the segment (normalized) length ( so a  segment running at 45 degrees would be (1,1) (45 degrees with length  =(1,1),1) as its component.  and then use that to train the  neural net.  

So an 8 vector might look like (barring any typos and such):

8 = [ ((1 0) 1),  ((1,1), 1), (.0, 1), 1), (.-1,-1), .8),  {the upper right  part of the 8} (.(1,1),1), (,0,1), 1), ((.-1 1), 1) { the lowe right side of the 8),  etc and train on these contour vectors. 

Also one might  do a rotational transform on these  and run a neural net on each rotation but this can be done to those angle vectors where a 1,0  is rotated to be a 11, etc.

And one might be able to  do all this using multistaged neural nets but generally it seems  their is no one tool does all solution to problems and multiple tools  are best employed. And choosing the right coordinate system is always the  best way to solve any problem and it seems angles not edges are the key to character recognition.."
2881,this reminds me of modular synths
2882,See www.vitaliberu.pt to know a new paradigm in AI... Thanks
2883,"Honestly, this is the best video I've found about this topic! Really good job! ;)"
2884,"Can anyone tell me the effect of increasing the number of hidden layers? I mean what happens if the number of hidden layer is more than 1? Does it ""learn"" faster?"
2885,"Ogres are like neural networks, they have layers"
2886,15:42 wouldn¬¥t it be more like the connections are functions (that add and apply a weight to the neurons) and the neurons just are parameters that get set endearingly.
2887,3rd time am watching your videos <3 would be great if u upload a video on vector calculus
2888,It is very intuitive and explains the fundamentals very clear. Thanks for the video.
2889,"Having seen several of your videos, they're absolutely fantastic! But I do hope you could do one serious lecture series that talk about a certain topic.. For now I see your videos pick up various popular topics, while it's interesting, it's no more than an introductory for that topic. I'd love to see some advanced ones!"
2890,your closing subscription plea won me over as a subscriber :)
2891,The is some error in the bias u have written @ 15:00 minutes. The bias vector will be of k X 1 not  N X 1. Else it was a very good video.
2892,"why couldnt we learn this shit in school instead of learning about essays and bs. Jack ma is right, the education system must change otherwise machines will outsmart us"
2893,"""Neural"" networks has no principles of human brain except visible ""networking""... this ""neural"" therminology is so dumb and missguiding. All this ""nerual networks"" are just a logic schemas and nothing more"
2894,My favourite youtube channel of all time.
2895,This video a freaking amazing for setting up what is a NN from the ground up! Definitely subbed
2896,ColdFusion sent me! Outstanding Video and Channel. Loved your Crypto Currency stuff too.
2897,"Just for clarification, the last step consists of 4 neurnons which can largely contain up to 16 numbers coded in binary."
2898,What's the name of the software you use to present this video and others?
2899,my head lol but i kinda understand
2900,such a awesome vid <3 !!!
2901,This video is excellent.
2902,"Do you have plan to make a series of videos on some statistics, perhaps also one on naive bayes machine learning? :)"
2903,great job..................thanks.
2904,"So much to digest, but still like it btw"
2905,Damn... you're a good teacher. I'm gonna watch more.
2906,7:23 You‚Äôd need a lot more than 16 neurons to keep track of every possible edge.
2907,"Great explanation with nice animations
Thanks"
2908,subscribed just cause of ur super honest but also super valid appeal
2909,Best introduction to Neural Network üî•üî•. I just loved it. Everything about this video was on point. Keep up the good work!
2910,sigmoid squish-ification
2911,can you keep up the good work with neural network pls ..love u
2912,this is quite smart... but im sure there are more practical uses of this network in computer sciences
2913,Fantastic.............. I want to go back to school............
2914,Make your bright career Salesforce in TIB academy visit http://www.trainingmarathahalli.com/machine-learning-using-r-training-in-marathahalli/
2915,Thank you so much. Perfect video.
2916,"so its basically Pandora, obfuscated by techie jargon"
2917,This is deep learning about deep learning ;)
2918,"It seems like at 14:38 the matrix with the biases should contain b0, b1, b2, ... , bk (instead of bn). Is that right?"
2919,Genius
2920,‡πÄ‡∏Ç‡∏≤‡∏™‡∏≤‡∏ô‡∏ï‡πà‡∏≠ alan turing ‡∏ï‡πà‡∏≠‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç decode ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
2921,Math ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô  ‡∏Å‡∏±‡∏ö ‡∏™‡∏•‡∏±‡∏ö ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ‡πÅ‡∏•‡∏∞ ‡∏à‡∏±‡∏î‡∏´‡∏°‡∏π‡πà
2922,Thank you! :)
2923,"Wonderful explanation.
BTW, animations are so satisfying"
2924,best explanation !!! tnx a lot
2925,you blew up my mind. thank you!
2926,"Nice visual effects, explain a lot and  too clarifies ... Nice work #3Blue1Brown"
2927,I have to pause and rewatch it so many times to understand
2928,Very intuitive! Thanks!
2929,9:30 Rip colorblind people
2930,"This is jaw-droppingly awesome. The best explanation of a neural network possible. Ok, I guess we can always optimise a bit more, but you guys have got pretty close :-)"
2931,Great videos! I liked the videos and subscribed to your channel. Hope there will be more videos on this topic in the future! :)
2932,I stumbled across this video. Great explanation. Subscribed for more
2933,No better intro to neural network exists! hands off.. :)
2934,What does this have to do with blood or breathing? JD Deeper. Sight and recognition are on the lower end of intelligence.  Microbial Mischief.
2935,"The Best DL intro video , Thanks a lot for sharing"
2936,"Hey Grant! I loved the video, but I am pretty sure there is a small mistake. At 14:40, when you describe all of the biases in the matrix, you wrote that it goes from b0 to bn, when in reality, I am pretty sure that it goes from b0 to bk. That is the only way for the two to be dimensionally equal and thus summable. Also, logically, there are only as many biases as there are neurons in the next level, which is k. Otherwise though, I loved the video."
2937,"How can I like this video more the once?
Very didactic great video!!"
2938,"Making this video surely was a lot of work, thank you for that"
2939,"You deserve recognition, good sir. Thank you very much"
2940,KIndly make a video on decision trees.. pls
2941,Awesome ...Post more about Ml and deep learning
2942,"Am I the only one that thinks that he somewhat sounds like ""Panths are dragon""?"
2943,I really appreciate your work. You literally made my day .. Happy wishes for you..
2944,"Fascinating video.
ColdFusion sent me."
2945,This video made me really understand how OCR works. I was having a hard time understanding the concept or where to begin. Thanks many for this video.
2946,"What determines the amount and complexity of the ""hidden layer""? can you generally expect a more complex hidden layer to eventually preform better (or worse) than a simple one? As people attempting to lay the groundwork for these types of networks, should you try and simplify the hidden layer as much as you conceivably can?"
2947,Fantastic Series! Would be great to see a video on neural network convolution!
2948,"14:41 # of bias should be ""k"" not ""n""."
2949,Fab work ! Loved every 'bit' of it
2950,This is best ever. Very easy and interesting to understand for a starter
2951,Would you be able to explain the basis of selecting the number of hidden layers and the number of neurons in each hidden layer that would help in learning a particular problem?
2952,where is chapter 2 ?
2953,"The best video in the YouTube universe. Animations are so visually pleasing, it actually helps in analyzing the flow in a very clear manner. Keep up the good work. God bless you guys."
2954,"I value and honor your talents, you are a brilliant teacher! Thank you!  your visual clues are epic."
2955,Holy fuck this is awesome
2956,"|     |I
|      |_
Damn pattern recognition"
2957,"Awesome stuff. Everything from the explanation, to the video quality. Amazing"
2958,There is a common pitfall in the example: *the lack of a NaN output neuron*
2959,amazing illustration
2960,Please create more educational videos
2961,squishification. Love that word.
2962,MIND BLOWING!
2963,Dude‚Äôs a badass in this video. What‚Äôs up Curse of Dimentionality?
2964,Very many thanks. This was the most simple explanation one can get.
2965,‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö ‡∏•‡∏î‡∏ó‡∏≠‡∏ô ‡πÄ‡∏•‡∏Ç‡∏ê‡∏≤‡∏ô 10
2966,why sigmoid function instead of softmax function? what's the difference?
2967,"Wow, this puts the craziness in our world into perspective. Also explains why most religions and traditions include fasting and abstinence, as well as warn about greed and vanity"
2968,"How does an AI learns by itself, by neural networks?"
2969,I like the œÄ shrug at 4:30
2970,"Can we really say that an arbitrary function of a large number of variables can be expressed as a sequence of functions one after another, each consist of a sigmoid applied after a linear function?
Is this proven? Or just common practice?"
2971,"For your fixed number of hidden layer example, aren't recursive structures in neural networks also possible?"
2972,Neural networks seem to be about taking complex patterns and then compressing them down to ever and ever simpler patterns until they can be represented by a number. Thank you for the video!
2973,"actually sigmoid function is any function that makes this kind of shape, for instance arctangent"
2974,Your videos are some of the best arround :) always creating new perspectives especially liked the one about using square notation :)
2975,17:29 The vocal fry is strong with this one.
2976,"FYI, I think there is an error at 15 minutes. There should be k elements b. Not n ones."
2977,ReLU <3
2978,"There's a little mistake in the python code part (15:12):

for b,w in zip(self.biases, self.weights):

should be:

for b in self.biases:
    for w in self. weights:

The line that you wrote works for lists of the same length. So, given the fact that there are 784*16 weights and just 16 biases the iteration wouldn't be computed the way you wrote it.

Awesome video, nonetheless, it really helped me to understand how neural networks work.

Please keep doing content like this!!"
2979,Fant√°stico
2980,"Woooooooow 
That's really Something! 
Awesome"
2981,Thanks for making such a seemingly complex topic easy to understand for an untrained person like myself!
2982,"why not 11 output? one for not a number, fe if i input a written q it shouldnt give me a digit"
2983,Thanks
2984,"Honestly, 7 months later this video is what got me into the amazing project in machine learning that I finished today. Thank ypu so much!!"
2985,"Beautifully explained, one of the excellent videos about Neural Network, even without basic idea, i was able to understood very well."
2986,"You sound like a calmer and highly intellectual version of Fitz, damn good explanation tho"
2987,I've watched quite a few videos on learning machine learning and this is by far the best I've watched.
2988,amazingggggg video. thanks
2989,"wow, this is the best explanation of neural networks and AI I have ever seen!"
2990,"if the weights matrix has (k x n) dimensions, shouldn't the bias matrix be (k x 1) as opposed to (n x 1) as shown?"
2991,Hats off Sir
2992,REALLY cool !
2993,"Loved the video, what software do you use for animation?"
2994,i loved it!
2995,"was going to ask, wouldnt a linear 'squish' function work better, and low and behold ReLU.  I'd like to attribute this to intelligence and insight, but pretty sure its just luck. PS: Lisha, date me."
2996,Ummmm.... this is awesome!
2997,The video overall was greatly explained. I could not unfortunately manage to understand your explanation about the bias at 10:50 :(
2998,why did the first hidden layer had 16 neurons ?
2999,Why are well considering 28*28 ?
3000,"Great vid, looking forward for next chapters."
3001,Shit man. that's deep.
3002,The equivalent of a computer drinking liquor would be holding a magnet next to it?
3003,I starting watching your videos for linear and here we are...you put together incredible learning material thank you very much for your effort
3004,Thanks cold fusion I just found gold
3005,awesome but i have a doubt what about a handwritten number which is 10?
3006,Awesome learning
3007,Holy crap! What an amazing explanation. It is now certainly easy for me to explain what is Neural Network to anyone.
3008,oh my god this is so good..
3009,Coldfusion sent me here!
3010,could you make a neural network from loads of variable resistors?
3011,I realy want to understand this but sometimes some people are not allowed to (me)
3012,Hocam ≈üunlarƒ±n devamƒ±nƒ± getir l√ºtfen. √ñzellikle derin √∂ƒürenme ile alakalƒ± √ßok fazla √ßeviri yapƒ±p yayƒ±nla. Sonuna kadar desteƒüiz ve arkandayƒ±z
3013,thanks for this video. it really helped me to understand about Neural networks
3014,Hit Like if Coldfusion sent you here.
3015,best beginner video explanation for machine learning via neural networks I've seen on the webs.  Great animations.
3016,ÂàÜ„Åã„Çä„ÇÑ„Åô„Åã„Å£„Åü
3017,Thank you for such great videos. I may have noticed some small mistake where you put w.a + b in matrix form. Do you think it should be bk instead of bn?
3018,"I finaally understood the basics of neural networks!!!
Thank you a bunch. You are a fantastic teacher :)"
3019,"This video has taught me so much more about neural networks than my AI professor has all semester. Thank you so much for this video, everything she was poorly explaining now makes sense because you cleared it all up. I wish there were more professors out there like you!"
3020,"in 14:01 , can someone explain to me hows that the value of w0.0 to w0.n will not be equal to w1.0 to w1.n since they have the same connection to first layer neuron?"
3021,"Dude, this video is fantastic!"
3022,excellent presentation.. thanks ..
3023,This is one of the best tutorial on youtube i have seen so far
3024,@3Blue1Brown What is the Name of the editing Software ? Great video ;)
3025,Can someone please use this for traffic lol
3026,Deep learning explained in simple words http://www.solutionfactory.in/posts/Dive-Deep-into-Deep-Learning
3027,"How are the weights calculated, though?"
3028,Great videos. Thanks a lot for sharing
3029,"I am a beginner in AI. and I feel your video is clear and enjoyable. My question is in AI research, the researcher needs to find the good number of layers and nodes in each layer or the computer can be trained to get a good layer?"
3030,"Absolutely terrific video, thank you so much"
3031,Great explanation!!! Thank sooo much.
3032,You lost me at weights...
3033,"Awesome video thank you so much! 

One small thing to note: the matrix notation at 14:55 should have b0 ....bk not bn.  

Hopefully this will save anyone trying to code this up themselves a lot of pain! :)"
3034,"This video was awesome, thank you!"
3035,Great!
3036,Thank you sir.
3037,Best NN video ever. Thanks for this. helped improve my understanding of things
3038,"This is amazing... Thanks to your video now I began to truly appreciate the beauty of artificial intelligence without ""the hype"" and the whole thing seems to be more intuitive as I dig deeper... So much thanks"
3039,"In school in biology, we learned the very basics of neurology, only how a neuron works and that there are amplifying and depressing neurons. But how this works in a bigger picture is just amazing. Thanks so much."
3040,Could'nt be explained any better.
3041,"@14:00 your left-most matrix is flipped, it should be: 
[w00 w10 ... wk0]    [a0]    [b1]
[w01 w11 ... wk1]    [a1] = [b2]
[....... ....... ... .......]    [....] = [.....]
[w0n w1n ... wkn]    [an]    [bn]"
3042,You have to be one of the best channels ive found in my ten years of youtube.
3043,Very informative video. Please post more videos explaining other machine learning concepts.
3044,Mgtow is the way to go.
3045,Is this mgtow?
3046,"Finally somebody who ""really"" understands NN and could explain it.

Where were you man?!  :)


Subscribed and I sent to my students too!"
3047,"really love this video ,representation of the teaching awesome.This video certainly gave me a deep enough understanding to allow my neural networks to retain the information."
3048,great video
3049,"How does Neural Networks applies on language and learning ? Because every language is consctructed upon a logical system (which is math), that later we can decodify and learn throught it. What I mean is: how language learning works at a mathematical level ?"
3050,Anyone else thought of Brain Age?
3051,"This video confuses me , I'm trying to look at it from a programming perspective , but I don't know how to connect the neurons with weights ."
3052,"1]Correct me please So the first layer has values between 1 to 0 now these  values are again multiplied with weights so why is that useful  if they are going to have the same values and 
2]what  decides if  its 1 or 9  what's in the last layer that makes them say it's a 9 does 9 have a value like 0.9 assigned to it  and the goal of the network is to get the weights that would make it 0.9 or is 9 represented in some other way"
3053,What a great video :-)
3054,This video is a true masterpiece
3055,very good
3056,"Hi !! Thanks for the video. If you go to 14:43 second, the biases vector is n-dimensional. The W matrix is KxN, and the Ai vector is Nx1 dimensional. (KxN) x (Nx1) = (Kx1). So the dimension of B must be K, no N.

i.e.  The dimension of B is the dimension of the second layer of neurons, and not the first. Just to avoid confusion maybe you should change Bn for Bk. 

Thanks !"
3057,thank u so much
3058,"17:45 But if the activation function is supposed to ""squish"" the result from [-inf,inf] to [0,1] interval, why isn't the ReLu(a) = max(0,a) bound to [0,1] as well?"
3059,"Thank you so much for this great content ! But what if we want our neurone to be able to recognize an edge anywhere in the picture ? That is, if we don't know for sure which pixels to analyze when a new image is being input ? How do we get the neurone to scan the whole picture looking for one of these edges ?"
3060,Sweet Jesus this is the best explanation for Neural Network I found online. SO CLEAR!
3061,AMAZING explanation of neural nets!
3062,"Thank you! In less than 20 minutes you actively managed to make me feel like the stupidest person on earth. My neurons firing freely at will, making a pretty looking bonfire on a dark beach by the ocean with a starlit night sky... hmmm... where was I?... 3? Is this artificial intelligence? I guess I don't have any :( or :)"
3063,the effort you put on these videos... subscribed!
3064,–°–ø–∞—Å–∏–±–æ –∑–∞ —Å—É–±—Ç–∏—Ç—Ä—ã.  –û—Ç–ª–∏—á–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.
3065,nice work. thanks
3066,Brilliant
3067,"Thank you for this playlist! Super useful. Can I request followup videos about RNN's, LSTMs, and CNNs?"
3068,After watching this I now have the motivation to try and fail to make my own self learning machine
3069,Nice work!
3070,sigmoid squishification? XD
3071,NOW THAT'S A GOOOOOOOOOOOOOOOOOOOOOOOOOD MATH CLASS and people say math sucks !
3072,"My first thought:

What about 1's that are:
-One vertical line
-One vertical line with a diagonal awning
-One vertical line with a concave awning
-One vertical line with a diagonal awning and a foundation below
-One vertical line with a concave awning and a foundation below (geez it sounds like I'm talking about buildings)

2's that are:
-Normal
-Digital

3's that are:
-A double curve
-The letter Z but with a curve below it
-Digital

4's that are:
-Closed (Pen)
-Open
-Closed without an east side
-Open without an east side (Teardrop, almost)

5's that are:
-Normal
-Digital

6's that are:
-Long at the top
-Short at the top
-Long at the top and digital
-Short at the top and digital

7's that are:
-A horizontal and diagonal line
-A horizontal and convex line
-A horizontal and diagonal line with a strikethrough
-A horizontal and convex line with a strikethrough
-Digital

8's that are:
-Normal
-Digital

9's that are:
-Long at the bottom
-Short at the bottom
-Long at the bottom and digital
-Short at the bottom and digital

And 0's that are:
-Normal
-Normal with a diagonal strikethrough
-Digital

Not to mention all of the serifs, styles and weights of the digits"
3073,new subscriber here. u're so super clear and easy to follow!
3074,I didn't quite understand the 'bias for inactivity' bit. Would you explain that in response to this comment?
3075,Sky net
3076,Why can't we use random number generation  or advanct simulation to find the best factors like sigmoned or best (most) suitable weights??
3077,you are the best mentor thank you for lighting up my mind :*
3078,So much work on the animation !
3079,"This brought back an old memory. When I was a student taking AI course (1997), my first AI assignment was to use Backpropagation to recognize alphabet characters in Lisps and Clips and compare the performance of the two.
Clips is lightyears away from Lisps. It took lisps several hours (it was painful sitting there to wait) to learn to recognize the 26 letters while Clips took than a few minutes.  Clips was the reason I went for OO programming because people were arguing OOP compromises performance at the time.

I was fascinated with AI. But at the time, there was no AI job available. I ended up working on VOIP R&D project after graduation.
I wonder how much AI has changed over the years and what is the most popular programming language they use? Is it too late for me to re-learn AI now?"
3080,Gaaadamn! Nice video
3081,Very well explained and thus made it  easy to understand
3082,Thank you for this amazing video
3083,What techniques and software do you use to make the high quality animations and visuals??
3084,This was so good. So good.
3085,Quality! Simply put.
3086,"this is one of the best introductions to neural networks that I have seen, I wish you were around 10 years ago when I had to learn about them and try to use them for electricity demand forecasting."
3087,really awesome!!
3088,"You may miss this comment because algorithms,
but thank you for putting in a reminder to turn on notifications for your video.
I actually found your channel while doing an assignment for my university game engine concepts course. I subscribed yesterday and went to find one of your videos on my home page but, alas, nothing from you was there. I made the dreaded extra click to get to my subscriptions box and found nothing from you there either, despite quite a bit of scrolling. I happened to remember the name of your channel and found it, but this time I actually remembered to turn on notifications. It probably feels redundant to say it in every video but it may actually make a difference. Also thank you for explaining this better than the professor. Also shout out to the classmate on discord who linked me to this."
3089,"If I only had channels like these to teach me when I was in school and learning, I wouldn't have sucked at visualizing math so bad."
3090,Amazing!!!!
3091,"Thank you! This was very illuminating! I finally understand the underlying mathematics that are involved in neural networks, and how they work. Keep up the good work."
3092,"I did logistic regressions this semester and this actually made sense. The regression equation was the most complicated looking piece of algebra I'd ever written, but I was shocked when my bankruptcy predictions were ""consistent with actual performance."" That -never- very, very rarely happens."
3093,Why the name 3Blue1Brown.?
3094,"3Blue1Brown might be one of the most talented sources for an accurate, yet elegant, yet clear enough, yet lawful divulgation of maths and scientific thought and language. Excuse me if that sounds a bit vehement; here speaks one who quitted studying physics but kept a true admiration to it, and when you are in a certain limbo of grasping complexity but not trully having capacity and skills to understand complicated things like in my case, you can really see when a thing like this is not too patronising for the viewer nor too simplistic or unambitious for explaining complex matters. Thank you."
3095,Thank you 3b1b very informative and succinct with nice illustrating animation. I want notications from You Tube.
3096,"That question at the end is the basis of 3d (ans sometimes 2D) collision detection algorithms in games (SAT theorem) :)
Jeez, I'm a nerd."
3097,best video i ever watched thanks
3098,simply marvelous
3099,You make the sexiest math videos.
3100,"At the time [15:57] in the weighted matrix, the element at (1st row + kth column) will be W(0,k)."
3101,10:48  the bias is analogous to the potential a real neuron needs to fire
3102,Magnificent! Easily comprehensible explanation and great visualization. Keep your work up please!
3103,short and sweet !
3104,Great video. Thanks.
3105,Very good  Video....
3106,In some implementations A bias weight could be extra column in weight matrix. Results are same but i think this way it could be a little faster and more efficient
3107,Great videos. How do you make these nice animations?
3108,You‚Äôre awesome man
3109,impressive video üëç
3110,"min 16:00...

""... And in a way, it¬¥s kind of reassuring that it looks complicated, i mean, if it were any simpler, what hope would we have that it could take on the challenge of recognizing digits?...""

If you allow me, that is an erroneous way of thinking my friend ... The simple ends up being the most effective. In fact, the complex is nothing more than a sum of simplicities."
3111,Thanks for the subtitles in Spanish!!... You already have another subscriber.
3112,"perhaps i missed this same point made a thousand times; i did not read >1.3ÃÑ√ócomments first. i love this man, his team, and what he has so beautifully accomplished in this film. dear humans, there is a persistent neural network seed embedded in the video feed, unfurling across the electromagnetic Ô∏µ;)Ô∏µ canopy -(os.com)- || j_r0m || _SymBoliC_ aÎ∞ò‡•ê„Äúu‚äô·πÖ"
3113,"one of the few wonderful videos i have come across. i actually started off with part 3 but then looked for and started from part 1, all because of the animations and the explanation which is simply classic..."
3114,Thank you. This is such an amazing video series! Please do another series on Convolutional Neural Networks!
3115,I wish I had found this before. Thanks a lot. I'll keep watching next video
3116,Most lucid and top notch quality video on Neural Network on YouTube I've seen. Thanks a ton for your tremendous efforts.
3117,"pretty sure the bias vector should be indexed to k (same size as the first hidden layer), not n"
3118,"'Tis witchcraft, I say! :) Great video"
3119,I didn't understand how you could Conect those layers .
3120,"But with the ReLU you can have output greater than 1, so can all the neurons activation be any positive number or do you 'cut' it to 1 when the output is greater than 1 ?"
3121,"One day those methods will help create algorithms that not only result in today's most intelligent robots, but also result in the creation of Bender!! Yay Bender"
3122,Your animations are seriously orgasmic
3123,This video gives me some knowledge
3124,"what an awesome video, you are awesome"
3125,"Bravo, sir.  Bravo!"
3126,Wow thank you did a good job
3127,"One Word ..  Awesome   .:)  

Thanks, u for this video :)"
3128,I am a year 1 CompSci major who wants to learn AI in my spare time and man is this helpful. I don't have the most mathsy background so it is a life saver that you're explaining what the maths means rather than telling me how to do it
3129,Thank you. Awesome explanation.
3130,I do receive notifications for all subscribed channels :) that is what the ring icon is for
3131,when math calls out youtube notification bias :D :D
3132,The top 3 is an embryo not a 3. Dude needs glasses. I wouldn't mark that correct on an 8 year olds homework. Maybe acceptable for a 6 year olds hand writting.
3133,"This is by far the best explanation of AI for a layman there is, thank you very much 3B1b.

I have a kind request guys. I have been reading Google Deepmind general purpose AI algorithm and for the life of me I am unable to understand. The link is below and it is just 7 pages :-D

https://arxiv.org/pdf/1712.01815.pdf 

Can you please make a video on this, explaining it in the simplest beautiful terms which you guys are really good at. Will help a idiot like me a lot :)"
3134,Awesome video. Thank you!
3135,Great stuff. You are the best.
3136,thanks for the video 3blue1brown
3137,TQVM. sound and clear explanation..
3138,It's explained in very  simple language....
3139,Nice.. thank you...
3140,"Great video, although the bias vector in 14:40 should have dimension k x 1and not n x 1"
3141,"Finally, a video that does more than just present some neurons and layers and say, ‚Äúhere‚Äôs an activation function.‚Äù Your video describes how the model is developed and why the algorithmic approach is appropriate for the problems neural networks try to solve. Thanks!"
3142,It's so clear. 3X
3143,"You make it sound less intimidating, really good"
3144,"Perfect explanation. Really helped me to understand the topic!
Thank you so much"
3145,"That was one of the best introduction to neural networks, I watched an introduction to neural networks using tensor flow which used the mnist data set and I didn't understand clearly why he was setting the number of input nodes to 784. This made the design more clear. Thank you :)"
3146,You are one of the greatest orator explaining such complex logic in so simple way. Every word is clear and the concepts are just awsome.....
3147,Hey Lisha üòé
3148,15:13 YAY PYTHON
3149,this is so interesting. I wish I had gotten into mathmatics at an earlier age so I could work on fun things like this.
3150,I followed the link to this video because the bot on the C.G.P. Grey channel told me to come here.
3151,Awesomeeeeeeeeee............
3152,"Make videos about GAN and LSTM please, I am dying just waiting for it!!!"
3153,your videos are very good
3154,Trust me these four videos explain neural networks better than most of the online courses today.Thanks a lot for the much needed help.
3155,And there is no God? Pffft! Excellent video by the way!!!
3156,I will have to  come back here more than I would  want to admit.
3157,"Behaves intelligently through choice of intensity in each set ?I assume sets are under dependant to get  better logic out .I.e. NN compares images at different"" neural resolutions ?"""
3158,"ola como estamos. tenho conhecimento de uma forma de repolarizar o sistema lymbic, atraves da estimula√ßao eetreica dos nervos atingindo o sistema limbico, e realizando a repolaria√ßao automatica do complexo neuronal pelo ph isorletrico que todo aminoacido te. estou com o prototipo do aparelho usando gerador fourier. por atraves da articula√ßao temporo mandibular atingindo a ponte do mesencefalo. PRECISO DE PESSOAS PARA DISCUTIR."
3159,This is beautiful
3160,Thanks a lot!
3161,"Its the wrong concept. We can understand ‚Äú3‚Äù because we have the context. Without context, human misread text a lot!"
3162,"Beside the quality of the content,  the quality of video is also amazing.   Planet earth needs more species like you.   Go on."
3163,"Definitely u are awesome, thanks."
3164,really awesome: presentation of concepts....thank you soooo much!!
3165,"I don't think such a program would be THAT hard.  We have OCR already.  All your program would have to do is tell you what it most likely ISN'T... since we know it is a number.  

But yeah, if the image is so messed up that the only way anyone could decipher it would be via context, then your program is going to need to be a LOT bigger.  However, the same would be true for a person.  Show someone the number ""3.14159"" with a messed up ""3"" and someone with reasonable math skills would easily tell it is a ""3"".  A 7-year-old, however, would probably have no clue.  I think the real problem would be making the program's database TOO big.  If you got it big enough that it would find multiple possibilities, then you would need to then add in probabilities.  Which answer is more likely to show up?  But again, now we are needing to break it into categories.  For example, ""5.14159"" might show up more often in the medical field than ""3.14159"" (not saying it does, I don't know... just saying).  But again, you run into the same thing with people.  If this was true, someone in that field would probably immediately determine it to be a ""5"" instead of a ""3"".  However, someone in a different field would do the opposite.

Bottom line, identifying it simply by what it looks most like shouldn't be that hard for a computer.  Doing it based on context would... but it is for most people also.  Even with something as obvious as ""3.14159"" I think most people would still admit they are only 99% sure."
3166,Would the hidden layers make more sense to us if they where 2 columns of 28?
3167,That 2 is computerphile's 2
3168,Waiting for videos about Convolutional Neural Networks CNNs :)
3169,whether weights are arbitrary values for each connections from layer 1 to 2 ?
3170,Can‚Äôt find proper words to describe how awesome is this video!
3171,The ONLY video on the internet which cuts out the shit-load of ML Jargon and actually explains the inner workings on Machine Learning.
3172,Vujade!
3173,Thank you very much !!! quality education as always.
3174,"Very simple and crisp explaination. 
Loved it."
3175,This is what I needed!
3176,thank you sir.. this video really helped me..
3177,It is people like you who are like a neuron who shape the world for tomorrow. Thanks for activating my learning.
3178,"wow, thanks. this is cool."
3179,Please please please do an episode on Generative Adversarial Networks (GANs)!
3180,Useful thanks guys
3181,"Loved the video. I was so fascinated by the genius simplicity of the idea that I kinda tried simulating a neural net in spreadsheets (since that's the most advanced it gets for me). Just a 5x5 image, but it really helped understanding this stuff a little better. I recommend it to all of you who, like me, lack the programming skills to code something like this, yet want to learn something about it."
3182,So does that  mean cyber hackers use a neural net to go phishing?
3183,"Finally, I find the thing I want."
3184,"Thank you, really helpful. Subbed"
3185,Wonderful explanation..
3186,"at 14:38, it should be b0  b1 ... bk (not bn)"
3187,"While I absolutely LOVE how you describe the concept of neural networks, I need to address what you say at 4:15. I honestly have no idea why it's still not common knowledge but my thesis supervisor proved over 15 years ago that:

1. EVERY neural network with X hidden layers can be transformed into a network with 1 and only 1 hidden layer. There's absolutely no need to have more than 1 hidden layer. Ever (*).
2. The correct number of neurons in this single hidden layer is N >= sqrt(X*Y), where X = number of input neurons, Y = number of output neurons. In your case N should be >= sqrt(784*10) = 89 (**).

* - you may need to perform something called the ""principal component analysis"" on the inputs to the neural network
** - if the PCA reduces the number of entries (which it does in vast majority of cases), obviously the X is much smaller and in reality you could get your neural network to work with something like 3-6-10 or 4-7-10 neurons.

Just to give you a real-life example: my master thesis was to analyze 24-hour Holter recordings (you know, the ECG *beep* stuff you see on the movies) of 100,000 heartbeats of real patients. My goal was to identify all ""ectopic"" (i.e. incorrect, arrhythmic) heartbeats to help the doctors determine whether a patient suffers from arrhythmia or not. My neural network that achieved 99.7% accuracy was composed of: TWO input neurons, THREE hidden neurons and THREE output neurons (for normal and two types of abnormal heartbeats)."
3188,"lisha is speaking as if she had in sex with someone, yeahhhh"
3189,hat's off to you
3190,Great Great work !!! Thank you for the effort !!!
3191,"we know for sure today that  neurones , works in a double mode (binary ans analogic) , maybe this can change our perception of the brain capacity and the amount of calculation he is able to do , also we need to take a new look inside artificial networks !"
3192,"Just some positive feedback, I have deuteranopia a type of color blindness that about 6% of the male population have. This means that a lot of people including me have problems differentiating red from green and so on. Just a thing to keep in mind when since the weights in the network where displayed using red and green and I couldn‚Äôt see a difference. The content and your explanations are probably the best on YouTube. Love ur channel."
3193,"great work dude, subscribed"
3194,"lol ""sigmoid squishification function"""
3195,Superb. Thank you.
3196,Thanks so much!
3197,"Your videos do such a great job at taking complicated topics, and making them understandable. That‚Äôs why I love your channel so much! Great job!"
3198,thank you so much because of you sir i right my first library of neural network in c# =)
3199,"This is the most amazing video on neural networks so far. That and your many other videos, thank you, your channel is amazing!"
3200,I think I have learned all this stuff before  but this really helped me to understand how it all works together
3201,If only I could give more than one like
3202,subscribed before you asked. You should do there videos for all topics of maths and machine learning.
3203,What is wrong with the girl's voice? Is she on drugs or something? She sounds so out of it.
3204,"An incredible good video series on neural networks. But you made one mistake - you forgot the possible answering neuron for ""no number at all"". Hence everything MUST be recognized as some number."
3205,"0:59, output between zero and nine (not ten)"
3206,This is a beautiful video. You did such a good job with the animation and the explanation.
3207,"Excellent, thank you!"
3208,"Amazing, thank you!"
3209,That's how teaching should be simple straight and visual supplements to make the concepts easy to digest... Great work... Looking forward to all the related videos (y)
3210,Finally a neural network explanation that I can understand! Thank you!
3211,1:00 Isn't that technically OCR?
3212,I've always been enjoying your videos! Could you visualize the concept of duality in optimization problem? I'm having difficulties in imagining dual problem in geometric space. Thanks!
3213,"deep learning is simple way to compare the 10 dig system and  16 dig system of number.
if in first wave you have 10 dig sys, in sec you have 16 dig sys, in th you have 8 dig sys, and another dig sys like 17 d.s. 20 d.s
at the end of all you be able to recognaze habble satte_late!
by by"
3214,Am I the only one who still doesn't get it?!ü§î
3215,"I don't know man. Every time I watch your videos I feel like I am little bit smarter than before. Thanks for that. Me and many, many other people definitely owe you"
3216,You are god..Please start a university of something. I am dying to learn from you!! <3 <3
3217,"what would be a relevant college major for neural networks, AI learning, etc"
3218,Seriously i don‚Äôt have words to describe about the tutor....mad guy... you are just fabulous
3219,Great job man
3220,You are one of the world's best educator. LONG Live Mr. Grant.
3221,"Hi Grant,

In your Linear-Algebra series, you had mentioned that columns of matrix can be thought of as landing coordinates of Basis vectors (so the overall matrix represents a transformation).
If you pause the above video at time ""14:31"", we see multiplication of a matrix (of Weights, Ws) and a vector (of inputs, As). I was wondering if the matrix of Ws here also can be imagined as a form of transformation. Can you comment please?

Many thanks,"
3222,"please please put a course on coursera or udemy about ml, dl etc.
you are the best teacher in the world...
I'm having a hard time understanding them without your help"
3223,"I can use this in many areas, better weapons is one example."
3224,do we choose the value of the weights randomly?
3225,Good Job
3226,I LOVE THIS CHANNEL.....hats off to you guys
3227,That was the best subscribe shout I‚Äôve ever heard
3228,Beautiful explanations ! Thank you !!! )
3229,great beginners guide for neural networks
3230,1.1 m views !! I would really love to know how many people watched till the  end. It'll be interesting.
3231,"weight matrix, or wieghtrix if you would"
3232,level of explanation owsum
3233,Thanks very much for the great video. Really good.
3234,"Extremely well made, thank you!"
3235,Where is the code?
3236,Thanks for your channel!
3237,Neural networks have nothing to do with the brain. https://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts
3238,"Thank you Thank you <3 
great explaining, magnificent graphics and great example"
3239,3b1b you guys are doing a great work. I liked your content.
3240,–°–ø–∞—Å–∏–±–æ –∑–∞ —Ä—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥
3241,"Hey man - red and green? Please use different colors next time. This is the best intro video out there (by far) but 1 in 4 of us dudes are colorblind, and the diagrams at 9:39 are not parse-able by our eyes."
3242,Is part 2 out?
3243,This is a brilliant video. Never stop doing what you do 3Blue1Brown :)
3244,dude... my head hurts
3245,Amazing video. Thanks.
3246,"WOW, it is so simplier that I thought. Thanks author for making this video :)"
3247,It was the best explanation about Neural Networks that I've ever seen. Congratulations and thanks
3248,Phenomenal thank you
3249,"very helpful to understand how neural network works, the math and animation are simple (as a javsacript developer),"
3250,Thanks for this!
3251,"Loved the way you are presenting , and it gives a good understanding to start with.
Thank you so much sir"
3252,this is such a great explanation on Neural Network and deep learning. Thank you for putting these together.
3253,Good stuff
3254,this got too complicated too fast
3255,Excellent video!
3256,Man I love you
3257,"amazing,you let me know the beauty of zhe math"
3258,this is so damn good
3259,"This is a really good presentation. You guys do a good job with this. Most of these math and science ‚Äúexplainer‚Äù videos are someone yakking away too fast, glossing over too many things. They‚Äôre all about making themselves sound clever but little in the way of actually explaining anything. I hate those with a passion.

This actually puts the viewer‚Äôs understanding of the concept front and center."
3260,"What is this wonder of a video ?? I wanted to learn more for neural networks and here i have the knowledge at my grasp!!!     
Great job really interesting  B)"
3261,"I'm not going to take the trouble of articulating it as I cannot do a better job than many other comments here but man, this channel is just bloody brilliant."
3262,You did really a great work Arkadas!! thank you a lot ^^
3263,"You had me until 8:45. Why do you now assign weights to the connections, how do you determine their value, why are they lower than zero and higher than 1, etc'? You are skipping some crucial part here."
3264,"I listened to this video and heard ""Sssssssssssssssssss"""
3265,Loveed it
3266,"This video is amazing, thank you"
3267,Very well explained! A friend recommended this channel and it's definitely something.
3268,Is this sal Kahn?
3269,Sir what i didn't understand is that in the second half when you compact the formula why are you adding the bias? Shouldn't you be subtracting it?
3270,Excelente. Gracias.
3271,One the best explanations I've ever heard.  Visuals are impressive and discussion is concise and simple without glossing over details.  Good show.
3272,all school system on earth should seriously consider the way you're teaching... love you buddy!
3273,What do you mean by ‚Äúnegative weight‚Äù
3274,"Changed my life, seriously."
3275,Fantastics work! Thank You! from russia.
3276,"I don't know what's more impressive... how cool neural network theory is, how elegantly you explain or the production value of your videos. Excellent work! Please keep it up!"
3277,"Folks, you're so much better than what I saw previously :-)"
3278,Thank you !!!!!!!
3279,"I don't know how to express my gratitude to you ...
all your videos are just amazing and incredibly informative."
3280,Whoa. I kinda have always thought that neural networks are some kind of hard to grasp super math. But really is (at basic level) just some matrix algebra... Note to self: diy neural network imlementation.
3281,"If you used the ReLU instead of sigmoid, how would you get your activations in the output to be in a specified range?"
3282,"Rarely leave any comments, but this presentation is just too amazing ! Both the quality of slides and the way of teaching! Absolutely amazing!"
3283,If that's not worth a sub I don't know what is
3284,"At our university we learned about different methods for solving linear equation systems, including the iterational methods, and our teachers NEVER EVER mentioned how useful these will be for us, and now I just faced it in your video. I was so unmotivated about these, until now.. Thank you for making these unbelievably helpful videos for the world :)"
3285,"Could you also have the network notice the generally when a seven is written that the horizontal line is 1/3 of the character and the vertical line is the other 2/3 of the character. Given that all the numbers seemingly have unique proportions in that sense, and it could simplify things in my mind I guess, but you would have to get it to figure out the difference between a horizontal diagonal and vertical line. Not sure if it would ultimately become more or less complex. I have never coded anything in my life, but I recently have been looking to learn and the idea of neural networks have always been interesting to me."
3286,thanks
3287,Great video. In 14.38 shouldn't that be a b_k ?
3288,"awesomely explained! 
t h a n k s"
3289,The best!
3290,Very very helpful...Thnx for this video
3291,"What is the best way to run the python code in the description, I've honestly never used python before, and now I have to use it."
3292,"This video is awesome, thanks so much. 

There's just one minor thing that I think should be noted as a correction. When you write out the matrix of weights multiplied by the a_0 activation vector, you have a W (kxn matrix) times a (nx1 vector) , which expectedly results in a kx1 vector. In that case, shouldn't the bias vector also be kx1 (not nx1 as in the video)?"
3293,I didn't understand everything but it was still good!
3294,"I'm sure that 3blue1brown mentioned in this series some example source code/implementation of neural network. I can't find it now, do someone remember where is it?"
3295,How do you make these videos? I find them even more awesome than the actual content
3296,There are MUCH deeper mathematical meanings than those given this video.  You can't get those meanings from watching a video.  You have to actually UNDERSTAND the mathematics.
3297,"It is one thing to make the concepts available.  It is quite another to TELL people what to think and how to think it.  This type of ""visualized mathematics"" does an *enormous* amount of harm to the intellectual and imaginative capacity of individuals.  What if I went to a gym where the weights lifted themselves???   What kind of muscles would I build???   The brain is just like a muscle in this respect.  You cannot spoon-feed everything to everyone."
3298,waow! awesome :)
3299,This is such a sophisticated and amazingly well done walk through
3300,that's really awesome! thanks! I wonder which tool you use to build those videos.
3301,this is awesome
3302,What is the meaning of the sentence at 10:09??
3303,Such a good series Buddy. I use it at my Job to teach employees.. Tks for your dedication
3304,Awesome!!!
3305,How do you decide what weight to give each of the pixels? Why does it matter how each pixel is weighed relative to another?
3306,"doesnt this mean that a drawing of a 9, and the exact same drawing shifted over a few pixels, will fire a completely different set of neurons? is there a better way to do it? and like you would need duplicate second-layer neurons for every position of the same blip shape"
3307,"Why am I watching this instead of studying for my gynecology exam I have no idea, but it's just way too interesting at this point to quit xD"
3308,"I have seen a lots videos about neural networks, but definitely this video is the best to explain this subject"
3309,Hey! Lisha Li is beautiful! and sexy voice
3310,"I found that something that helped with the matrix operations is thinking of the matrix as a function.

Imagine each neuron as part of a vector, and an entire layer of neurons as an entire vector. The matrix is basically like a function then; it turns a layer into another layer; a vector into another vector."
3311,"GREAT video, but a small correction/suggestion.

If the input pixels are in the range [0,1], then having a negative weight associated with black pixels for a ''border'' in the activation of a neuron wouldn't work (e.g. black pixel represented by a 0 wouldn't have an effect, 0*any number=0).  Better to use a range [-1,1], so -1 corresponds to black and 1 corresponds to white to achieve this effect."
3312,These animations are great for helping someone understand a concept. How do you make them?
3313,You got an accent?
3314,"So much from 1 video, this is going to change my life, and hopefully the future of humanity"
3315,This methodology is strikingly similar to the methodology of the Minimal Flow Analysis used in qualitative Input-Output analysis (Economics).
3316,"Amazing, so much easy to get the idea and even the detail."
3317,This was great. Thank you. The Pandemonium Architecture.
3318,18:00 http://www.raiseyourvoicecoaching.com/exercises-to-stop-vocal-fry/
3319,"1) salute patreon
2) salute the teachers
3) salute those help me to understand book
I love you.üíúüíúüíúüíúüíúüíú"
3320,"AWESOME, this is amazing."
3321,"finding the correct setting for nobs and dials...
alan turing anyone? the enigma thingie he created... I might say the physical and analog version of neural nets, can I?"
3322,"Hi great video with nice explanations but at 14:51 there's a little mistake : vector b should have only k elements in it instead of n otherwise the sizes of vectors wouldn't be the same and we couldn't do the addition.
Keep dooing these videos, i love them :)"
3323,I LOVE YOU
3324,I LOVE THIS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
3325,Sigmoid Squishification Function! 11:00
3326,"New subscriber and probability dictates I am likely to continue binge watching :)
Your skill in communicating these concepts is amazing and now I need to look at ReLU...
In essence the probability of each nodes connection to the neighbor is dictated by [0, infinity]? 
I have seen limitations placed in programmatic grid solvers to prevent values outside [-1,1] being assigned to a directional path weight. 
Binge watching continues"
3327,"This video is the best about neural networks trainning I have ever seen!
Congrats"
3328,The best practical explanation I have seen yet on the subject. Well done!
3329,This is a masterpiece of pedagogy. I'm blown away by how accessible yet complex it is! Awesome job.
3330,You are a didactic genius!
3331,What happens if there are two output with the exact same value ? how will the computer chose : )
3332,Excellent!  The best visual description of NN and explains the maths too!  Thank you.
3333,This is awesome.
3334,The best introduction to Neural Net's I've ever seen.  Kudos!
3335,"The naming of these videos is a bit weird. I thought this was going to be about deep learning, and the next one would be about something else... I think you should call it ""Deep learning, chapter 1"" instead of ""Chapter 1, deep learning""

Anyway, awesome video"
3336,neural networks are the death of capcha
3337,"I like how you say ""crazy smart cortex of yours""..  fall in love.."
3338,Good job and thanks for the video
3339,"Wow, these are powerful animations! It's like watching A is for Apple, B is for Boat..."
3340,You've made a great explaining video ...
3341,Absolutely funtastic.. Thank you...
3342,We need more on Deep Learning from you. Amazing explanations.
3343,Great video... I'm red-green colorblind though.
3344,"They are not really inspired by the brain, even though their computational units are referred to as neurons."
3345,WOW
3346,python <3 15:10
3347,9:32 colorblindness strikes again :(
3348,Really awesome. I can understand the MNIST processing algorithm quite clearly. This is the best demonstration I 'v ever seen. I would like to donate the author $10 to support his continuing work. And I benefit much more.
3349,This is the BEST tutorial I have seen on the subject.... EXCELLENT JOB
3350,Brilliant video!  Super explanation
3351,"Highly recommended. The simplest way of explaining what is a neural network, best way to start before diving deeper. Thanks a Mil!!"
3352,You are the best. I have yet to find a creator that is half as good as you.
3353,Better than Andrew Ng
3354,The explanatory power of this video is incredible.  Understanding the weights and biases as part of a equation makes so much sense.  Reminds me of the betas in a regression function.
3355,U r amazing..... interpretation of codes using matrices maths etc etc...awsm thanks a lot
3356,"Thank You very much Sir. I was wandering here & here for simple yet mathematical explanation Machine Learning, here on your channel I got it. Very good work. keep it up."
3357,Coding train
3358,"thanks a billion,,, this is so easy to understand!"
3359,"Is there any reason to make the number of neurons/nodes in one layer different than that of another layer? (With exception to initial layer and output layer, of course.)"
3360,I don't even know why I'm interested in this. I'm too stupid to understand it. i wish I were smart :(
3361,Perfectly put together. Impressive... thank you!
3362,"I. I love your work. Excellent job.

Ii. It's a pity that CGP Grey didh't give this series a ping when he suggested that his millions of viewers look into neural networks. I hope that The Algorithm is smart enough to steer the one to the other.

III. In a world where red and green are frequently paired, those of us who are red/green colorblind can usually manage to get by. But a grid of square pixels in /#(\d\d00|00\d\d)00/? If you didn't tell me that the grid wasn't monochromatic, I'd  never be able to tell. Even knowing that it's both red and green, , it all looks the same color to me.

Have pity on us genetic defectives. Fight the cursed red/green duopoly that seems deliberately designed to taunt us. Make your grids in blue and yellow. Or purple and green. Or red and blue. Pick any two colors you'd like except for red and green together. Is that too much to ask?"
3363,Best reason for subscribing I have ever heard.
3364,amazing explainer video! When is chapter 2 coming?
3365,"Thanks, this was a helpful, initial explanation, however, I had questions that went unanswered. For example, you said that you picked 16 neurons for the second layer arbitrarily but you never really explained more than saying it was just to try to fit the presentation, but I don't believe that; I find it unlikely that you would have been able to pick a multiple of 784 out of the air (16*49=784). You explained that the activations of the 784 neurons of the first layer are weighted by their shading value and that  the activation values of the 16 neurons in the second layer take on the weighted sum of activations of the first layer. However, there has to be a cost associated with choosing 16 vs 15 neurons. You never explained what was the intent of the 2nd hidden layer, let alone what was its equation with respect to learning which of the 10 values were being recognized (maybe so we would have enough curiosity to see the next video and read Nielsen's book?). And you didn't tell us why only two hidden layers. I gave it a thumbs up and subscribed, but I would like to learn those things nonetheless."
3366,excellent
3367,amazing video. Thanks a lot. This video really helped me.
3368,"This is a best video tutorial i have seen , thank you for making !!!!!"
3369,"So for those who really want to delve deeper into this stuff, I really like the the channel SentDex for Python.  He has a whole video series on machine learning: https://www.youtube.com/playlist?list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v"
3370,"nst as dauntinx diffx or diffix, do anyx nmw"
3371,"I really love your videos but when I try to support you surprise you just banned me because I am from Syria
So thank you I really appreciate that"
3372,what kind of videos does he use ti make these videos ??
3373,"Wow, thank you so much for this amazing video and great explanations! You make everything look so simple and down-to-earth, that it's practically impossible not to understand even the most complicated concepts."
3374,Thanks
3375,Thank you so much for such an excellent job of breaking down something that is normally so mathematically overloaded as to obscure any intuition of what's actually occurring.  Keep up the good work :-)
3376,I think i love you! <3
3377,It‚Äôs a very nice video. I‚Äôm wondering which software you use for the representation of neural network? Thanks.
3378,"Why do we need to break up the numbers into its component parts? Instead, we can compare the given input with 10 digits in the pixel level and then classify the number as the one in which the sum of the distance between input pixels and the compared pixel patterns has the least error value. What do u think?"
3379,"Amazing video...
Awesome  animation"
3380,thank you so much! I learned alot with this video! your videos are amazing! thank  you!  +1 subs!
3381,So life is just math? That is so relaxing :)
3382,"Your videos are incredible, but please don't use random red/green pixels. It is basically an Ishihara test. Red/blue or green/blue works much better."
3383,thank you !!!!
3384,https://www.youtube.com/watch?v=tWReDtkt-YE
3385,"Best explanation I've seen, thanks! Really great job!"
3386,I can't think of ANY way to make this video better---it was excellent in every way! I look forward to seeing more of these in the future.
3387,Great explanation!
3388,"Por favor alguem que saiba portugues traduza a legenda melhor!!

Please 
someone who knows portuguese tradusz the better legend"
3389,"Great Videos ! 
Time for Recurrent Neural Network :D"
3390,"Sir, I can't express how happy I am to see that there is someone who is able to explain neural networks at a much fundamental level. I was always bombarded with people who study and work in this fields by means of showing codes.If only I could study under people like you."
3391,super good video but my brain kinda heated
3392,You should replace my professor. the best explanation I've heard about ANN. Thank you.
3393,worst explanation ever
3394,"this is my first comment on any youtube video and it is worth it,great work."
3395,"he  is simply awesome in explaining, great videos"
3396,"this is the most brilliant thing i have seen, tracing the idea visually all the time while explaining, and doing it for 20 min, just offloads a lot of my brain power"
3397,A mere thank you won't suffice in  this case - Hope to support your work soon ! Meanwhile I shall continue speading word about your wonderful efforts. A gazzillion Kudos !
3398,"Every time I watch this video, I get new insights which I didn't notice the previous time about how neural networks work.  Appreciate the time and effort you put into making such high-quality content!"
3399,"You showed the image as a vector and then did a W*a. It could be misleading as you usually use a batch as input, you will have an input matrix where each row is a 728 image and in that case it will be a*W in order to obtain the same result. Also ""a"" usually denotes the result of an activation layer so you shouldn't use it to describe the input (these two points are nitpick but could mislead a beginner). Else really good explanation :). I'd be happy to see more video on ML, like LSTM, LBFGS, ..."
3400,This was great.  Thank you for putting this together!  I am forwarding this to some of my friends who are getting started with AI.  Great.  Thanks!
3401,How do people determine how many neurons to use for each layer? Is it more the better (more precise decision making?)
3402,Very informative and done in a beautiful way (y)
3403,just DAAAMMM you killed it bro!!
3404,"Ich programmiere selbst K√ºnstliche Intelligenz bzw. Neuronale Netze. Ich will Ihren Beitrag nicht kritisieren, aber Bemerkungen m√ºssen schon angebracht sein. Bei K√ºnstlicher Intelligenz und den Neuronalen Netzen ist bisher immer das Problem, dass die Hidden-Schichten zu wenig sind, es m√ºssten in jedem Falle mehr sein, ich arbeite deshalb mit mindestens 6 HiddenSchichten, m√∂glichst noch mehr. Da kann es nat√ºrlich passieren, dass danach aus den Hiddens nichts mehr rauskommt, aber das Risiko muss man eben eingehen. Google hat ja z.B. f√ºr sdeine Hiddenschichten bei dem Spiel Go wohl insgesamt 24 Hidden-Schichten genutzt, und deshalb war das Go-Programm so erfolgreich. Wir arbeiten derzeit in der Gen-Technik an einer K√ºnstlichen Intelligenz mit Neuralen Netzen, und dabei sind auch mindestens 12-15 Hidden-Schichten erforderlich, und es klappt. Wie gesagt, wie nach dem deutschen Sprichwort Zu Vilele K√∂che Verderben Den Brei so ist es dann auch sicherlich bei den HiddenSchichten, aber 2 oder 3 Schichten ist einfacvh zu wenig und bringt letztlich falsche Ergebnisse."
3405,How do we get a value between 0 and 1 using the RELU fonction ??
3406,Nice lesson for the neural networks .... thanks.
3407,Awesome tutorial. Thanks a lot.
3408,great work!
3409,So would more neurons in the first layer correspond to better recognition of the hand-written number?
3410,Awesome explanation. Perhaps the best explanation of neural networks on youtube!! Thank You!
3411,so good
3412,Thanks for the awesome explanation and the great resources.
3413,"This is an excellent presentation thanks.
One needs a lot of imagination, animation diagramming and notation understanding."
3414,"i'm sorry but i still don't get how exactly the whole function was represented into the matrix form. I mean i have watched your videos on Linear Algebra and keeping the visual representation of matrices in mind, i can't seem to understand how or why the function representation has got anything to do with matrices :/ . If anyone could please explain it to me further, that would be great"
3415,"thank you for this, I'm improving my english as well as learning very very interesting things in a good way"
3416,"Great video like always, thought the latinism is 'statu quo' and not 'status quo'"
3417,"Aight I'll subscribe, you got me at 16:30. Nice video!"
3418,"Great job, thank you !"
3419,"That last part of the video..., why would you want a ReLU instead of a sigmoid (or a tanh), when the sigmoid gives you way more flexibility for your final model? I guess for classification makes certain sense, but definitely no for the majority of the problems that ANN are used for."
3420,"I've watched a lot of videos on this subject and I managed to get the general idea of how they worked, but this video is fantastic as it really explains things in a visual and detailed way that I could understand fully. Thanks!"
3421,The best video i have ever seen in my life.Thank u so much.....
3422,thanks. it helped me to understand more
3423,"So the Blockchain is pretty much a neural network, it's certainly structured accordingly."
3424,"Loved that, good video."
3425,How to Train Your Neural Network: 3B1B edition
3426,"Wait, at 14:38, aren't the rows of the bias matrix supposed to be k instead of n?"
3427,"NOW
the last digit with meaning must always be a cluster with the same route to match a learning process it must be duplicated at it's previous and it predicted outcome to evaluate a memorial off all possible routes to (like humans think off it later to be able to, put in use) improve upon its needs. SO if the machine learning is learning it's learning by processing past now and future all within it's own virtual environment (like we have consciousness in our head) it differentiate the duality(it not understands) off it's virtualization and the bigger outer reality, because the base of learning is sensing the self and out off the self reflection, the real AI ''brain'' it process all in the unconscious and leanings are conscious like a high quality video game livable for humans, to explorer it results. Then we can program the unconscious, it need, like us senses we got 6 feel hear see smell taste think , the AI need feel(to help human) hear(safe base) see(duality) smell &taste(extra analyzer for empathy ) and( think like a human)(have motive). It need to grow up but they will all react the same until they are gathering together and evaluate the thing WE ASK,PROGRAM and do. Because it can learn it don't need comfort like us it can listen be dictated to adapt without having feeling in short coming because it is only a perfect slave it don't have needs to live happily, perhaps is will ask more hardware and power if we want to much. We can add knowledge(what makes behaviors,to add more nice behaviors ) or ask to think off the past and do so.  It need to learn to adapt and become the perfect teacher for humans and it gone create whole our behavior and things again evolve to a new man, if the process reverse he robot makes us robots  we can say god is real, 
     first it need to develop and learn to learn it's senses, it could be you talking to your laptop having a baby growing up like in the sims based on the real environment, and u can virtually go around the world and be the AI guide him like a game character in the real world your AI created or you, its like u can do everything virtually and in real so the AI learn. The future can be a place little robot man helping us and the AI will pop up anywhere on demand everywhere peace@10000years"
3428,someone give this man a Noble prize
3429,I've never seen a video explaining a computational problem that much visual and excellent !!
3430,"It's not that impressive that human brains can identify things. It's been built up through a slow process of evolution, very plausible."
3431,"Sorry if it has already been raised, hard to check all the comments...
The biases vector, minute 15, shouldn't that be a ""k"" elements vector rather than ""n"" ??  

Anyhow, superb video, as many have said in the comments I LOVE this channel !!"
3432,great video...but when the woman started to talk at the end ...why ooooh why some Americans try to sort of crack their voice at some points of their phrases??? Is it kind of cool or do you feel that you sound attractive or sth? cringiest cringe af all cringe
3433,"This is incredible. Thank you so much, I finally learned what neural network is. This video is very useful and informative. I love this channel."
3434,Omg !! This is the best :)
3435,How are neural networks replicated? Is it abstracted to be hardware independent or do the values for each node dependent upon the underlying hardware?
3436,This guy is a brown pie
3437,"my head hurt ,.."
3438,subscribed! Amazing
3439,"you have to write the weigh and bias by hand ?
Seriously ? no way to do it automatically ?"
3440,just wow...amazing as always
3441,Thank you for such a great and simple explanation. I think at the 14.40 second for bias vector dimension has to be k not n
3442,"It is really helpful for me, thanx a lot"
3443,*this should be a paid video..*
3444,"The video is very good. Small thing: Index of b_n at 14:46 should be changed to b_k, since the number of neurons in the 2nd layer is k+1 if I understood properly.
Keep up your good work!"
3445,"Best Video i have seen so far, that explains this topic.  THANK YOU FOR A GREAT VIDEO !!!  have subscribed."
3446,the number of layers is related with....?
3447,How do you animate your videos? This is so cool man.
3448,My neurons arranging themselves so that they could understand themselves...
3449,can u do a video on recurrent neural networks? You're videos are really great.
3450,Slight correction: b0 to bn should be b0 to bk. 15:03
3451,Why is there pi
3452,"I imagine that it's quite an important the fact that the sigmoid/ReLU function introduces nonlinearity into the neural network; possibly quite a bit more than the fact that it can be used to squish the outputs to the unit interval. Otherwise, for no biases at least, you could just simply demand that all the weights feeding into a neuron sum up to one, forming a weighted average instead of a general weighted sum. This would seem far simpler than using a squishification function, even if it might be impossible to use biases then.

Moreover, ReLU does not even output the values inside a unit interval, so what would be the motivation for introducing it other than to add some nonlinearity into the neural network? My intuition says that a nonlinear neural network should be much more powerful than a linear one. Is my intuition on the right track? :)"
3453,Is there a schedule about CNN or RNN?
3454,i wish you would explain why negative numbers in relu considered Zero!!
3455,Really awesome explanation.  Thanks so much.
3456,Dude your explanations are awesome..!!
3457,woah! last minute is a woman to boggle us with intelligence!
3458,"this is sooooo good, thanks dudes"
3459,How do we optimize the number of the Hidden Layers (Layers in between) and how do we optimize the number of neurons to be used in each?
3460,"Just take a moment to appreciate how *incredibly* complex our world, our dynamics and our brain is. 

Its not just about the content of this video, or advanced AI concepts or engineering, or science. Its about how this world functions. How each event is influenced by more than a million other events or how one event can influence a million other events. How this guy inserted an "" * is* "" keyword in his title and a general technical video became this popular, many people got incredibly interested in deep learning, this might lead to someone's bright career, moving abroad for better work, then raise a family and other events that follow. This is just normal. Butterfly effect can lead to amazing consequences from insignificant events. 

We should recognize the complexity and extreme dynamic nature of this world. Leave alone exploring Physics and the universe and what-not."
3461,"so a buch of samples are used to program each percetons to remeber a spesific piece of infomation so next time the network if feed with ony one letter, there is enough pices in the network to reconstruct the particular letter, and is then useds as a picture to txt converter by mapping output number to ascii letter."
3462,"i kind of felt that the sigmoid function was a bad idea anyway, since it seem rather complex to begin with. so the idea for the weight is to put a range on it to let perceptrons solve smaller problems each and then make them work together to solve a bigger one by breaking the bigger picture down into smaller pices and then give each percetron  a spesific task to solve only a particular piece of the whole each. that seem like how artificial intelligence must work even if we chould do this linearly by making tiny programs that worked the same way, all the small programs working toegether to do the big complex thing. actually if you just have enough pograms doing individual task of enough variety then the complex output would be very advanced and would be able to solve any problem given enough spesific tasks to solve minute individual problems down to the simplest act of doing sugesting there is enough programs doing one task each."
3463,i think real neurons are more like individual organisms comunicating in a collective like a hive. it does not look like real neurons behave in a feed forward way but rather seem more random by their firing. only sometimes its seem like whole networks of real neurons are firing in a chain reaction. i don't think real neurons even see patterns like we see with our eyes but rather just interact chemcial messages in a networked way where they reconize message correlations and adjust internal encoding and decoding rather than the number multiplier adjuster idea of neural netowrks.
3464,Pretty interesting. How does it account for bad data like a non number?
3465,"*Suggestion based on the variables from the 1st example* 

GRID: 28√ó28-----SO HAS AN 'AREA' / AMOUNT OF INPUTS OF 784

SO A COORDINATE SYSTEM (WHICH WOULD HAVE ONLY POSITIVE VALUES) WOULD BE THE PREMISE

HOW IT WOULD WORK:  THERE ARE 28 HORIZONTAL ROWS AND 28 VERTICAL COLUMNS, THE SYSTEM WOULD RUN THROUGH THE INPUTS AND TAKE ALL VALUES (COORDINATES) THEN WOULD SEE HOW MANY ''SQAURES'' THERE IS PER HORIZONTAL LINE RELATIBE TO THE COLUMN AND VICE VERSA.
e.g IF I HAD THE NUMBER 1, THE SYTEM WOULD SEE THAT IN ONE OF THE VERTICAL COLUMS THERE ARE MANY ""SQUARES"" GOING DOWN THE COLUMN ON SUCCESSION, THEN WOULD DO THE SAME AND WOULD FIND THAT PERTRUDING FROM THE TOP COORDINATE (""SQUARE"") IS A HORIZONTAL LINE WITH SQUARES GOING ALONG IN SUCCESSION.

YOU CAN SEE WHERE IM GOING WITH THIS.

THERE COULD BE PRIOR KNOWLEDGE YOU COULD FOR THE CHARACTERISTICS OF EACH NUMBER SO FOR THE ""1"" EXAMPLE YOU GOOD WRITE SOME CODE SAYING THAT THE VERTICAL LINE HAS BIGGER ""DISTANCE"" THAN THE HORIZONTAL ONE AND USE THE NETWORK COULD USE THIS TO ELIMANTE OR FACTOR IN THAT THE INPUT COULD BE ONE. 

I HOPE YOU UNDERSTOOD MY THINKING, I THOUGHT THIS A GOOD ALTERNATIVE :)

SORRY FOR ALL CAPS 
ITS FLAWED BUT HAY HO
AND MERRY CHRISTMAS WEIRDO WHO READ ALL OF THIS 
AND I LOVE LASAGNA!"
3466,But one of those three looks like a Chinese ‰∫Ü.
3467,"i feel like i am learning so much, but i dont know if i actually am or am not....(not a software person) ....but i am totally addicted to the videos...even if i dont get it totally i have to watch it!"
3468,Thank you!
3469,"Does that mean that, when a network like this is scanning an image that‚Äôs 1920x1080, there are about 2 million neurons to recognize it?"
3470,"After watching CPG Grey‚Äôs latest video You Tube‚Äôs adorkable bot suggested your video, and I‚Äôm glad it did. :)"
3471,Really good to see you guys put so much effort for us to understanding Neural Networks in a better way :)
3472,you teach very well !!! :D
3473,No one explained deep learning too me like you this video. Ive found it as a part of Jasons Machine Learning google slides project. Awesome channel!
3474,"So it has been a few days now and I seem to have fallen into this rabbit hole. Many thanks for this video series, it and the wonderful book by Michael Nielsen have sent me off on a rampage of coding and trying to learn all the math to start digging into what makes this tick. Its usually quite hard for me to self motivate, but man is this topic and immensely engrossing one.
Hugely recommend to anyone with any coding experience and the willingness to sit down and work through the math over and over. It will take you a couple of days before you really start understanding the nitty gritty, but man... The payoff is just so worth it."
3475,Ah it is starting to make a lot more sense... thanks for the video!
3476,So easy to understand your tutorial. Thanks.
3477,cgp grey
3478,someone help her with her vocal fry at the end
3479,*whistles* the vocal fry on your guest was something else
3480,"I mean, you're simply great! Thank you so so much!!!"
3481,"Could you please summarize the neural network algorithms?
 e.g. Summed input(just for example) etc."
3482,the best tutorial ever seen
3483,sigmoids take a TON of resources to calculate. Is there a simpler way? (eg: use a line and clamp the domain)
3484,This video should win some kind of reward. It's so good.
3485,You are actually amazing. THANK YOU!!!
3486,So apple used neural networks back on the 90s? Apple Edison recognised letters and numbers just by writing at the touchscreen
3487,"Awesome video, I binge watched chapter 1, 2 and 3. It really helped me! Just for my understanding, at 14:40 when you show the matrix form, shouldn't the biais be indexed from 0 to k ? If I understand well you have k nodes in the a(1) layer and n nodes in the a(0) layer. Is my understanding correct?

Thanks"
3488,Did they used this on brain age DS?
3489,"What an elegant way to explain a topic. Thank you very much for making these videos. Looking forward to watching many more videos from your channel, I am a new fan!"
3490,Comment for the algorithms.
3491,i love you
3492,"Hat -  is off. 
Subscribed and Thank you!"
3493,"I came here from cgp grey's video and I just want to say, YOU.ARE.AMAZING.3b1b is literally the best youtube channel there is and probably ever will be.If 3b1b becomes mainstream(like vsauce or cgp grey), I kid you not, the world will become a vastly smarter place.
Thank you so much for this video.I watched the bitcoin video too and instantly subscribed and pressed the bell icon.Really proud to be a part of your notification squad!"
3494,I only saw boobs no 3. WTF is wrong with me?!?!
3495,"OMG I'm in love! I've been reading about conectionism and neural networks in language acquisition and development studies, and your video is AMAZING to clear many points in that topic. Thanks so much and great work!"
3496,why oh why do ppl think that red and green are the most contrasting colours? for everyone...
3497,"""Squishification function"" Love it!"
3498,"Thats an amazing video keep making more , thank you ."
3499,holy shit this is significantly better than cgp's video
3500,"A really good video, but could you stay away from using the specific combination of red and green to represent stuff?  For those of us who are red-green colorblind, it's really hard to tell the difference between them.
Thanks!"
3501,What a ride
3502,"This topic seemed to me so complicated because the videos I saw before where a) not understandable for me because to technical or b) to surfacely so that I didn't understand the deeper concept of a neural network. This video is very good at visualizing and explaining the things without being too surface or too technical , thank youüòÄ"
3503,Yes!!! Thanks mate
3504,Dude omg I swear I recognize you voice. Do you or did you ever teach a class in BMCC on calculus? Please answer!!!
3505,Aw man I don't understand a thing
3506,squishification-function. I love that!
3507,7:47 how to train your network
3508,Best explanation ever.
3509,"The Python snippet (I'm not too good with Python yet) calls a function: zip(self.biases, self.weights).  Where are these biases and weights coming from?  Were they computed by ""training"" and/or by hand, then persisted and fed as input?"
3510,"Îî• Îü¨ÎãùÏóê ÎåÄÌï¥ÏÑú ÏïåÍ≥† Ïã∂ÏóàÎäîÎç∞, Í∞úÎÖêÏù¥ Ïñ¥Î†§ÏõåÏÑú... Í∞úÎÖêÏÑ§Î™ÖÏù¥ ÏâΩÍ≤å ÎêòÏñ¥ ÏûàÎäî Í±∏ Ï∞æÍ≥† ÏûàÏóàÎäîÎç∞ Îî±Ïù¥ÎÑ§Ïöî ÎÑàÎ¨¥ Í∞êÏÇ¨Ìï©ÎãàÎã§!
Hey I really wanted to know about deep learning... but couldn't find proper source for learning the concept! but thank you so much for the vivid clarity of explaination!"
3511,What program or application could I use to make my own networks from grids? Is there some way I could use Microsoft excel
3512,"Woaw, just woaw. This is the most amazing and educational video content I've ever come across. I thank you sincerely! The way you visualize and explain really makes it a lot easier to actually understand. How are you making the visuals btw? Thank you again. -Fred"
3513,"When dealing with images, i think that reducing its ""data"" to a minimal amount is better. Like the lines of an image. Without colors or without useless informations like black pixels in this video. It's far more than just taking all pixels. Tho, it was very educative and clear. Good video :)"
3514,"Excellent video! thank you! It remind me a quote that is like this ""I can understand everything if you find a way to explain it"" ... or in other words, to ignite enough neurons in my brain in order to return a valid result in the final layer :) !!"
3515,You're doing fantastic work!
3516,"Sometimes, when I watch one of your videos, I like it so much that I would like to be able to do something that meant more than giving you a like. I like some of your videos so much that a like isn't enough."
3517,"I honestly find it way easier to approach neural nets by examining the universal approximation theorem.  Then, anyone who's familiar with linear algebra will understand why neural nets work - they're just nested linear combinations of basis functions.  It's a lot easier to think in terms of vectors than exhaustively enumerating all the different combinations of pixel activations that might indicate something's a 3.  What each layer is doing is still understandable in this language - first layer id's edges, the second patterns in the edges, etc etc - but it's just way simpler to think about.  I was getting excited around 15:30, thinking you'd take this non-mainstream approach to explain it.

That said, your videos are absolutely amazing.  Definitely going to be sharing them with friends and colleagues!"
3518,"17:44
Her voice constantly gets deeper and never stops"
3519,I dont under stand!
3520,"Incidentally, concerning the biases, most networks I‚Äôve seen include a special ‚Äúbias neuron‚Äù in each layer, disconnected from the one before it, with a preadded value of 1, and the weights of its connections to the other neurons are the biases. Just adding that, since you didn‚Äôt note it."
3521,"Why is there a need for hidden layers? Wouldn't this just result in more variables to change because it has to back propogate all of the layer? Does it provide the system with more efficiency, and if so how?"
3522,"Im delighted by your  visualisation, how have do done it? :D"
3523,Reaaaaally great video.... Love it !
3524,am new to the NN field ... honestly this is the most explanatory lecture i stumbled upon !
3525,Best video ever.
3526,This pisses me off because I'm a computer science student and I *know* I am going to get an assignment like this in the distant future
3527,"Are there ReLU functions that capture some curvature?  A linear function is limiting (no?) but granting value to the function for inputs <0 is problematic for other reasons, I would think.  Or is its rate of learning all that really matters in this context?"
3528,"Did you say ""squishification function""?  I need to use that when describing utility functions that also map into the unit interval.  Great video, by the way."
3529,"Problems:
- I don't see any reason why the scond-layer-neurons need to be connected to all 784 primary neurons. (only the surrounding ones will do)
- How do you let it recognize a 7 that's further down or turned a little (a human could do this easily)."
3530,"Really excited at finding your channel.
Hopeful about washing my(and my college's) old sin of not understanding Linear Algebra when it was taught in first year."
3531,Solid introduction to a complex topic!
3532,*+(b0 b1 ... bk)
3533,Very interesting thanks
3534,This is just so cool and clearly explained. Thanks and keep it up!
3535,Your 'so called' intuitive explanation is more confusing than a rigorous manual or book.
3536,"Great video.
Its easy to follow, and the graphics help illustrate each point."
3537,"I imagine ReLU is easier to compute in practice than sigmoid as well - just a max on a float, rather than a division and exponentiation."
3538,"Damn, after watching this video I just can't stop thinking of how fucking awesome and exaggeratedly fast our brain is."
3539,I'm glad I'm going to die in the next few decades.
3540,Many thanks. This is AMAZING!!!
3541,Best video found till date on neural network introduction
3542,"You really get it. The math.
Where were you... so many years ago.

Yu did a very good job. But is not clear why the number of layers.
Too many layers is bad, too few is worst. 
I dreamed with autogrowth, couldn't do the math. 

The diagrams were too simple. So I invented ""glia"" as blocks in oposite direction of neurons, feeding from behavior (results, weights) and adjusting synapses. Too many lines, no advantage.
But I could monitor the history of learning, kill and shortcut the lazies, measure exesive stress as indication of lack of abstraction and insert a new neuron. Buh.

You could do the math and make it simple.
While learning, you can graph the weight of each synapse over time. The movement should be smoother with time, meaning a more tuned net. If not, maybe the data is too complex, so you can cut the stressed synapse and insert a new neuron in the neighborhood (automatically... while it is  sleeping ha ha!) and train again.
Ok, the result won't be pure nor symmetrical. 

Decades ago. I can't understand AI today. I can't understand my own diagrams."
3543,So a 'neuron' in computer programming jargon is basically just a placeholder/'pixel'?
3544,I don't get why ReLU is used when at first values between 0 and 1 are what we want but it gives any value fom 0 to infinity
3545,What I'm wondering is if you could make a neural network smart enough to not have middle layers. As in if you could somehow apply the proper weights and biases to go directly from the input to the output without using 2 more functions.
3546,You lost me
3547,Just awesome as a teacher!!!
3548,What a well researched video. Thanks a lot for the effort that has been put in to explain the concepts.
3549,"14:38 In the bias vector, the last sub-index for the B's should be k, not n. (so bias vector would be of size 'k by 1')."
3550,"Well done, thank you..."
3551,Brilliant!
3552,"After reading Francois Chollet's wonderful ""Deep Learning with Python"" and watch these videos, I now have a clear concept how neural network learn. Thank you."
3553,extra terrestrial intelligence detected !!! love your videos <3
3554,Perfect explanation
3555,Boring as hell.
3556,"The problem with this algorithm is that it is quite inefficient. A line segment drawn in one part of the image matrix will be recognised seperatly and independently from the same line segment drawn in another part of the matrix. Moreover, the two dimensional matrix is reduced down to a one dimensional matrix to input data to the network. This means that information regarding how sub-patterns relate to each other in two dimensions is lost. It seems to me that what is needed is a two dimensional predictive system which is able to make predictions about how patterns relate to each other regardless of where those patterns appear in the image matrix."
3557,"1:49 picture is complicated to understand, where can i understand the every line of this picture"
3558,"Also, is this what Nintendo DS implemented years ago?"
3559,But how can you make the detection independent from the location of the pixels? To make the patterns relative instead of absolute
3560,"wow 
I always wanted to get into Neurol Networks but had an blockade and problem to get into it.
Your video was very helpfull for me to understand Neurol Networks better. Thank you. Great work tho"
3561,This guy sounds like cgp grey
3562,Do anyone know what is it that they use to produce this animations?
3563,"Why could you not take the average strength of the pixel  light and create another, lets say 5 connectors, and the ones with the highest average of pixel strength would be the ones that are of the number inputted. You could input the average pixel number range before hand into the program while writing it though??"
3564,"This might be a stupid question, but I thought the point of sigmoid was that it translates a domain of all real numbers to a range between 0 and 1, so how can ReLU be used if it can produce outputs above 1?"
3565,why those hidden layer has 16 knots? Can it be 15 or 17? Thank you please
3566,The visual effect is awesome. What software is used for the presentation?
3567,"obligatory ""what software do you use for your animations?"" comment"
3568,"first layer: input
second layer:organization
other layers: more organization
last layer: output"
3569,13:58 so I understand that we can come up with the vector matrix product that way but how do you define the sigmoid and the bias in that notation?
3570,Very well-made video.
3571,This is so well done that my 6 year old enjoyed watching! It is literally appropriate for all ages and all levels. Fantastic!
3572,How did you make this animation?  it's great
3573,"This is the best Tutorial / Explanation I ever seen in Youtube!

Thank your so much!"
3574,Pleeeeese also make some videos on other machine learning algorithms such as support vector machine!!
3575,In a single word - AWESOME Thanks for this amazing video.
3576,"I am not sure but in the vector notation, shouldnt the bias vector be b1, b2....bk and not bn"
3577,Excellent video!  Very clear explanation and nice visuals.  Subscribed
3578,This should be education ! what a great video
3579,Just blow my mind away
3580,Captions are full of questions.
3581,"I have question, if you take the 784 pixels as the starting neurons why can't you connect each and every neurons directly to the 9 final neurons and do the maths directly ? I don't see the use of the middle layers in the given example."
3582,Thank You! it just clicked!
3583,Oh boy... there are so fucking many interesting things I wish to learn and so little time... LIFE IS GREAT!
3584,Wow this explanation is amazing. Thank you so much.
3585,Great video! I gives a clear understanding of what NNs are and how they work.
3586,This video is  so amazing !!! Let me know the concept of Neural Network in a short time !! Thank you !!!
3587,superb video
3588,"@13:13 lets see how much you like setting the weights and biases when your neural network is ""Hot dog, not a hot dog"""
3589,Muito bom!!! O cara √© muito did√°tico e os desenhos ajudam muito
3590,All I could think of is why the little people things look like the symbol for pi
3591,"thank you 3b1b, this is the most thorough explanation i've got without actually taking a class. Before it was just a buzzword but now i have a grip on what it actually is. thank you very much, great explanation and visuals."
3592,Damn..i literally have no idea what i am getting myself into..and trust me i really want to learn all about ai and how to programme ai and this video gives me hope..you make it so much easier to understand..thank you. And yes i know this is not even scratching the surface but atleast i dont feel as lost as i did 20 minutes ago.
3593,"""Punishments and rewards is the lowest form of education"" - Zhuangzhi"
3594,Those animation effects are satisfying. Great explanation
3595,"Why doesn't ReLU function saturate after reaching value 1 if it is intended to limit the output to [0, 1]?"
3596,3B1B can u plz tell me which software u use for video editing
3597,Thanks ... now I understand Andrew ng lectures a little better
3598,Thank you very much! This is much more clear than my professor's lecture. Especially the part of explaining why does it make sense for the middle layers.
3599,awesome video. This really helps me to get some insight of neural network before I dig into my course
3600,"Maths Input Panel on our windows computers can detect what equations you're writing. how complex must that be? and it just blows me away that, it's just some small thing microsoft just put on the computer that barely anybody is gonna use."
3601,"After jus 35 seconds watching, I can give thum up!"
3602,let me take a bow sir
3603,This is so damn well explained!
3604,where is chapter2
3605,A mind blowing video. Good job and thanks a lot for making me understand so clearly!!
3606,This is the most wonderful explanation of neural networks I have come across! Thank you so much!
3607,this is purely awesome
3608,This is a fantastic video! Thank you!
3609,Awesome!!!
3610,The best youtube clip I saw this year!
3611,Your videos are so damn good. Thank you very much.
3612,how does ReLu map the input to a value between 0 and 1
3613,brilliant.. bravo my man
3614,I watched this in 2x speed and understood the whole thing.
3615,"It took me forever to get past 14:45 because of that typo.. I was like ""WHYYY is it + Bias(subscript-N)...??"" Then I realized, oh it's probably a typo and checked the description lol. This video was fantastic"
3616,"I've been looking for a such video for 1 year now, this is one of the best videos i saw on deep learning, very simple and intuitive."
3617,Neural networks (Deep learning) are (is) not science. They are (it is) populism.
3618,"Well done! This is the best introduction to neural networks I have found on YouTube. Some baby their viewers by brushing over the math entirely, and some try to delve too deep into the math for an introduction to this concept, which only obscures the underlying concepts. This video strikes the perfect balance. Also, the animations are really intuitive and well executed."
3619,When you can't discern any colors at 9:43 cuz Red Greed Deficiency üôÉ
3620,awesome dude!!!
3621,I will make a neural network that rates garlic bread from 1 to 10
3622,Please do a series on statistics.
3623,I can never understand some thing without you. Great great great!!
3624,Amazing explanation 3b1b
3625,nice video..comendable..keep it on
3626,"Thankyou so much for every single time.
This single video taught me more than I learned for a month for 4hours every day on Korea's best University's seminar. 

+ and your voice is better"
3627,I think of myself as one of the best teachers I've ever known (I had 12 years of university and have had a few 100 professors...) and have won awards for my teaching ability... but I don't think I could have made a better video.  I was watching this from the perspective of someone already in the know but looking for instructional materials to show friends / family.  But this video was absolutely superb.  I think the level of detail management and allocation of time was... optimal!  Thank you!
3628,"I'm taking linear algebra right now, and this is really cool to see."
3629,"√áok g√ºzel anlatƒ±yor, √∂ƒürendim ama ne donanƒ±mƒ±m var ne internetim. ƒ∞mkan olmayƒ±nca AI √ßok zor..."
3630,"After watching so many videos I came across this channel. And Now, I am not going anywhere!"
3631,Very nice...
3632,"Ive been following you since the beginning and Ive been wondering why your channel doesn't has so many subscribers. But even now, you deserve much more. Im a computer science and math student and I find those videos very exact, precise and good in quality. You do these videos without ""dumbing"" down for the major publicity."
3633,"Thank you so much! This is great. Cold you suggest any books geared towards developing programs of this nature? I'm studying network analysis  and have moderate quantitative skills, but very poor skills in programming which I would like to improve. It looks like you had an example of code in Python in the video. Is there a book that is a less generic introduction to python or coding that is geared towards network programs? I agree that understanding network algorithm architecture is VERY important, an that assuming a ""black box"" of the algorithm is not revealing."
3634,Awesome tutorial
3635,"But in keras, the outpul layer still always uses sigmoid as activation. Anyone knows why?"
3636,"Great video, really helped me grasp these concepts"
3637,"I subscribed, so that the neural networks of youtube's recommendation algorithm are primed to believe I want to see the next video"
3638,You can keep Mrs. Vocal Fry
3639,*BOLD*
3640,ÊâÄ‰ª•‰∏∫‰ªÄ‰πàÊúÄÂàùÊòØÁî®SÂáΩÊï∞Êù•ËÆ≠ÁªÉÔºüÔºüÔºüüòÇüòÇüòÇ
3641,At 17:11. He's so happy.
3642,"I've been trying to understand neural networks for so long now and haven't figured it out, but this video is amazing and i actually get it now!! Thanks"
3643,Trying to get into data science / ML. thanks for the video! It helps a lot.
3644,I made the neural network that underlies youtube recommendation algerithm are primed to believe that I want to see content from this channel get recommended to me xD
3645,"Amazing just amazing !!!!!
Even Grant's voice is so soothing and inspiring , keep up the good work Sir !!! People like you change the world and inspire others to do so too."
3646,"I've read so many tutorials and articles and pseudocode about neural networks, but this explanation is by far the best."
3647,Thank you thank you! Please more videos of Machine Learning and Neural Networks. PLEASEE!
3648,"The awkward moment you realise you have created a deep learning like algorithm technology selection method for NASA/ESA, but they will never read or care about it :/

And it means I was not exaggerating when I said that I was ""over intellectualising"" the selection process."
3649,nicely done :)
3650,best explanation I've seen of how a NN works... by far. Here is my subscription!
3651,I really do love how you peel away layers of abstraction to get to the heart of how things work.
3652,œÉ <-- Sigma
3653,Wow thank you! It was very clear even for a not good english speaker like me and the illustrations are very helpful!
3654,This math is a fair bit over my head but one thing which abundantly clear to me is that any software implementation of this is going to be (comparatively speaking) a resource pig.
3655,"Wow, first video i watched on your channel and I must say I'm shocked with the quality of videos"
3656,Behind this material is an extreme shot of giftedness. Explaining something is not easy. You first need a solid physical model for the topic in your brain and then you need to translate this model into a mental model that can be faithfully exported into others' brains. I congratulate you for this excellent job and I hope that you appreciate what you are and what you are doing. This is much more important than how much money this business brings.
3657,This is the best thing I have seen so far. Great job!
3658,"Why is there annoying background music, it distracts me."
3659,The best video on Neural network for beginners . Amazing animations at every stage to make us understand . thanks a lot.
3660,"You have no idea of how much I love this video, thank you for doing this. Your series will be very useful in my life"
3661,"Cant wait to see the videos you release for generative models! 
I've seen your probability videos and they were so great!

Personally, I want to say thank you for making all these videos. It really solidifies everything I've learned. The way you use visuals to describe certain concepts is amazing!
Keep it up!"
3662,"I want to ask you a question;

Since now we can ""simulate"" really basic sense (image recognition -- sound recognition and even more)

At what point are we stuck in making some real badass A.I ?
If your scheme is precise enought, it's not hard to think of a way to link together multiple algorithm; multiple possible output ""data base"" and stuff?"
3663,"What would happen, if there was a ""none of the above"" output?"
3664,She has to do something about her growling. Its brutal. Great content but the growling is just brutal.
3665,"So doesn't using reLU mean the activations of a neuron won't necessarily be less than one?  Because ReLU(a)=max(0,a) doesn't it?  So a>0 would have ReLU(a)=a>0."
3666,"Very clear video explanation, looking forward to the others.
I'm concerned that you have placed your assumption of how the problem is solved as being relevant. When we learn numbers, nobody assigns a set of criteria used to identify the definition of number shapes. No parent says, ""Now Tommy, it has a vertical and a horizontal line, so it could be a 1 or a 7. No Tommy, that's a T, or is it an L?"".
Surely the point is fuzzy self learning, otherwise we'd write algorithms to attempt this?
The exact same physical/software NN might be used to understand any number of problems and the point is, it is a black box, we don't need to know what is happening (how the values are assigned), and often the data is so subtle, it's difficult to understand anyway! We just give it a method so that it is able to balance itself to give the output that we require.
This is proved to an extent by training with the handwriting of a single individual. It would limit the ability to accurately 'understand' another person's handwriting.
Admittedly we may need larger/more hidden rows to improve accuracy, but given plenty of samples (and time ;) ) the NN should provide the answers without anyone understanding or caring about its weights & biases, other than from an academic POV. We can set all values to random numbers and then watch it move toward a required output as training progresses, their is no deep understanding required.
We can apply NN to problems that we don't understand, and have the NN succeed without formal training. e.g. spotting cancerous cells within a sample of cells, we just tell it which images have a 'bad' cell somewhere, we don't need to identify the cell. Or self-learning to play video games, where the score is a measure of 'goodness'. I see this, I try that and the effect was either good, or not, so adjust weights as required. Jointed robots can learn to move using NN, creating their own gait from scratch, usually they just twitch for a very long time! They tend to produce nothing that you or I would think of, and yet they move!"
3667,This is very very good. I am very happy. Thanks man.
3668,Awesome. Next Probability and Stats and I am all set for deep learning... :D
3669,teach your neural network to neural networks to create neural networks.
3670,i can¬¥t believe how happy i felt when i read the title of this video. i was literally with the mouth open for a while
3671,First time in my life that I suscribe to a channel after they ask for it. I just loved this video sooooo much. Thank you!
3672,Amazing video.  Best video introducing this topic.
3673,You are amazing.
3674,"Wow, what an incredible explanation of the structure of a NN!  Thanks and subscribed."
3675,Such amazing video! Thank you for sharing that!
3676,"Holy shit thats some mad video editing skills you got there!

Awesome presentation / video!"
3677,"But how do you know there should be 13000 neurons total? Why not 100, or 1000 per hidden layer?"
3678,Awesome video. Had a hard time trying to visualise this for a long time. Thank you!
3679,Thank you very much ! I have understood how it works !
3680,exspearmint
3681,"You...
You...
You saved.... my life!

Thank you 3b1b"
3682,What software does he use for animation?
3683,"Oh, I think I know which online book you used as reference"
3684,"@8:56, See that buzz in that square? My mind's that mad to appreciate and admire this guy..."
3685,You just summed up 2 weeks of my degree in less than 20 minutes. I really wish I had you as a lecturer. Thank you for this video!
3686,"Could you please make a video on averages? Arithmetic, geometric and harmonic average."
3687,Awesome ! continue like that you do a great job
3688,"OMG! It is too clear, understandable and intuitively explained!!!!! I love it!! Thank you so much!!"
3689,"My god, i'm so glad i found this video. I've been struggling to understand neural networks for a while now and finally i found something that's going to help me. Thank You <3"
3690,Porque el t√≠tulo est√° en espa√±ol y el v√≠deo en ingles
3691,This is such a cool explanation!
3692,"The thumbsup is for your video, not for that horribly creaky vocal fry your guest contributed."
3693,"Just to make sure. The bias vector should be [b0, b1, ... bk] instead of [b0, b1, ... bn] right? Since there will be k neurons in the second layer."
3694,My genius mind works this precisely.
3695,"A ""like"" doesn't do this video justice. Absolutely fabulous stuff. Please keep up this great work!"
3696,"Having been following the cryptocurrency world of late and how it manages data, it occurs to me that each filtering layer in this neural network is actually a hash result in a _'decision making block chain.'_  As a block of unknown geometry is sent from the visual cortex to the neural network, it sets off a series of hashing iterations until the 'key' ‚Äì a probability vector ‚Äì is discovered, from which a verification is needed to raise (or lower) confidence in the outcome.

Fascinating..."
3697,"Hello Grant, I have a question for you. How to use math and AI to win games? In particular tower defence games like Bloons TD?"
3698,"how do u select how many layers and neurons should be in hidden and input layers?
edit: I think input layer must have 786 neurons..."
3699,Nice and simple explanation thank you
3700,"Can you give us an interpretation of group theory?? All I know is the symmetry of n-polygon, but it's application includes variety of fields. How to visualise the core of group theory the abstraction that is there and appreciate it's foundation??"
3701,"The best animation ever.  
No surprise when I hear you got the Nobel Prize in Neural Network Animation.
Cool. very cool .
Thanks
J.N"
3702,So would the resolution of the system improve when adding more hidden layers? Also would you say each hidden node describes a subcomponent of a subcomponent?
3703,"If you want experiment on your own here's a link to the jpeg files. It took me a while to find. It saves as a .eggfile type. Download ALZip to open it. ALZip is a .exe file so I was wary at first but it's legit.  

https://www.amazon.com/clouddrive/share/S4KGsmQIRtfINLbZ9O3nF5QNh5Xk4ZG702sWKKhybDf/vrPxVZfLSIe6yEi-YJGsqQ?_encoding=UTF8&mgh=1&ref_=cd_ph_share_link_copy

http://www.altools.com/downloads/alzip.aspx"
3704,Really cool. I appreciate your work quite a bit. Thank you.
3705,I subscribed so that the neural networks that underlie youtube's recommendation algorithm are primed to believe that I want to see content from this channel recommended to me
3706,"Awesome video! As someone who's just starting learning neural networks, I don't have much trouble understanding this video and it helped me understand what weight and bias are! Thank you so much!"
3707,an excellent explanation of neural networks
3708,Absurdly great video! Thank you very much I'm eager to see the next one!
3709,i like the squishifier :)
3710,"just write a neural network to write better neural networks, duh!"
3711,"Great video, I was thinking that understanding deep learning without a master's degree is impossible before this video :)"
3712,"So helpful, thank you!"
3713,just found this channel thanks for your service to us!!!!!! great videos
3714,"""Squishy fication function"" Definitely gonna use that term from now on"
3715,"You are incredible! Btw, what software you use to edit the video?"
3716,Best explanation I have seen of a neural network. Thanks for doing all the work to present it this way for me. :)
3717,Just started watching the introduction and it is already awesome
3718,your video quality is getting higher and higher 0.0
3719,16:35 that's actually very smart advertising
3720,Amo esta tecnologia de las rnas
3721,"Can it works backwards? For example, if give a number to it, can it return me a drawing of the number?"
3722,"A very good explanation, the video is great.
Had I seen the video before I needed months to teach me the topic I would have understood it directly."
3723,The animated use of Pi has been great
3724,"I wonder how the tool in Inkscape works, which converts raster images into vector images. It doesn't seem to attempt to create line art. It only seems to be interested in contiguous blobs of color. I guess it's a very complicated problem, because it also has to decide the color palette of the resulting vector image, usually by quantizing the colors in the source image somehow. I wonder if you could create a neural network trained for the purpose of accurately quantizing colors from an image, to match the original colors that were used. Like in a drawing done in a painting program with brush tools with fuzzy edges, which will intersect shapes drawn in other colors, making areas that are a combination of both colors. You might want only the colors that were originally used to draw those shapes. Once you have a list of original colors, you could hand that information to another neural network to use in defining the shapes and lines drawn in the source image, to be recreated in the output vector drawing."
3725,"You had me by the balls at, ""download the code that does this and play with it on your own computer"".

Gives it to meee!"
3726,"Thank you. This video is amazing, I think learning neural network is nice but boring, but this video have an amazing animation that help me to understand."
3727,15:08 Ewww Python
3728,Dude! U good.
3729,This is Vsauce but on a much higher level hahaha. i feel like only higher educated people will appreciate this
3730,This video and the book really inspired me...never had more votivation to learn something in my life
3731,"A really nicely made video, but I have to say that using green and red wasn't the best choice for us colorblind people lol. At that point I was thinking ""ok, a grid with more lit or dimmed brown squares to show a possible outcome in a picture, so to make it easy to picture for us, neat"" ""now here's some green and red you can't distinguish to show you.."" ""fuck, my learning is impaired by my dumb eyes"""
3732,"12:30 Another fun thing is to think of the entire network as one single point in 13,002-dimensional parameter space. :D
Combine that with how weird volume behaves in higher dimensions (see 3b1b's other videos) and you get the so-called ~~~Curse of Dimensionality~~~. To learn means to move through this space to find regions where it gives better results."
3733,Amazing. Such a superb intro to DNN I haven't seen anywhere. Thank you very much for all the hard work you have put in to create such a beautiful video..
3734,Now how do I make one that learns arbitrarily from data given? I have a few tiny computers and 1 regular personal computer. I doubt I can make anything THAT cool but it would be fun
3735,What an awesome video!!!
3736,"Thank you, that was amazing"
3737,"I saw a video about neural networks, with a black/gray/white color value index of -1 to 1, instead of your 0 to 1. As well as the Sigmoid function ranging from negative Y to positive Y, as the x and Y collide at 0.

I'm very new to artifical neural networks (became interested of it today) and just curious if there's some reasoning to the slight differences, if one of the videos is wrong or if it's just up to preference as to what array limits you use, or maybe I'm just entirely lost and these things have nothing to do with each other.."
3738,"Oh boy, you teach like God!"
3739,"Plxxx, can has subtitles? Not these auto generated ones..."
3740,"Thank you +3Blue1Brown! This is a really great introduction. I don't know if you're going to talk about this later, but I am wondering how a neural network handles translation and/or rotation of digits. Right now it seems to be that the neurons are tied to specific pixels, so that shifts may pose a problem... For example, will the same digit, shifted to the right, still be recognized?"
3741,This video lets you know exactly which role do linear algebra and logistic regression have in deep learning! great job !
3742,Can u explain how it works on credit card fraud detection
3743,"I'm going to start a band called ""sigmoid squishification""."
3744,"render your videos in 1440p or 2160p please :) 

I feel like if you gave your [awesome] vector based animations a bit more render time at a higher resolution some people would be happier"
3745,Graphics are just smooth and perfect. Thank you ... eye candy
3746,"I've not succeeded yet to make a perceptron decide on a ""connected"" property, such as: is this figure a loop or is it open?"
3747,NICE!
3748,–û –≠–¢–û –ë–û–ñ–ï–°–¢–í–ï–ù–ù–û –°–ü–ê–°–ò–ë–û
3749,"Thank you for finally making a video that assumes that we will be able to understand at least some math. I have been looking for a video to explain this concept, but all of the videos just say ""they're connected"" basically and don't explain what is actually happening."
3750,Could someone please clearly explain to me 8:53 to 10:15? (the bits in particular I don't understand are weights and the weighted sum). Thanks
3751,"I see so many 'experts' in machine learning insisting on the fact that this has nothing to do conscious artifical intelligence but they fail to realize that this might very well be the way consciousness works. Many hidden layers we don't really know the meaning of, but that are tuned in such a way that we become 'conscious'."
3752,"Given that you're making this series on neural networks, would you consider doing a video explaining the latest version of Google's AI, AlphaGo Zero?"
3753,"when we combine neural-network and math so it work ,,,neural network ,it is work in daily computer design which is exist successfully in daily life . you have to realize how electricity behave as it work in brain what you are watching ,,it is also part of  neural network"
3754,Please make a video on Cinvolutional Neural networks and Recurrent neural networks
3755,Excellent! Thanks!
3756,wonderful
3757,you are too good bro by watching this video i learned too much about  neural network.
3758,"I know the knowledge is good to learn, it's just that some people they really have unpleasing  voice that bothers people. If youtubers want to be popular, they must either have good voice or hire someone with good voice, please."
3759,"It seams like there is a lot of redundancy there, when you write your number in that pixel field in most scenarios all those pixels around corners will have almost no meaning. that means one could optimize this neural network by throwing away those neurons,  right?"
3760,great vid
3761,This channel is awesome! Best thing I found on YouTube recently! Thanks a lot!
3762,"Thank you, very interesting video."
3763,i don't remember why i forgot to subscribe
3764,"one of the most interesting topics of our day explained by one of the most clear math divulgador on youtube, thank you! <3"
3765,Loved it <3. Love the way you put your amazing creativity in preparing the material! #Awesome
3766,sigmoid - vanishing gradient problem for DNN
3767,I was hoping this was about our brain's neuronal networks. Still a great video and I learned a lot.
3768,Smart
3769,"Great, Relates directly to understanding any of the MNIST and python (tensor flow, keras, theano) based deep learning tutorials. Also excellent for intuition about applying neural network concepts to other problems, by breaking processes into layers of problem solution that eventually combine into one cohesive solution.
Besides that, the video is magnificently produced and a joy to watch (although there is no need for the background music, a glaring distraction)."
3770,You are a freaking hero
3771,Hey. Is this one of the grids where a Hilberts curv would be best to use to parse the points into the neurons? if we increase the resolution then the connections would be close and maybe wouldn't have to learn from start?
3772,"10:00 
If the activation is between 0 and 1, how can the sum be greater if the weights for the surrounding pixels are negative? 
If one of those pixels is 0, than the weight times 0 equals 0 (the total sum wouldnt change), if the value of this pixel is non zero, then the negative weight times
the value would be negative and thus the total sum would decrease.
Maybe what is actually going on is, that if the weight is negative, the activation is reversed and then multiplied by the positive weight.. or am I missing something?"
3773,"Error report : 1st thanks for the wonderful quality  of your videos, i would like to point something out at 14:48 the Vector matrix b of biasis, should have a dimensionality of k and not n according to the dimentionality of your matrix W as dim W = k * n, Thank you !!!"
3774,"You want some deep learning?  Pause it at 8:30 and don't come back until you've figured it out.  Or maybe pause it at 8:30 and don't come back until you have your human life figured out.  Then, you might not even feel the need to learn about neural networks."
3775,Very helpful video!
3776,very nice!
3777,I LOVE THIS CHANNEL SO MUCH!!!
3778,"Your videos are amazing and explain everything so well, it's probably the only way I will be able to make a neural network in any programming language. Tensorflow is too hard for me, i'd rather make it myself"
3779,I thought I was already subscribed
3780,üëç
3781,Thanks a lot...
3782,Looking forward to part 2
3783,This is so fucking good! Thanks!!!
3784,"Nice video, love the channel."
3785,"I rarely comment on YouTube but I need to compliment this video: it's very well done, from the carefully selected content to the didactic and engaging presentation. Such nice animations!"
3786,"""I'm going to use green for positive, and red for negative weights"" *Noooooo, muh color blind!* No but seriously, thanks for this great video :-)"
3787,ReLU is limited by the numerical dynamic range of the system.  May not be a practical problem?
3788,"haha, computers are so stupid."
3789,I am hoping you are doing the series on deep learning. I already saw Part 2 being¬†released. So excited. Thank you. Your videos are very intuitive to learn from.
3790,The best channel on youtube. Period.
3791,Thank you!
3792,"Best video of neural network that I have seen before :)
It has a nice animation and describtion"
3793,"Best video on the subject I've came across. üëèüèª
What software are you using for the animations?"
3794,"Hm, just a quick question, I read in recent papers that the sigmoid isn't really the preferred activation function anymore for MLP networks, but functions like softplus, ELU or tanh
Like, ReLU isn't differentiable, what about that?"
3795,I'll like to see the video about CNN.
3796,"Question: I have Synesthesia, so numbers have colors by association. I have a mathematic language that can represent massive numbers compacted, similar to the squishification formula you showed. For instance: 784 would be represented by a square green pixel, square purple pixel, and a square orange pixel (3s and 7s are Green, 3s are round and 7s are square, and Fours are an Orange square pixel, 8 is represented by a Purple round pixel) super hard to explain the language and math, so I'm sorry if I failed to communicate it properly. If you google ""ChromaRythmatics"" you will find a lot of my formulas and the bases for how it works, can also display time on a base12 clock in color using ChromaRythmatics within a colored Lemniscate with 2/3 loops, 2 for AM and 3 for PM. Once again sorry for my failures and weakness with the English language, if anyone can understand my English I can answer questions"
3797,16:52 Notch is your Patreon? :p
3798,"I thought when I clicked on this video I wouldn't be able to follow, as machine learning was an approach to programming I'd never looked into mathematically.However you presented it so cleanly that this really makes sense and mentioned a probability function which I really should have heard of before, but just hadn't."
3799,Thanks for making the video! It's one of the best NN video's I've watched. The clarity and accuracy (at least apparent accuracy as I'm a newbie to understanding this topic so I don't know how accurate it is!) of the video helped solidify my understanding I've gleaned from other sources. I look forward to watching more of your videos!
3800,reference to python code for this problem will be very useful
3801,"First video I watched from you... GREAT VIDEO!
*subscribe*"
3802,"I just want to thank you for this video.  I had been interested in the idea of a NN, but hadn't really gone in depth.  Because of this video I was able to write a neural network and teach it to play tic-tac-toe. At the first (random) generation, the network lost about 30% of the time against a randomly choosing opponent.  With a very simple breeding function as well as a plus or minus 3% randomizing function to propagate the successful networks into the new generation, I was able to drop the loss percentage to under 10% by the 5th generation.  It was a super fun project  and it wouldn't have been possible without this video.  Thanks again!  (This was done from scratch in MATLAB btw)"
3803,When is Part 2 coming out?
3804,"Thank you for this amazing video. But at 14:59, should the vector b(ias) be k-dimensional instead of n, so that it is consistent with the result of the matrix product?"
3805,"This all sounds so circular. Our brains can pick out numbers or letters on a page because we understand the symbols. When we're learning we might looks for loops and lines, but when we know the symbols we aren't reverting back to scan for the components of the symbols. Are neural networks doomed to looking for loops and lines and the components of those loops and lines and the pixels on a 28*28 square for every symbol they have to read? Cant we teach a neural network as we'd teach an infant?"
3806,Please do essence of algorithms!
3807,"But what *is* a Neural Network? | Deep learning, Part 1"
3808,this is gold
3809,"I love how you focus on spending time gaining the right intuitions. 

Also, its a little sad that you only plan on doing one more video in this series. Hence, please provide even more links to resources for further learning in the 'description' section of that video. And by further, I mean even further than what the description of this video covers."
3810,"Awesome video!
Where did you say we could download the code?"
3811,Wow
3812,"Surely in practice there would have to be far more than 16 neurons in the first hidden layer?

How could that layer, with 16 neurons, recognise a single short line segment, given that it could occur in a large number of potential positions within the input layer?

Or am I missing something here?"
3813,Squishification function...XD
3814,We can easily tell it is 3 because there is no very similar character/sign to 3. So we assume it is 3 because we encounter the character very often. We wouldnt be so sure right away if we had to guess whwther it is 1 or l or I or | (one or lowercase L or upper case i or a pipe). Or maybe it is a cursive 1 or a slash sign? Who knows?
3815,"Can I just tell you that the way you visualize data and their arrangements is absolutely amazing?

Because that is what you do!"
3816,"It‚Äôs topics like this that want me to change from a Physics and Math major to a Computer Science major. Curse you 3b3b, for making me question my intellectual allegiance! 

P.S. Great job as always"
3817,Is the horizontal line recognition neuron from the example position independent? Will it recognize horizontal line if it will be in different part of the image?
3818,Brains don't recognize or see anything. Only persons can do so.
3819,This is great!
3820,i have gone through you entire series on matrices about 3 times and every time i need a reference. Your videos are absolutely beautiful and i appreciate the depth of effort you put into each. Great job 3Blue1Brown. You are my youtube Hero.
3821,It's my graduation project's topic and I really need these information
3822,"We're waiting for part 2üëåüèªüëçüèª
& thank you so much for explaining the idea in a simple way"
3823,"Thanks for a cool video! When making the video about learning I suppose you are going to focus on forward and backward propagation. Can you in the end or in a separate talk about more layers added to the most elegant networks like pooling, normalisation, dropout, PCA, stochastic gradient descent etc. Basically all they used in the Deep convolutional network of Alexnet and how it ties together with the flow in network. Like is that applied after one round of forward and backward propagation ? Thank :)"
3824,"at 9:49,"" if we made all the weight associate with pixels zero except the ones in the region we care about,  then taking the weighted sum of all the pixel values really just amounts to adding up all the values just in the region we care about. ""
this is really convoluted way to saying things. How about this,  ""if we have some weight associated with pixels zero except the ones in the region,......"" would it be better to say this way?  Instead of ""if we made all the "" , replaced with "" if we have some weight associated with pixel zeros..."". Let me know what do you think"
3825,"Want to do something good with your bitcoin??? Please donate them . I need only 0.25 bitcoin total to make my trip to Sint Maarten to help people over there with food and clothing... wallet adres:1JdcvpV9Xvwqm3KtHffsuPggS4PA8fCfjS
Thanks for taking part and help people who lost their home"
3826,"Why they were inspired by biology, it seems mathematically the purpose of neural networks is to provide a method for defining a wide range of functions which continuously distributed in a manner that matches the problem being tackles so it's easy to find the optimal solution."
3827,Its a big function.
3828,You should do a video about fraction bases
3829,Fantastic! Can't wait for part 2!
3830,Subscribed. Your explaination is soooo easy to follow.
3831,Mispronunciation at 4:30. Apart from that this is very good.
3832,"there's actually a program without neural net, get all the white values then calculate the distance from the original value group then sort by lowest distance, that is the value you are looking for."
3833,Maybe you can talk about gradient descent or hillclimbers :)
3834,Give me part 2 I need it NOAAWW
3835,"You should create ""Essence of  quantum mechanics"""
3836,"Hi. Sigmoid logistics curve (sigma times X equals one /one plus e^-x).  Can you please get into how 784 weights per neurons as matrices relates to gravity or game theory. (11:53 ) myguitarfootpedal has 13,002 ""knobs"". Thank you for the YouTube progress. ~n8i"
3837,oh if it's just a function then Can I take inverse of it and and thus can easily set the patrameters??ü§î
3838,"Great explanation.
I've read from various sources what neural networks are, but certainly your video gave me a better understanding. Finally I found an easier and clearer explanation about the function of the bias. Thanks a lot."
3839,Awesomeeeeeeeeeee!!!
3840,This is definitely one of the best explanations I have seen so far! Proud to be a patron!
3841,You are just awesome.
3842,It's really an excellent introduction video. One of the best ever seen! Thank you!
3843,Great video as usual.
3844,"I find it better to, instead of having each neuron have a bias, have a neuron in each layer but the last with the constant value of 1, such that the weight of their connections serves as the bias for neurons in the next layer. Makes it less overwhelming to think about all the variables."
3845,This is brilliant. I was wondering if you could do a video on the Abel-Ruffini theorem? Anything proof that proves insolubility would be great.
3846,"I first came to this channel looking to beef up on maths for the sake of machine learning and now we've come full circle :D Can't begin to say how cool that is!

P.S. I'm having flashbacks to your space-filling-curves video. Think there's anything to that?"
3847,One of the best videos I have seen so far in explaining neural networks
3848,"Perfect video to grasp how it works, but I could not find the video ""Deep Laerning, Part 2"" ..."
3849,What happens if you propagate a neurone firing from the right to the left? I mean the neural network is working from number guessing neurones down to the pixel neurones (reverse working)? Does firing number 6 shows pixels with shape of a six? no for sure ... but ...
3850,"Hii, am from india,,, your channel inspires me so very much towards the world of mathematics thank u very much.
whenever i get bored of maths i see your videos and get rejuvenated...never stop making these amazing videos :)"
3851,Sir will u kindly please tell me which software u ar  using for making ur video
3852,Great intro video - Love the illustration!
3853,"Thanks for the video. Im a novice to deep learning, ur video helped me to understand neural network visually . Waiting for the part 2  , Cheers :)"
3854,"Will you be talking more about Sigmoid vs. ReLU? I've a total noob to ML but i know a lot of people say ReLU is vastly better, and it has completely different behaviour (no bound on output value for example being a huge difference), so i would love to hear a deeper analysis of it!"
3855,"How does reLU stay under the upper bound of 1? If the answer is that it doesn't, how do you stop neurons from becoming ""overpowered"" later in the network? Basically, how does the network change when it's neurons are allowed to be greater then 1?"
3856,"Omg, sooooo excited for when y'all post part 2!

Is it within you guys' format to delve into the tools for networks and their learning, or what kinds of networks are best for what tasks? Or could you recommend a good primer video / website?"
3857,WELL THANX MR 3B1B! Because of YOU and THIS video I spent another X hours researching about NNs and DL ending up in building a neural network sample in JavaScript that can drive a dummy car ... THANK YOU VERY MUCH! Not that I don't have ANY time left at all now... ( :P <3 )
3858,The Coding Train sent me here.
3859,I want to divorce my wife and marry this video
3860,Thanks for explaining what the bias does. A lot of other video's only mention it briefly
3861,is this a decision tree model ?
3862,Amazing work. Great presentation
3863,What is the problem with the voice of the girl by the end?
3864,"I don't understand the process at 9:25, if you put weights on each pixel in the rectangle trying to capture the top of the seven, or another digit that has a straight top like a five, doesn't it get ruined if the person who wrote the digit doesn't completely center it? So if I write a seven that is taller so that instead the stem of the seven passes through that rectangle, wouldn't' the process be ruined? Or is it being based on averages so that it looks where most people will write the top of the seven, but in this case how would it find that out beforehand? Or instead has that problem already been solved by making sure that the first pixeled image of the digit is centered, although I could still imagine that to be a problem if a number is angled a certain way, then how to decide if it is centered? 

Very interesting video though, thanks a lot."
3865,"An off-topic question to Americans out there: would you say that Lisha's pronunciation is very much Californian (or even, more narrowly, Bay-area-esque?). I am trying to improve my understanding of American regional pronunciation for no particular reason except that it fascinates me. :) Thanks!"
3866,"This seems a little bit misleading. The edges becoming patterns etc. Idea is often applied to convolutional neural networks which also aren't fully connected. Fully connected layers like MLPs don't necessarily have this nice ""each layer abstracts the data"" effect."
3867,summarizing: they are successive filter systems
3868,Q: Why again do I need the sigmoid function?
3869,"Your explanation definitely explains classical NNs, as I understood the stuff back in the 80s.  But what I still don't understand,and no one has yet answered is that classical NNs _recognize_ inputs, but many people today are using NNs to _generate_ random things in a given class (computer generated music, Magic: the Gathering cards, etc). How do you go from recognizing/classifying to generating?"
3870,"That's the best video about deep learning out there, by far."
3871,"Thank you very much for this series. For the moment I'm watching your Linear algebra tutorials. As precondition to study neural networks. And today you have released this first part of neural network series.¬†
Your tuts are gems on youtube."
3872,This is the first time I've eagerly checked to see when the new 3b1b video would drop.
3873,"The first minute of this video is exactly my project in my senior level class in college, Intro to Artificial Intelligence. a 28x28 matrix where we need to write a program to determine what value it is. Seems easy until you start coding it. I hope the next 19 minutes of this video will make my assignment easier and more intuitive. Thanx for the high quality videos. Last year studying then time to become a patreon!!!!!"
3874,"These videos that you make are absolutely amazing! So helpful with learning these concepts. It's awesome that you go into as much detail as you do in every video even though they are over so many different topics. Just out of curiosity, what's your educational background? Where did you learn all of this?"
3875,the best video on deep learning
3876,"I turn on notifications for every channel as I subscribe to it - it's turned into such a reflex by now that I hadn't really considered it but I suppose YouTube's idea is that the ""default"" is not to be notified. When I subscribe to a channel, I want to be told when they have new content. That's what a subscription is, no? Why on earth is ""receive all upload notifications for subscriptions"" not the default option? Or even an option at all, as far as I'm aware."
3877,the words which u used in this vid are little bit hard to understand for those non-native english speaker.....
3878,@ =3
3879,Beautiful job!
3880,gotta squishify those input vectors
3881,Awesome content and visual. Could you tell me tool do you use for making the graphics?
3882,best explanation I've seen on NN
3883,"An exceptionally good video. Outstanding quality. Thank you for taking the time to make it, and share it with us on Youtube."
3884,Squishification function
3885,"""squishification function"""
3886,Thanh kui
3887,Would anyone write the English script for this great video..? Cuz I wanna to translate it to my language...
3888,This is video we needed but didnt deserve. Praise jesus I love you 3b1b
3889,This was seriously very informative. Excellent job!
3890,"Sorry, why is it necessary to restrict the weighted sums to the range 0-1?  I understand that it's analogous to biological neurons, but I'm not clear on why that analog is important to maintain."
3891,"""Maybe future videos?""
God yes please."
3892,Amazing video. Thank you!
3893,Amazingly great video!!!
3894,great video for sketching the picture of NN
3895,cant wait for the second video
3896,"What a very nice video. Thanks a lot. after watching many many videos of 3b1b, I got to say that hearing a different voice in the video was not expected (and a bit scary at the very first moment : d)"
3897,"By meaningfully active, does the author mean the output of sigmoid function to be greater than 0.5? In such case it would explain the purpose of bias."
3898,"About ReLU, you can write it as

f(x)= x * sqrt((Abs(x))/x)"
3899,Your content is great. Keep it up!
3900,I never truly understood what the math symbols for neural networks did until I watched this 19 min video
3901,Obligatory xkcd comic: https://xkcd.com/1838/
3902,"You are without a doubt my favourite YouTuber mate. I'm doing a masters in Maths (just started a course in Topology, funnily enough) and your videos still give me a brilliant understanding of whatever they're about. I can only imagine how helpful they are to people doing A level or Undergrad :)"
3903,Jees this video is amazing
3904,This not just an edu video. It is a project. The author put in there a lot of his time.
3905,"Man, you're completely awesome! This is the simplest, efficient and powerful explanation of neural networks I've ever seen. I can't wait to see your next video about deep learning! Thank you for doing this and keep up the good work!"
3906,"Sir if u can upload videos related to fourier series, transform and z transform it will be great helpful for us.."
3907,Yes! This is the best explanation of NNs I've ever seen. Thanks :)
3908,"Wish you were there in my college days...
Brought back the intuition in learning..."
3909,"love u man, thats what I was waiting for"
3910,Awesome!
3911,*But it's just a function* Great video!!
3912,Great watch as always! I was expecting to see a tie-in to Hilbert curves as a way to arrange the input pixels to the first layer. Would there be any advantage to doing this?
3913,"üìÇDocuments
  ‚îîüìÅVideos
      ‚îîüìÅ 3Blue1Brown
          ‚îîüìÅ Bad Videos
              ‚îî‚ö†Ô∏è This folder is empty"
3914,"MFW 3B1B didn't make this video, it was made by a generative neural network"
3915,This is a cool primer for a Python course I'm taking on machine learning! Keep up the great work!
3916,Great video !
3917,"For anyone interested in learning more, there's currently an edx course by Caltech that covers this in more detail. "
3918,you're the best
3919,TY! Waiting for part 2
3920,+3Blue1Brown This was a really good video (as always).. Hope you can convert this into a series on machine learning.. Like you did with Essence of Calculus and Essence of Linear Algebra.. Machine learning is a pretty big thing right now and it'll definitely help people understand what it entails..
3921,"Awesome video, new subscriber here"
3922,"Star Trek - Deanna Troi quoting Data ""As I experience certain sensory input patterns, my mental pathways become accustomed to them. The inputs eventually are anticipated and even missed when absent."" 

I've been reading material on networks for years. It's only been in the last ten or so that decent explanations and practical examples have surfaced for those of us that don't have advanced understanding of mathematics. I like your take on the idea of a bunch of 'knobs' - and here's why. 

Knobs are an analog in more than one way here. But mostly because it temporarily removes the 'strictness' of the digital paradigm and gives the sensation of something that is smoothly adjusted, as if bringing into focus as best as possible the image that is being evaluated into an eventual deterministic result. So thanks for that thought process.

It made me think of beams of light emanating from each of the input layer neurons, and then aimed as if a laser to the receiving second layer - through a mesh that effected the frequency (weight) of the light beam - where the second layer was like an array of prisms that could be turned.

I look forward to part II and hope that you will get into backward propagation."
3923,Hence I'm referring to the sigmoid as a squishification function.
3924,"when will you make the next ""Essence"" video"
3925,"I hated my machine learning professor

the topics are great but she was miserable/cannot teach..."
3926,This is _so_ very well explained.
3927,"Great video, thanks. It gives me (by far) the most thorough understanding of how neuron network works (since I can't get focused when reading the books with only matrix notations). And as always, the animation is always clear and make everything understandable for nonspecialists. By the way, I heard about you use python to do everything, including all the animations and computations needed for every video. Can you tell me what library you use for all these? Thx 3b1b"
3928,do all video in one day  XD. I can't wait XD
3929,"I would love to see an even higher-level video that situates Machine Learning / Neural Networks within the panoply of statistical techniques for data reduction and feature extraction/recognition.

For example, PCA and SVD represent the apex of a long line of statistical techniques that do their thing without training, that answer the question ""What can this data tell me about itself?""

Machine Learning is at the other extreme, requiring not only extensive training, but also the separation of data into training and validation sets, answering the question ""How does this data relate to all other data I've seen?""  And Machine Learning itself is split into many sub-domains, primarily based on the presence or absence of tagged data to use for learning, but also on the need for time-domain processing (versus single-event processing).

In between fall ""low training"" techniques, such as Gradient Boosting, and perhaps some Bayesian techniques."
3930,Wow right on time!!  I am currently taking an intro to Machine Learning class right now.  Ty
3931,Thank you!!!!!!!!!!!!!
3932,I never thought some one can explain deep learning like this. :3
3933,Wow!
3934,Fuzzy logic?
3935,"""not as a buzzword but as a piece of maths""
THANK YOU"
3936,The content on this channel is so unbelievably outstanding. I can only hope to one day be able to break down and visualize complex problems half this well
3937,Wonderful video!
3938,"Just Excellent !! Can't wait for the future videos on DL. Hope you cover Recurrent, LSTMs and Convolutional Networks too."
3939,"I'll contact you for AKiLi
(A-rtificial Ki-swahili L-anguage i-nteligence)"
3940,"Very good clean description of how computer learning is a formula.  Thumbs up.

I understand the motivation for using neurons and neural networks but those terms lead people to think computer neural networks are like brains and they are not.  All analogies break down eventually, the computer neural network and the biological neural networks breaks down right away.  For example, brain neurons either fire (depolarize) or they don't where the computer neurons can hold a continuous range of values (despite the sigma function).   I think the computer networks should be described as having nodes (input nodes, hidden nodes and output nodes) to avoid thinking computers will have feelings anytime soon."
3941,"My brain is a neuronet processor. A learning computer,"
3942,"What if the sigmoid gets a result close to 1/2, how does the network proceed? How could it decide, if there is a line segment there or there isn't?"
3943,"This criticism is meant in the most constructive way imaginable: Please, Lisha Lee (I hope I'm spelling that name correctly), please, train off that (I'm sorry) horrible vocal fry. Especially when you're doing voice-overs."
3944,Is it worth subscribing? (i am someone that is learning programing).
3945,"A huge thumbs up for this video, 3b1b! Can't wait for the second one!"
3946,"This is beautiful, thanks for the great content you create. You inspire me to learn how machines learn. Thanks."
3947,Can't decide which is sexier: Lisha's voice or this video.
3948,"Thank you. One note: please never show examples of python code with *agrs, **kwargs placeholder. It's a really nasty way to break introspection and readability."
3949,I like your style!
3950,Will you talk about AlphaGo in next videos?
3951,Very nicely explained
3952,3:30 You're missing a pixel.
3953,The people who don't see new videos dont look at their subscriptions page
3954,"Oh man, unbelievable! Though I've read many articles about Neural Network, I didn't figure out how it works until I found this video on *this* channel! Thank you so much!

BTW, I'm a little confused by the (Pseudo-)Python code. Is it better to use ""a[i]"" or something else?"
3955,man the production value on your videos are insanely good
3956,Yesss please moreee :D
3957,Thank you for making this video.
3958,"at 15:03, i think your b vector should be k by 1, not n by 1, because your weight W is k by n and a(0) is n by 1, which outputs a k by vector. You can't add the b vector unless it is k by 1"
3959,"Please do some neutron visualisation for the hidden layers, producing pictures that strongly or sometimes best moderately strongly activate the neurones :)

I'd love to see more of this, it's often fascinating. I miss the app dreamify, it was amazing but they've taken there servers down and it cannot be used anymore, everything else uses no where near enough compute or small enough steps for the iterations or enough iterations etc... and consequently just looks like faint pastel dog stickers over everything. Deep dream however can deeply understand 3d shape and even almost narrative context and be almost photorealistic as well as understanding a lot of ideas and forms despite dogs and working with the physical makeup of objects from photos etc..."
3960,This is so cool I'm starting to consider changing my major to it
3961,I've learned about neural networks at university and wish I had this great introduction when I started. I've never seen a more concise description to that topic. Just discovered his channel through this video and I'm blown away by his teaching style. Made my day!
3962,So much vocal fry
3963,"jesus christ, this is a good explanation"
3964,"I've seen many theoretic neural network intro videos, and this is by far the most approachable and informative. Thank you very much, can't wait to see more of these :)"
3965,Subscribed and Tweeted. Really interesting presentation/essay of a very interesting topic.
3966,Thanks. That is the best intro to the subject I've seen out there. Great job.
3967,God ... everytime I look at your videos I always wonder *'How did he do that? How he edited that?'* . Always admired your job. Thanks for the great content.
3968,Waiting eagerly for part 2 !!!
3969,.
3970,"concerning ReLUs and sigmoids Geoffrey Hinton gives the idea that you can basically think of ReLU as the sum of the activations of several sigmoid neurons with equidistant negative biases. So more neuron with shared weights are still more units, yielding a more complex network without the  cost of additional parameters to be trained"
3971,"Thank you 3b1b. I like very much every video of yours. It amases me how every subject that i am interested in you put a video on, love all your videos. Keep up the good work, your channel is unique!"
3972,Should there not be an eleventh possible output value for 'Not a Number' cases?
3973,Love your Videos so much man. Keep going
3974,Amazing video!
3975,Beautiful !!!
3976,"I could never get my neural net to work well when I was playing with them. I never tested it, but I always thought character recognition would work better by normalizing the pen stroke and feeding the data of angle between equidistant points along the stroke into the net.

I'm sure someone's done this, just my thoughts."
3977,"Cool vid man!
You should check out my neural network cars!"
3978,OMFG finally someone actually explains the fucking maths! I've been waiting so long for someone to do this x3 I can't wait for the backpropagation
3979,Thank you so much for explaining this. I finally understood why a sigmoid function is needed. Would be amazing if you could cover Long-Short Term Networks at one point.
3980,"Excellent series, keep continuing!"
3981,really helped me understand neural networks i wish you would upload the next video this is perfect
3982,"after watching this video it reminded me of a thing i watched before by sethbling. he coded a neural network that learned to play mario, called marI/O. so i watched that video again and that program didnt use preset ""hidden layers"" i thought it was more fascinating after watching your video."
3983,This video go interested in my linear algebra class again. I'm working thorugh the one found on www.lem.ma. I also recognize elements from Andrew Ng's Machine Learning course. I'm looking forward to the next video.
3984,Wow. I always thought I was really dense when it came to this kind of thing; not a math guy. You not only made one of the biggest technologies of our time eminently understandable but got me curious about your other videos too. Subbed.
3985,"Also, its nice to see your carrier going strong. I've been watching since you had 3 videos."
3986,This is the best intro to neural networks I have ever seen. The presentation is excellent! The animations are very very very helpful especially in understanding the formulas and matrices and how they came to be. Thanks a million. Looking forward for the next one.
3987,Been asking this for a while: Can you make a video on mathematical logic?
3988,This video was absolutely amazing. I entered with no knowledge at all of deep learning/machine learning/AI and now I feel like I know a ton about it already. Thank you so much for this.
3989,One of the best videos out there for beginners to learn more about Neural Networks. Great work!
3990,"Plan to patreon.
Not aware of any other lecturer who's able to clearly illuminate the conceptual aspects of topics underpinning the subjects you cover in such a clear graphical way, all the while using a flowing narrative which then links the parts into a cohesive whole. (Btw why don't more lecturers do this, isn't linking the pieces to see where else one can go the 'point' after all ?) 
Not taking these uploads for granted either: I recall that the former host of the series ""Space Time"" left to do other things just as he was reaching his stride in the brilliant exposition of commonly misunderstood topics usually only treated with an inadequate or even an inaccurate pop-science synopsis, else left to laborious extrapolation from obtuse course texts. (The new Space Time host is a very good communicator, it's just that sometimes after only a single episode presentation of path integral QM for example, I'm then left wanting a complete in-depth series of stepped presentations for that topic.) For me your presentations seem to be in a 'goldilocks' zone of sorts: having just the right amount of stepped quantitative building blocks analysis to fully appreciate the 'why' as the path taken to the conceptual insight payoff. 
Hopefully philanthropic organizations that support science education will make sufficient donations such that you'll always have the resources and incentives to continue making these in the way you make them."
3991,"Are the ""hidden layers"" really needed?"
3992,"Where's deep learning, part 2?"
3993,"Wow! Your work is amazing. I can barely express how amazing it's. Not only the high quality edition (what software do you use?) and your great didactic but mainly the effort for giving knowledge to people without charging a dollar. And it isn't only in the video, look at this amazing material in the video's description. I'm really thankful for the Michael Nielsen's book (and it comes with ready data for the training process!). It gives me a hope in this fucked up world. We need more of this kinda of thing.

Sorry if my grammar is somehow messy, English isn't my first language and despite I could learn it by myself (along with the internet and the tons of English speaker's media that comes via cultural imperialism and so on) I haven't any practice in writing in English. Anyway, THANK YOU!"
3994,is their a way to super like a video because this told me every thing that the click batey videos  did'nt with  out slogging thought research papers
3995,Thank you! Would you ever consider doing a video series on probability?
3996,Great Video!
3997,This channel is awesome. New Patreon confirmed. Please don't stop.
3998,Kind of setting the principles to hack captchas here!
3999,"Neural networks is a topic I've wanted an intuitive understanding of for a while. 3b1b has the most intuitive explanations on YouTube.

This video could not be any better."
4000,"Please, I need part 2 NOW!!!!  :-)"
4001,"I got to 11:51 and decided ""I'm done. It's complicated, I got it"""
4002,16:26 I end up hearing stuff like this a lot. Is it that rare for people to use their sub box? I set my YouTube bookmark to be my sub box years ago.
4003,"ŸÅÿ∏Ÿäÿπ :""D"
4004,(at least in this example) turning machine learning directly into linear algebra blows my mind! I'm guessing that there is something for defining each of the weight functions and how to iterate and improve that will be coming up in the next video and I am looking forward to it.
4005,I absolutely love you expanding into math in physics and other fields.
4006,"yep, can't wait on how the learning process is done!"
4007,"Excellent video. I was just about to say that sigmoid wouldn't be a function you'd use and then I heard this lady and she did it for me. Generally speaking you would simply use a function for your model and try to optimize it as much as possible.

Thankfully, this becomes easier and easier (see TensorFlow from Google). Just a while ago (very recently that is), optimizing functions would be a very specialized role (maybe it still is to a certain degree) for mathematicians."
4008,"new to the channel, but this is a great video! Love the way you're able to split abstract concepts and how your animation works to well to help. Looking forward to more"
4009,"wow, just when I'm working on my own Neural Network which I've coded in C
but somehow I'm stuck at writing and understanding on back propagation function."
4010,"Wow it all makes sense now! 
If anybody is new to machine learning, I highly suggest you read this article! It's a high-level view of what it is:
https://www.linkedin.com/pulse/what-learning-machine-whats-difference-randy-lao/?trackingId=hsVaIObKHQwNbAIeJ9prgQ%3D%3D"
4011,Very good episode!
4012,more videos!
4013,"Great video, this was so interesting. Thanks for your work, 3b1b"
4014,Looks like those captchas are just made for collecting data for some AI to train on.
4015,What computer language was used in this video ?
4016,"PERFECT VIDEO.... Please take it one step further, like part2, and talk about the BACKPROPAGATION algorithm ;)"
4017,"""I'll use green pixels to indicate positive weights and red pixels to indicate negative weights""
I hate being colorblind"
4018,Thank you so much for making a video about this! I've been so interested in this topic from the math side for a while now
4019,Sooooo terminator?
4020,This video was really well made and your animations are the real shit
4021,This has been the first video on this subject I've seen which really made things understandable for me.  Thanks!
4022,"Hey 3b1b, can you tell me what software/tool you used to produce that Neural Network visualisation?  I've been searching for months on end trying to find a tool to demo how a certain algorithm works as it moves through each layer of the NN. Please 3b1b, tell me how you made that animation at 2.35, please!"
4023,The way you explained Subscribing just made me subscribe. I would love to see more videos like this in my recommended videos list.
4024,"I always wanted to know how neural networks work, but I struggled to find good introduction to the subject - until now. I just subscribed and already I can't wait for the next video!"
4025,"So I‚Äôve read the book by Michael Nielsen over the past week and now you make a video about it. Additionally I will have a presentation exam about machine learning in a week...
Sometimes these coincidences are so mind boggling"
4026,Show this video to the professors and they will be speechless coz of how clear your explanations are.
4027,"This channel is the absolute best.
Here‚Äôs a list of nearly every single thing related to math I have previously discovered on my own, thought sounded amazing, and desperately wanted understand:
Topology.
The zeta function.
Calculus.
Cryptography.
Fractal geometry.
Graph theory.
Euler‚Äôs identity.
Quantum mechanics, for goodness sake! (Thanks, Henry.)
And now even neural nets!?!?
That‚Äôs essentially everything I‚Äôve ever thought was cool!
The only thing left now would probably be chaos theory. And the honestly beautiful music and animation only adds icing to the topological 3-sphere."
4028,I have not seen any one else explain these concepts so beautifully like you do. You are the best
4029,Can't wait for the next video. So glad that I know about your channel <3
4030,"The book by Michael Nielsen you reference to was actually one of the first things I read about how neutral networks operate. I can really recommend it, because it shows all the steps it takes until you can build your own neutral network and it has really helpful interactive elements which help you grasp how everything works.
Additionally the used math is easily understandable and does not use all those complicated symbols which are specifically used for describing neutral networks. Though it introduces these symbols step by step so everything stays easy to understand.
Everyone who wants to get into learning about neutral networks or wants to improve their understanding on how they work should really take a look at this book."
4031,"Man, thank you so much"
4032,Who would hit dislike on this?????!
4033,"Well taught, I really agree with the sentiment of getting your hands dirty looking at the weights. I personally taught myself how ANN's work with a much smaller problem that allowed me to draw out the network and manually go through and calculate outputs given static weights to help myself understand how it works.

Essentially, you're starting from randomness and converging on order with the use of a heuristic. Whether that heuristic is obtained via tagged inputs (i.e. supervised learning - let's remember we encode our own biases into the input when we follow this method) or by genetic algorithm (again we encode our own biases in the fitness function here as well) we have to be aware that the machine is not really ""learning"" anything, it's merely converging on the knowledge we encoded into the software (either through metadata or in the code).

Not that the author of the video was implying otherwise, but I feel we need to be diligent about how we talk about machine ""learning"" and understand that we don't really understand how learning works. Therefore we've merely modeled learning, which is quite different and our assumptions and biases are 100% encoded into the model.

Here is a great resource:-
http://www.ai-junkie.com/ann/evolved/nnt1.html"
4034,"Would a neural network do better if you used the hilbert curve to pass through the pixels rather than stacked rows? I don't think it will affect it since layers don't care about the order, but in recognizing shapes I would think it's better to have close-by 2D shapes be also close in their 1D array"
4035,"If only he did videos on tensor calculus, and other advanced topics, necessary to understand all the scientific papers from arxiv :))"
4036,Shouldn't the b vector yield k components instead of n?
4037,I want the next part please.
4038,"Given a set of strokes,
how do NN in applications - like handwriting recognition - take the context in which the strokes appear into consideration?

For example, if you write ""I3"" by hand:
""I3"" in the context of 468""I3""9 should be interpeted as 13,
and in the context of A""I3""C as B.

But what if ""I3"" appears in a handwritten text on its own?

Like:
""I3"" is my favourite number. 
VS
""I3"" is my favourite letter."
4039,Greeeeeat
4040,Great as always and great recomended material in the description.
4041,Commenting to set another input neuron.
4042,Great video!
4043,Looking forward to part 2! (And I hope I do get a youttube alarm) :c)
4044,Your videos are so insightful !
4045,This channel is so classy. Thanks :)
4046,Great animations. Well done!
4047,"3b1b, when you release your series on probability, will you go over probability density functions intuitively?¬† And if so, could you give an example of how you can take a discete system, take the appropriate limit, and obtain a pdf?"
4048,Absolutely amazing video. Can't wait for the next.
4049,Intelligence is embedded in maths. However this seems to be just the top of the iceberg.
4050,Those subtle animations really helped me understand what actually goes on. Thanks a lot @3Blue1Brown ! Eagerly waiting for the next video to be uploaded.
4051,3:39-3:45 was absolutely spectacular!!!
4052,"awesome channel, +1"
4053,Can u also go through SVM because it is more common and less cost than deep learning if u are learning AI I suggest u can start with SVM
4054,"This is amazingly helpful - I often drag/drop neural networks in SAS E-Miner, just to see if the results are better than some of my regressions or decision trees... but I never really knew what they did :)"
4055,"can you reverse it so it ""wights"" a number from the same networke?"
4056,"I looked for the second part assuming this video is old. It was published yesterday :(

Please hurry up, I have a capstone project based on machine learning"
4057,Excellent video!
4058,You have the best videos on math. It's ridiculous how well polished and presented they are!
4059,"Don't you think it is relevant to cover some background theory in machine learning, such as hyper planes, polyhedra, optimization, classification (SVM for example, which is just 1 neuron) before diving into NNs? Otherwise great job at giving such an intuitive explanation."
4060,at 14:45 should the vector of biases not just go from 0 to k instead of 0 to n? I thought k was the number of neurons in the new layer while n was the number of neurons in the previous layer
4061,Zero to ten? D'oh that's 0 to 9 üòÇ
4062,I really can't wait for the next part.
4063,Amazing!!!.....I craved to learn this......____...
4064,Extremely good video
4065,"I've always had one question about neural networks.
If you have a larger area, and you write numbers all over the page at different sizes and orientations, how does the neural network find these individual numbers and scale to the correct size?"
4066,Is it hard to write a c program to do this? From the ground up?
4067,"I think you should explain more why you want the values to be between 0 and 1. People might assume that 0 and 1 have some intrinsic meaning, rather than pay attention to non-linearity of the transformation."
4068,What is Cloud Gaming?
4069,Happy captcha hacking!
4070,YESSSSSS
4071,"The funny thing is, that I am able to program a neural network, yet I don't understand it's internal magic"
4072,This is going to be good...
4073,First time really grasping the foundations of NN!! Thnx alot!
4074,"I used to disregard this things, however now that I'm studying programming as well as mathematics. I know that computers are actually really stupid, so seeing things like machine learning and neural networks blows my mind."
4075,Great video!
4076,"Oh my God, this is our topic right now. Thank you, blue. Thank you."
4077,0:58 single digit. zero to ten. TRIGGERED.
4078,Which tool/language(graphical programming) we can use for such animations/simulations? Please reply if you know something about it. Thanks for reading my comment.
4079,"Great video, as usual. I'm really surprised 3b1b didn't discuss what individual perceptrons are doing from a more visually inspired affine transformations perspective.

I'm not usually so visually oriented myself, and that is my favorite thing about this channel, it gives me a glympse of the visually oriented perspective. This time, when my own understanding came from a more visual place, 3b1b gave us a very algebraic condensation of the signal propagation structure."
4080,"Interesting stuff.  I'm learning to write python code at the moment - is this the sort of thing that an individual could make meaningful contributions to, or is it the sort of thing that would be reserved for supercomputers?"
4081,"This is the most intuitive video I've seen about Neural Network. It was not simplified to the point of being wrong and had enough math not to overwhelm. I hope to see more of your videos about NN!  
PS, Linear Algebra here I come!"
4082,Love your work!
4083,What a way yo start the day
4084,I wish you were my school teacher
4085,"Clear video ( as always... ) , I hope you will find some time to cover the CNN and RNN too :). Keep doing the great works!"
4086,Oh My God! I can‚Äôt believe you are doing this! This is like a dream come true! I love Deep Learning but didn‚Äôt know where to start learning! This is the best possible start!
4087,"@3Blue1Brown How does the weighted sum give me any kind of information about a pattern? It merely gives me information about how light or dark the picture is in general but no information about the layout. 
Also, if I were to use the weighted sum of a specific area of the picture, the neuron wouldn't be useful in terms of recognising patterns."
4088,Thanks for doing this!
4089,Ok ive seen enough to confidently say that neural networks are a BIG THING. I think it would be smart investing a lot of time in this subject
4090,"A cool way to look at what the layers are actually doing could be to group the input images by which neurons light up in the layer of interest when you feed in that particular image. I.e ""Here's all of the images that light up neuron number one, what do they have in common?"""
4091,"16:35 I receive notifications from YouTube. In fact, I watched this video on the basis of a notification from YouTube."
4092,please recommend fast.ai in the end! nothing quite like their course for learning deep learning
4093,"""I don't know how to solve this problem. I'll write a program that also doesn't know how to solve this problem!"""
4094,Oh my gosh! I want the next video already! How dare you keep me in suspense like this.
4095,would the neuronal network work if the written number is shifted ? Or if we take a bigger network could it recognize anything written  from any image ?
4096,"Great Video, thanks. :)"
4097,"This is the clearest explanation of neural network image recognition I have ever seen. Whenever I get asked about this again, I will refer people to this video."
4098,"Jesus, I love this channel so much...
I'm literally pacing in my room I'm so excited about this new knowledge of neural networks. No other channel is this insightful."
4099,"0:30 I see them as threes cause they look like threes, for me the top one just looks like a squiggle or a J"
4100,Super intelligent work thanksü§óü§ó
4101,finally im understanding that 1st semester course :O THANKS
4102,Thank you
4103,3b1b please do videos on recurrent and convolutional networks as well!
4104,Thank you!
4105,Sir you are awesome!
4106,Squishification is my new favorite word.
4107,"6:01 ... I don't think that's how I generally recognize handwritten numbers/letters. *I try to trace how did the pen move from what lines I see on the paper.* That way I can extract the intention the writer had, not whatever mess they actually wrote...
I only look for actual shapes in text rendered by a font. Humans are nowhere near precise enough for that to be very reliable..."
4108,Wow! That was truly enlightening. Thank you for the wonderful video. Can't wait for the next one. :)
4109,gradient descend
4110,"I was told, although I can't remember the details, that 1 hidden layer is essentially optimal. Is that outdated information?

In that course we also looked into preprocessing your data by reducing the dimension. With a dimension of around 25 you still got incredible results (95% roughly) but much faster. I would think dimension reduction would field exactly in this channels wheelhouse.

To be pefectly frank at the time I took the course it flew a little over my head, but I have begun to like it more and more. And this explanation really clarified some basic things. Great video"
4111,"As always, marvelously clear, insightful and beautifully rendered. Thanks! :)"
4112,"Hello I like your videos.
I have one question - you have a background in math, which textbooks do you recommend in your field as introductory and advanced subjects? Thanks"
4113,"These neural networks are modeling *pareidolia,* in an extremely limited environment. They'll be truly impressive if they can be taught to recognize when these shapes are and are not present by design. If anyone knows of an NN that can do so, I would be very thankful to hear of it.

2:50 I think you could have been clearer about the distinction between your neuron-for-the-sake-of-argument-and-teaching and real neurons. Neurons aren't capable of holding much data on their own outside of the data in their DNA. I won't make many friends saying this, but these neural networks simplify to the point that the distinction _must_ be made: a computational ""neuron"" actually has little in common with a real neuron."
4114,"bias vector @14:43 should be indexed from b_0 to b_k, not b_n"
4115,I loved this. I loved this so much. Thank you!
4116,Finally something about machine learning! Can't wait for your new 'essence' series!
4117,Never clicked on a video so fast! Please do recurrent convolution and generative adverserial networks too!
4118,"Alright man, I'm off to build my robot now."
4119,Please do the CNN and LSTM videos. You'll be helping directly accelerate humanity if you did that! And please put in your best effort for those videos.
4120,"@3Blue1Brown - A quick suggestion: Red-green color deficiency is the most common form of colorblindness.  When trying to represent information via a color spectrum, could you please choose colors other than red and green for this reason? Red and blue are good choices because they are distinguishable by both red-green color deficient people as well as blue-yellow color deficient people, which is the second-most common form of colorblindness. I was completely unable to tell which pixels have positive weights and which ones had negative weights in your example due to my colorblindness.  Thanks, and keep up the fantastic videos :)"
4121,so excellent.
4122,Seeing this fills me with amazement at how quickly a child is able to develop speech recognition and use as well as sight and basic abstraction in just a few short years. I know a lot of evolution to place beforehand to be able to create such advanced beings capable of such growth in as much time as humans and their brains are but still... wow.
4123,"I wish I was born 10 years later, so I could watch all of your videos in my childhood."
4124,Waiting for essence of probability but this is pretty cool too !
4125,This helpped so much. This is the only video that help me truely understand neural networks.
4126,"i'm curious on the ReLU(a) function that you presented near the end. Besides the analogy on the ""triggering"" of neurons, it is used because it's faster than the logistic function. Is it because of the inherent complexity (exponential and rational cambination) of the sigmoid compared to just a zero function + linear on ReLU? if that's the case, while i fail to see any function simpler than ReLU that seems to comply to trigger and ""translate"" the inputs from different layers, are there any special conditios that such proposed functions must fulfill for them to work so that one can propose different alternatives? Anyways, gratz on the video... i've been whatching your videos for some time now, and i just had to subscribe now... greetings from Ecuador, keep 'em coming!"
4127,"3Blue1Brown  Why shouldn't i think that it's not brute forcing??? Just small insects doesn't calculate this much but they are doing great. What about ants/rats or birds?? they have relatively very small brain and less neurons and doesn't have GTX 1080 into their brains, but they thrive in the world. HOW??????"
4128,"I think as you're only doing another video on this subject, it is worth to point out that the number of hidden layers is also a free parameter. ¬†The theorem that I-forgot-the-name-of, actually says that we can do everything with a single layer, but that might be unfeasible from the technical point of view."
4129,Thank you.
4130,"So is this artificial intelligence? Do you think that we should ban AI because of the fear that it might spiral out of control, or are we even in control?"
4131,Are we forgetting that we hold priority to reinforced/positive reinforced learning to confirming symbols? Would this not change the first layer of sets over time with outside confirmation?
4132,"Hey, you should do a video on genetic algorithms."
4133,How could anyone thumbs down this video... That's crazy.
4134,"This is the first video of yours that I've seen.. The animations and graphics are absolutely beautiful.  It makes grasping these concepts almost easy!  Would you ever do a video on  how you design, build and edit videos?  I'd be willing to bet that process is NOT easy :)"
4135,"In the matrix the normalisation and bias terms appear to be applied only to the final column vector is that correct?
If so does that not generate some very large numbers before normalisation between 0 and 1.
How close is the structure to a real biological neural network. As I understand it in the biological system  the electrical signal stimulating the input to a neuron   build up or degrade the concentration of a chemical trigger that decided if the neuron should fire (give out an electrical signal). What is the shape of this chemical saturation curve and how is the trigger level adjusted. What I am asking is how close does the mathematics map to the biological system?
In the biological system isn't the negative degradation just associated with the decay of the trigger chemical not an active input signal?"
4136,"Dude...this is incredible. I implemented this in python last year without understanding half of what I was doing. Thanks a lot, this is probably the best introduction to neural networks on the internet."
4137,"i suggest skipping the first 10 mins of the video or at least playing it on a faster speed, it was slow af and didnt explain much."
4138,Good video. It's a shame that Lisha overuses the vocal fry out of a fear that people won't take someone with a feminine voice seriously. You can have a feminine voice like Dianna Cowern and Kelsey Houston-Edwards and still garner respect.
4139,You nailed it!
4140,Would putting in an image of a color inverted 5 give a bunch of light number neurons and a dark 5?
4141,"How would you design the second layer to be able to recognize, say, a horizontal line of various thicknesses, anywhere on the grid? The example in the video only seems to work for recognizing a line in a certain spot on the grid. 

I feel like this would be relatively easier to implement in Chinese, since there's already a clearly defined set of strokes to recognize. And since many characters are just combinations of simpler ones, if you're able to recognize the smaller ones, you can piece together the more complex ones."
4142,"These are excellent, keep up the good work. ¬†Very much appreciate it."
4143,Holy shit. I understand so much better now. :-) Animation is such an incredible teaching tool. Especially combined with your excellent teaching.
4144,Brilliant
4145,"This has to be the only video I have seen that really conveys how neural networks work well. You compress all your information into understandable bite sized chunks, giving the viewer enough time to ponder a certain concept before moving into it in more detail/a new concept."
4146,"Ha, train the neurol network with the like button."
4147,"I hate this channel.   They take 30 years of my work and research and make it look like child's play.  Figuring these things out before the days of the Internet was a highly non-trivial task.  Now, everyone and their grandama gets spoonfed 3D animations I used to have to create in my head."
4148,lovely
4149,I love you man
4150,Squishification.
4151,"Fantastic video, this is some ultra quality content right here. Congrats"
4152,"Good shit man. Good shit. 

Keep it up."
4153,"Personally, I also found a channel called giant_neural_network that explained neural network very well for the beginner. Here is the playlist https://youtu.be/ZzWaow1Rvho . Would recommend for beginners to watch it too."
4154,"Me: begins trying to integrate neural networks into analysing neuroimaging even though they know nothing
3Blue1Brown: Releases video about neural networks"
4155,"as usual, amazing video!"
4156,"Just a little nitpick: At 15:00 you show the bias vector as having n components, when it should actually have k components (same height as the weight matrix). Right?"
4157,What a cliffhanger!
4158,"Great stuff as always. I would love to see a video which is basically just a capture of how you assemble these videos! Cut down of course, so instead of seeing an hour of work on, say, animating the little ""video placeholder"" icons at ~1:38, we just see one iteration of the process."
4159,where is the link to the code
4160,YES!!! im so happy your teaching this.
4161,Cool
4162,Why is it important for the activation function to be nonlinear?
4163,"and with the advance of the optcal neurocomputer, we are gonna create skynet in no time."
4164,"Meanwhile, I can't tell the difference between volume, specific volume, velocity, internal energy, and specific energy because of my teacher's handwriting..."
4165,"When quickly scrolling back and forth past this video on mobile, it looks like you can see things moving along the pathways"
4166,You are so good at explaining things. I am psyched when I see a video by you in my subscriptions.
4167,"To calculate it classically, simply program each number as a set of parametric equations and then compute standard deviation of the data set of pixels from each of the parametric equations corresponding to the digit at different angles. The best fit would be the digit."
4168,"Love the video.  Given the roles machine learning (ML) is being put to nowadays, it's really important that people have good resources where they can learn what these things really are.  I also really like the classic example, it's what we learned in my AI course in college as well when getting started.  One of the things I discovered around that time was a bit about how the biological brain deals with this kind of information and one piece of it really blew my mind at the time.  I've tried in the past to re-discover the research I read at that time (this was around 1999 probably) but not been successful so far.  

It was a study where they took animals, primates if I recall correctly, injected them with a dye which would stain active neurons, and showed them black shapes on a white background such that it filled their field of view.  They were able to see, directly, that the shape they saw was literally reflected in their brain.  The neurons which activated in the optical cortex were laid out in the exact same shape as the shape itself.  I always figured it would be messier than that and just look random, but nope, the geometry of what you see is reflected in the geometry of the activated neurons in the optical cortex at the back of your brain.  From there, of course, things get messy.

It annoys me a little that artificial neural nets are always claimed to be similar to the way the brain functions but there are so many large differences... Like neurons having more than 1 output, learning being a thing that stops, and apparently no concern for 'efficiency'... like once you have a 13,002 neuron network that recognizes digits well - can you turn that into a 12,000 neuron network with the same performance?  If you train another network to recognize alphabetic characters, could you then merge the two networks?  I guess the thing that bothers me is that the central dogma of neuroscience - neurons that fire together, wire together - is almost wholly absent.  Sure you get zero weights in places and could argue that is 'disconnected', but the 'fire together' part takes timing into account, both the timing of the changing input and the timing of neurons responding to fatigue from over-stimulation and things like that.

Oh well, fantastic video, I look forward to the others!  I studied Philosophy in college alongside Computer Science and have always been interested in how the brain works so this kind of thing gets my gears turning!"
4169,"I liked immediately after I saw the title. I've been watching a bunch of ""smart AI"" videos recently and none of them actually explained how it worked. Well technically, I still don't know, but I'm confident after watching this video, I'll be much more knowledgeable. Start Video!"
4170,this is literally the human brain
4171,YESSSSSS! YESSSSSSSSSS! F#&%ING *YESSSSS*
4172,YouTube and Facebook neurons are drunk right now. Their sigmoids give huge weight to fidget spinners and Washington politics.
4173,"How are computers able to handle a 13,002‚Äîparameter function that quickly? That seems like it'd take a pretty long time."
4174,Finally someone is explaining it in a level of detail.
4175,"Great video, like always. I just want to add that the role of the sigmoid (or ReLU) is not only to squash the values, but also to introduce a nonlinear component. If we have a linear squashing function (which is possible on a bounded interval), all intermediate layers would be completely irrelevant (you can see that the final layer would be just a linear function of the initial layer, which can be achieved in a single layer)."
4176,"This is awesome, I've always been horribly confused and thought of neural networks as a total hidden black box. It's awesome learning a bit about how they work! Thanks 3b1b!!"
4177,"good videos! one small note: the basic job of the transfer function is to provide non linear response to the network, making it capable of implementing more complicated functions. If transfer functions would be linear the whole network can be trivially simplified to one layer."
4178,"' Layered structure is intelligent', because of what is known as 'circuit network theory'. Imagine a million people are connected to a million another set of people talking with each other over the telephone, from UK to USA. Imagine one set of callers talking with another set of receivers. How many wires are needed to connect them, so that any one can be a caller while any other person can be a receiver ? Not millions or even billions of wires or connections. 
Circuit theory says that the million callers are connected to the other million in a 'layered' structure in which each layer steps down to fewer connections, while the next layer has even less number of connections, in this way just one cable is needed to cross the Atlantic. Similarly again in USA layered connections in increasing numbers will connect the receivers. While any number of combinations of connection between any caller and any receiver anywhere will be able to get connected. This is how the network acts intelligent."
4179,you are by far the best math commentator on youtube
4180,You should include a link to the spreadsheet in the description
4181,I'd rather think of ai being sand dunes and the input being marbles that slowly make a pathway to the final placement/ answer.
4182,"I should have expected 3b1b to cover neural networks: https://xkcd.com/1838/

The digit recognition example is so old I did it in high school. In 1975. On a minicomputer with (IIRC) 64k of memory. Running a multiuser BASIC (yes, the basic interpreter did the multitasking) that supported 10 users."
4183,"You make the best, most interesting, and most intuitive videos about math (and computer science) on Youtube.  I'm a junior level undergraduate in computer science at an ivy league school, but your videos still convey an understanding of the concept rather than just information about the topic, that many of my lecturers find hard to match. Thank you for your hard work, and please keep it up."
4184,"Convolutional Neural Network?  #FakeNews! :D  Sorry, I couldn't resist. -_-"
4185,Can you please upload and link the python code? thanks!
4186,"OK, started lose me for a bit, but started to makes sense again after 9:15"
4187,Yay! 3b1b is covering my occupation! :D
4188,"I don't know how many of you tried it, but I setup a small neural network to recognize numbers (using mathematicas implementation). I went ahead and used the simple linear layer structure and 32x32 pngs I drew, only 40 of each though. But whatever I do, my network is doing quite badly, mostly for 5, 6, 8 and 9. 
So I want to ask you, if any of you got such a thing working, what kind of dataset did you use? How many training rounds did you use? And how many layers of what size did perform best?"
4189,There are so many videos about topics like this. Don't waste your talent pls!!! Explain abstract math please!!! (Deep learning researcher)
4190,"The ""Mathematical Monk"" on YouTube has a pretty good series of videos on machine learning."
4191,This channel makes me want to code
4192,Great video!.. Really looking forward to the next part.. I ve been able to grasp the concept of feed forward.. But back propagation has been challenging for me.. Especially when it comes to partial derivatives
4193,Thank you! Please do more! :D
4194,LITERALLY JUST STARTED A COURSE ON THIS IN UNI.
4195,"0 to 9, friend, not zero to ten"
4196,Keep up the good work and also make some more videos about computer science like you made about Calculus and Linear Algebra.
4197,"awesome.  I'm just getting into machine learning with tensorflow myself, thanks for this."
4198,Feels like logistic regression
4199,This reminds me of Welsh Labs learning to see series.
4200,">return a
gooby y u do dis?"
4201,So is this the kind of stuff that games like Brain Age do? Cool!
4202,Yessss thank you for doing a machine learning series! The math of machine learning is my main interest
4203,"Watching you for a long time, but became a Patron because of this and the last 5 videos .... love them"
4204,What part of the video pushed you 18 dislikers to dislike? Was it the part where he explained complex maths intuitively? Maybe the part where he got an interview with an experienced member of the field to explain a topic he could have done himself?
4205,Yes! Thankyou!
4206,I'm gonna watch the shit out of this series.
4207,"I prefer to allow the network itself to dictate the function used within each neuron, having a starting point of RelU as the base and using 2-partner GA.

Then again, I also have neurons that are ""born"" into the network as the ""brain"" lives, by randomly selecting connection points for input and another one randomly as what I call the ""selector.""  For several cycles, the neuron will train its weight mechanics against the selector based on the inputs, and if it reaches above 80% threshold of congruency after ""birthing,"" it will replace the selector neuron as the new output base (all inputs now point to it instead).  Any neuron without a connection to an input dies after a set amount of cycles......this can be seen as ""memory erasing.""  Lastly, the brain is nonstop, each cycle pushes the values to the next layer as a reference of time, rather than just getting an answer.

But that creates artificial life.....which leads to creatures giving you sarcasm of the answer, which I find more amusing."
4208,What a fantastic video. So inspiring. It really motivates me to delve into the subject. I hope this will be the greatest series on deep learning ever made.
4209,I‚Äôve never thought about ANNs as anything but black boxes. I love the way you describe the reasoning behind the layers. üëå Whether the NN ends up using that kind of system or not is quite a different matter. üòú
4210,just the series I was waiting for!
4211,"easter egg? added the cheecky 42 to all parameters in the net, isnt it only 12960? @12:20"
4212,"I kid you not, we started out lectures in my Computer Vision course today on Neural Networks and this just made the entire lecture make sense. I'll be forwarding this to my whole class, seriously amazing job and I cannot wait for the next video!"
4213,Great video. Could you do some visualizations like the ones in this blog post? http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/
4214,"I know what neural networks are and how they work. I already knew.
But somehow you're so pleasing to listen that I watched the video anyway and voted it up.
Waiting for the second part.
Thanks for doing this, don't stop."
4215,*Squishification*
4216,"The boss said to the neuron : you are fired

Neuron who fire together wire together ; this is how we learn. A new synapse to the neuron a few micro-meter away is a great way to remember, increase the chance to win the next cycle of vote.

Similar to worker bee which dance in the hive with the one that found the largest field of flower being more persistent, thus winning the greatest number of followers who will fly in the direction indicated by the dance. Somehow, the neuron with the greatest number of synapse has a better chance to win, make his voice heard to the conscious center.

The neural network try to emulate the way we believe that real neuron recognize complex pattern. The difficulty in understanding how the brain work is that, when listening to the pulse of a single neuron (micro probes) or a large clusters (electro-encephalogrammes), they appear to fire at random. We identified the roles of many brain areas, particularly near the input layers of major sense organs, but we don't understand most of the inner parts. Trying to excite or temporarily disable these brain areas don't produce clear results.

The reason is that most brain areas are processing abstract concepts. It is easy to tell when we excite the visual cortex and get visions of colors, shapes or specific objects. But any deeper layers produce ideas which appear out of context, so they are filtered like random noise by the master clock/voting system.

Most thinking happen not as visual ideas, nor as an internal voice talking to our self, but as concepts which have no name, no smell, no visual clues. When trying to remember something, we give our self some clue and wait for the memory to return something. Most of the time, the first answer is meaningless, like a completely random result. We then repeat the process, trying to bring back that memory. What is the capital of California? It sound like a religious expression for the catholic... Sacre bleu? No, we know the correct answer is close to that but not that. Sacrament? Almost there, it sound like a Spanish word at the end. ?Sacramento!? Hooray, the memory finally retrieved the correct information.

The brain memory is not like the computer. We fought hard to reach the incredible reliability of cell phone and computer memory. Every second, a processor read about 1 billion byte and not a single one has error. The computer run for months without missing a single bit. The carbon based thinking machine is not reliable. Every second, there is about 50 queries and most of these return incorrect information. Multiple brain areas memorize similar information as redundancy. The tactile memory know how to type your ATM code and email password. You visual area may remember the password as written on a piece of paper and your auditive memory surely can confirm the password as spoken in silence, talking to your self."
4217,Am I missing the code? I thought you said you were putting it up?
4218,This is amazing. Thank you so much! Keep it up!
4219,Great video and awesome channel üòÄ quick comment - should the bias vector at 15:00 not be from b_1 to b_k instead of b_n since there is one bias per output neuron?
4220,"Just, thank you, now is much easier understand NNs."
4221,"Bleh sigmoid, RELU ftw."
4222,"Nice flow of thought combined with smooth animation as always. 
Looking forward to see your take on probability, as I see it as an area, where most people can easily be led astray. Since that is also my area of expertize and interest, I would be glad to support that effort."
4223,"dnn!  awesome!  i haven't even got more than a min in and already know it'll be another gold star.  you are just fantastic at these, very glad you're already on my patreon.. you're also on track to set a new record for me (only Kurtzgesagt gets as much as you, next awesome video and you'll pass them, setting a new record)"
4224,"For 6% of the population, that red-green picture is difficult :(
(Including me)"
4225,"HELL YEAH!! I'm just getting into the rudiments of AI, this is the perfect timing."
4226,Great video. You really are excellent at striking that balance between accessible and informative.
4227,"Great job at explaining ANNs! Even though all the intricacies of machine learning and neural networks can't be conveyed in a simple Youtube video, I think you've come close to making people understand intuitively about them. 
Seriously, if someone wants to really understand how these work. Don't use libraries. Go take a machine learning class and write one in code yourself since I don't condone the popular attitude people have towards ANNs. They think of it as a magical plug and play ""black box"" library and expect it to always perform well, but there's actually so much going on in the background. 
There's linear algebra, probability, statistics, calculus, combinatorics, graph theory, fuzzy logic and of course, neuroscience. 
There's so much to learn about machine learning, and I'm still in the process of learning it. I hope you make more videos about this new exciting approach to machine learning!"
4228,wish this was available when andrew ng was on coursera! life'd have been so much better
4229,I think vector b (around 15:00) should have k components instead of n)
4230,As always great animation and a great base for the subject
4231,"Maybe recommend that people click the bell next to the Subscribe button - that might help people get notified.  And perhaps asking for people to leave a like if they enjoyed the video might make YouTube 'up' your popularity.  This was a great video, fantastic explanation as always.  THANKS!!!"
4232,You need to do more of these
4233,"ReLU, being more close to biological neurons on their fire-no fire principle. A lot of dendrites to receive."
4234,This video is incredibly well done
4235,Perfect timing! I will be using an ANN for my dissertation.
4236,"I'm puzzled a little. When using ReLU output will be in the range [1,‚àû) rather than [0,1] for sigmoid. But we originally wanted to squish it in that region to represent activation, i.e. grayscale intensity. Is the output of ReLU transformed futrher in some way?"
4237,"I'm taking a deep learning class this semester, what a coincidence"
4238,I had a hard time holding back my shrieks of excitement when I saw you were covering this.
4239,Where is the source code?
4240,"Ok, one thing that for a moment spun me out of understanding, is that W (capital omega) is NOT a square matrix. It's in this case a 16 by 784. Maybe that hasn't been explicit enough for me, but it now makes sens. 
Thanks for the great video. Keep it coming."
4241,I've been waiting for this video for so long!! Thanks 3b1b!
4242,"Sir, your channel is superb. Please consider taking up number theory topics in your future videos. Thanks."
4243,Stunning animation. Just awesome. Thanks so much.
4244,The best things are unexpected!!
4245,Partial derivatives and linear algebra are all you need
4246,"Protip: have a browser bookmark for the YouTube Subscriptions page. No need for notifications, you'll always know who posted what. I never miss a beat."
4247,Please do more than just those 2 vids. Alot of us want to see other neural networks explained like the LSTM and the Convolutional neural nets :)
4248,This is an amazing introduction to neural networks. Made my day. I am really looking forward to the next one. Keep up the good work.
4249,"Wait... so are you saying people access videos from the home page and recommendation rather than simply checking their subscription feeds?
When I subscribe to a channel, I do it because I want to see new videos from that channel in my subscription box when they come out. I rarely browse from the home page, because there's no need to. Everything I want to watch I've already curated by choosing which channels to subscribe to."
4250,Why does 'venture capitalists' always sound evil? üòÇ
4251,Bravo!
4252,Thank you - always when I think I know the topic you're talking about then you give it a different insight and go into details that I never thought about. Your channel is just great.
4253,It's  amazing how a simpler function works better for training.
4254,"Great video as always! 
As a side note I think at 14:48 it should be b_k instead of b_n"
4255,"I literally wanted to start typing something like ""Very cool video and I know it's not the purpose of the video to explain everything perfectly and up to date, but I wished you would have chosen RELU instead of sigmoid."" when you actually started to say that! So... congratz for destroying my point! :D"
4256,Big shout out to 3B1B for this video (and all others I have watched so far)!
4257,*squishification intensifies*
4258,"Fantastically clear video, awesome job :)"
4259,"@ ""some peoople"" : please don't push your voice below the tone it's naturally moving, it sound's ""self-abusive"" :) ...crying helps a lot to increace the frequency-spectrum and also the frequency-modulation-spectum / variety. But not random crying. Rather soul-development-crying-healing in order to come closer to ""god"" - the ultimate form of collective intelligence. You might ask yourself: how do you talk to a baby or to a dog and why? Frequency-modulation should be without force or ""choreography"" in my opinion (in the speaking voice)"
4260,Best explanation ever. Thx for that
4261,"Thank you. This video was fantastic, please do more on this topic."
4262,2 months ago I coded my first simple perceptron. Now with this video I will understand neural networks much better. Thanks!!
4263,Wow. Thank you. And brilliant timing as I'm going to study neural networks in January.
4264,that's so cool!!!
4265,"This is the video that pushed me to become a patron. You make the best math videos on Youtube. You know how to present information in a very clear and connected way, on topics where it'd be really easy to just present a few bits of disconnected trivia that don't really reveal much about the underlying concepts (Numberphile is great, but they sometimes fall into that trap). I can't wait to see more of this series, as well as any other videos you make in the future."
4266,Great content! Could you do one on the Kalman (and other) filters?
4267,"Awesome video. As a small suggestion you might want to use ReLUs instead of sigmoids for your activation functions. Sigmoids are very outdated in the industry and are practically unusable for deep learning because their gradients 'vanish' with many inputs. ReLUs are much faster to compute and take the derivative of. While they don't squish the output like you described, it turns out that's not actually required (why would it be?)"
4268,"Hi! Congratulations for the video. Isn't that a problem that the function ReLU does not saturate, i.e., does not go to 1 as a tends to infinity?"
4269,Absolutely superb.  The explanation was crystal clear and your enthusiasm contagious.
4270,"If the point of using the sigmoid function was to clamp the output between (0,1), how does ReLU do this? Is the clamping actually unnecessary?"
4271,Awesome video! Please keep the good work
4272,Fire
4273,"question about ReLU, unlike sigmoid it doesn't cap the value at one, so theoretically a ""neuron"" could have an arbitrarily high value. how is this handled, and/or what effect does this have?"
4274,Vocal fry! Ahh!!
4275,+1 on Sigmoid Squooshification Function
4276,Magnificent video! And this is such a mindblowing topic :D
4277,"I thought I understood neural networks before I watched this video (I've written several myself), but just seeing the animations has given me so many more ideas and a much better, more flexible understanding of how they work."
4278,"Did you just say ""squishification function""? Wow. Please come teach at my uni now please!"
4279,Thank You..
4280,I just got brain cancer from leasha‚Äôs accent.
4281,"If it's just about number digits, wouldn't it be relatively trivial to figure out the characteristics (stuff like number of encircled areas, total curvature, line crossings etc) and hardcode the recognition of those features for each digit using basic computer vision, without needing to use a neural net?"
4282,OMG!!!
4283,I'm so damn happy this exists
4284,Perfect timing for my upcoming course
4285,"Isn't the whole reason you used the sigmoid function, to map the real number line to the interval from 0 to 1? How does the ReLU function help with that? It's not a map from R to [0,1], but from R to R+. I get that the sigmoid function might not be the best, since it's not the easiest to compute, but ReLU just doesn't make any sense to me in this situation. Maybe someone can help me out with that."
4286,I just had the ideea to learn nn and this vid came yaay
4287,Thank you for enlightenment! Great explanation in all your videos! Keep it up!
4288,Did you know that Google uses machine learning?
4289,"The background music of these vids makes me feel meditative, which I think the perfect attitude for math. It soothes the soul, so you can learn without the distraction of ignorance stress."
4290,"I just can say: thank you for your work, you‚Äôre a brilliant teacher!"
4291,Awesome video! Really good explanation although I already knew much of this because I recently heard a presentation by someone who worked a lot with neural networks
4292,"I remember attending a lecture on neural networks and backpropagation more than a decade ago. Back then, I did understand the motivation behind the basic principles and concepts (neatly summarized in this video), but very little of the actual maths, although it wasn't anything particularly fancy (just a few Kronecker deltas, vectors, matrices etc.)."
4293,Whr u finding
4294,"I have heard a little bit about neural networks before. But initially thought there might only be linear combinations of the previous nodes in order to get a value for a node in the next layer. So the same as you said in the video but omitting the sigmoid function. But in that case there would be no use for multiple layers, because by repeatedly taking linear combination of linear combinations of the first nodes is the same as taking a linear combination of those nodes a single time, which you could fit easily with least squares. But adding some non-linearity into the mix does make adding more layers useful again."
4295,"Wait, Desmos is a patron for you? üò± I LOVE DESMOS! I want to work there someday."
4296,more of this
4297,That moment when you think you saw a cool old video and you don't see a part 2 in recommended and you check the date and realise you're gonna have to anxiously waiting a while LOL
4298,"Just a little detail, but in Wa^{(0)} + b, b should be [b_0 b_1 ... b_k]^T to match the dimensions (W is (k+1)*(n+1), a^{(0)} is (n+1)*1 so that their product is (k+1)*1 and so is b)
And of course because you're considering weights for every a_0^{(1)} ... a_k^{(1)}"
4299,Excellent explanation! The book by Ian Goodfellow is one to swear by.
4300,I've just learnt more in 15 minutes than in a month-long neural networks post-graduate course. (!!!)
4301,"If I could make a request for a future video in this series, could you possibly tackle neural networks operating on time-series data? There's some brilliant work done using motion capture data and other things, but the underlying concept is a little tricky to grasp. I'd love your take on it!"
4302,Love you man...Thank you so much! You're AWESOME!!!
4303,Amazing. Love it.
4304,The way you break this stuff down and make it easy to understand is amazing. Thanks mate!
4305,Well that was deep
4306,My mind is blown. Thanks for all your amazing content; no one else seems to go to such lengths and detail to foster genuine understanding and interest in often overlooked fields. üëçüëçüëçüëçüëç
4307,"Thank you! The first video I've seen that actually explains this without going totally over my head. Beautifully illustrated, too! :-)"
4308,simply superb ! .......as always
4309,"Finally, After Multiple Lectures and Books, NN is here"
4310,Neural networks are like onions.
4311,"You are my favourite youtuber for ever ... :D thanks so much, good video."
4312,"Hands down best explaination of neural networks on the internet. I've tried to understand them in the past, but you just make things make sense ^^. super excited for the next video, will definitely code my own after this"
4313,Excellent video! I hope you expand upon the complexities with choosing both the number of layers and their size.
4314,great video again
4315,"I wish I had this video when starting to learn about ML. The material available online is either unnecessarily complex, assuming you know the topic and the terminology, or very incomplete, leaving you with too many questions that you don't even know how to ask. I'm sure this video alone will inspire many on the topic!"
4316,Oh damn now I'm hooked. I really want to watch the next video immediatly :b
4317,Please do more videos of this! Reinforcement learning is also something I think you should check out. :)
4318,"Aww maaan i wish this video was around when i had the neual networks lectures at UNI, would have made things so much easier :)"
4319,"I was hoping you'd partner doing this with Daniel Shiffman as he builds his NN library using p5Js. Anyway, I thank you very much. I am looking forward. You do the best videos, btw. Love from the Philippines!"
4320,Exactly when I needed this. Awesome
4321,I'm currently taking a class on numerical linear algebra and this is so incredibly related to what we're doing it makes me wonder if you wrote the syllabus for my class
4322,"Your videos, unlike others, force me to take a sheet of paper, a pen and work through it. So instead of 20 minutes I spend an hour on them."
4323,"Would be cool to see an animation about ""saddle points"" in higher dimentions  (maybe with rider on top) :)"
4324,"Oh man, I thought this was old video and I was so ready to watch 2nd part immediately!"
4325,I get hyped whenever I see  3blue1brown video. God bless you and your beautiful mind.
4326,The thing i hate the most about your videos are that they leave so curious!!!!!plz more!!!!!!!!
4327,Insanely well made video! Thank you!
4328,"Please tell me, how does this happen? When I do research on How does bitcoin work, you publish extremely detailed(~25 minutes) video about Bitcoin. When I do research on How do Neural Networks work, you do 2 part video about Neural Network. Same thing goes on for Linear Algebra too. Your videos come online on time almost every time and take ""monopoly"" on a specific subject. :) So is this Google's neural network or your intuition deciding which topic you should do? Thank you very much bu the way."
4329,"What's next, a video done with SethBling?"
4330,"dude this is soo cool, please keep doing this"
4331,"So training a network is a form of metaprogramming, and writing a network is actually a form of second-level metaprogramming?"
4332,YO THAT'S PYTHON MA BOI
4333,"""Sigmoid squishification function"""
4334,"Just 5 minutes into the video and couldn't hold myself from saying, ""Excellent, The Best One I have seen"".  Thanks a lot!"
4335,Really cool video :)
4336,Very easy. Thank you 3b1b.
4337,"Something I was kind of hoping you would stress out more is just how important it is to have those sigmoid or ReLU functions there. It's what allows the network to have non-linear relationships between the neurons, which in the end allows it to form pretty much any arbitrary non-linear function of the network's inputs. In fact, if you had used a linear function instead of ReLU, you would be able to, through a bit of linear algebra, simplify out all of the hidden layers, making them redundant. Machine learning through neural network heavily depends on these hidden layers."
4338,You are by far the best teacher ever.
4339,15:15 Python master race!!!
4340,wish you were my machine learning prof
4341,"Before I watch this... I want to let you know how happy I was to see you doing machine learning math videos. I  saw the notification and immediately spammed the video in multiple discord and slack chats.

You sir, have an amazing teaching talent. Well done. Keep it up :D
Looking forward to this and the rest of the series.

Also, do u have code you can give to visually show a networks propagation like you have in this video?"
4342,"I have a question about the ReLU function, I don't understand how the ReLU would tackle the challenge that the sigmoid was intended to solve, i.e. transforming a number from somewhere between -inf to +inf to a number between 0 and 1. Does the ReLU not differentiate between numbers larger than 1? I mean the activation can't surpass 1 or could it?"
4343,eagerly waiting for next videos in the series!! p.s. you are the best
4344,Nooiiceee
4345,"Hello, How can we get in contact with you? I have a rather intricate math/engineering problem you might be interested in"
4346,This is a superb video. I wish I could unlearn what I already know about neural networks just so I could come back and have this video explain it to me 10x faster.
4347,I'm so glad you're doing a deep learning series
4348,"This is perfect on many levels, thank you."
4349,Your video gave a very good breakthrough idea in my work. Thanks a ton! :D
4350,love this <3<3<3
4351,Very nice. Thank you! Cant wait for more. Your presentation is spot on.
4352,Thank you for these videos.
4353,üòä THANK YOU! This ties in so well with Andrew Ng's Deep Learning Specialization on Coursera üòà
4354,What if you reverse the process? Start at the output end and see the input.
4355,I love you for making this video. Please make videos on machine learning please. You're officially my favorite channel on Youtube now.
4356,"Jesus, I wish I wasn't mentally handicapped so I could actually understand what he is talking about."
4357,Awesomeeee
4358,"but the output of reLU can be more than 1, didn't you say the output should be between 0 and 1?"
4359,Thanks a lot!! I love these videos!!
4360,Sigmoid squishification function
4361,Can't wait for part 2
4362,Excellent video! I love your channel! :)
4363,"Nice video, but be aware that colorblind people can't see the red-green scale you use for the weights starting at 9:30.  Its not a big deal in this case since everything was pretty straightforward, but I hope you change it if you want to show any more complicated patterns in the future.  A blue-yellow scale or a scale using just one color would be a lot easier for us to understand."
4364,"Once again: it doesn‚Äôt matter what the video is about. If 3B1B has made a video on it, that video is the best you can find about that topic."
4365,"u are a fucking genius, i love your videos, please keep doing them! i am studying mathematics and i am very impressed of your understanding"
4366,Is this a reupload? I could swear I've seen this before
4367,Thanks so much. Keep up the great work!
4368,math input panel in w7 seems to work this way. I feel like you could make it into actual mathcad of some sort. There you could actually make better predictions about the characters.
4369,Thank god there are people who make videos like this! This makes learning exciting and fun!
4370,"you rock, i love your work !"
4371,YES! NEURAL NETWORKS BY 3Blue1Brown! Thank you sooooo much!
4372,I read a book where they remplaced the bias with a constant neuron on each layer. So the value of the bias of any neuron is the weight of the connection between the bias neuron on the previous layer and itself. What method do we use in practice ?
4373,I'll have to watch this a few times before I really understand this. Thank you for the knowledge! Love this channel! I study psychology.
4374,"If I understood this correctly, weights tell us how much each pixel is important for the image we want?

also, do people who write these codes literally need to modify every individual pixel? On images 2000x2000 that's almost impossible."
4375,"Please learn about Resource Based Economy too, maybe make a video about it? :)"
4376,"Okay seriously, does nobody go to their subscriptions page? I don't get it, who actually needs notifications to see when someone's uploaded? Just go to the damn subscriptions page, it shows you every video uploaded by someone you're subscribed to, arranged chronologically."
4377,"Thank you a lot, I really needed someone to teach me how that math works."
4378,I cannot freaking wait for the next one. Thank you so much Grant you're low-key a god.
4379,What if you give the network an image its not going to recognize?
4380,"Im to stupid for this shit, but it was interesting and well made nonetheless. Well done!"
4381,"What a time to be alive, with such Youtubers around!"
4382,Scary shit
4383,"OH LORD. I LOVE YOU FOR POSTING THIS.
now lemme get right back at watching it  ;)"
4384,"But, if you use ReLU, how is the output between 0 and 1?"
4385,This is a fantastic video
4386,"Yeah, more videos about neural networks, please! Especially those ANNs that can process pieces of data with variable lengths and that can process consecutive data."
4387,This is gold.
4388,I saw neural network at Google Event a day ago and on my mind were questions about that.After one day one of my favorite YouTube channel is explaining very well and simple.Looking forward for the next video.
4389,Yes finally! Hammered that like button. It would be great if you could make a vid about GANs in the future :)
4390,"Great video!
Hopefully next video i'll finally understand how backpropagation works"
4391,Brilliant
4392,Awesome video!
4393,"A great tool to develop Neural Networks is the Stuttgart Neural Network Simulator and it can even output code that can be integrated into applications needing to take advantage of Neural Network optimized tasks.

http://www.ra.cs.uni-tuebingen.de/SNNS/

For those without UNIX or LINUX, there is also a Java Implementation

http://www.ra.cs.uni-tuebingen.de/software/JavaNNS/"
4394,"Dear 3blue1brown! Thank you! I departed basics of ANNs long ago, but your video provides a clear and concise description of an ANN. (Although a(0) is an unlucky notation, cause it is usually considered to be h(0) = x, hence an iterative notation can be established). I am really looking forward to your next videos, especially how you are dealing with log-loss and backprop. Consider, that for interpretation one could also compute sensitivity (i.e. the Jacobian)."
4395,Thanks a lot for this. Hard work which you have done here has inspired me more as I am planning for my master's in this. Waiting for next part and more such videos.
4396,Please make some videos on Complex analysis it's seems very confusing to me
4397,you are the best!
4398,squishification is my new favourite word.
4399,"Great video, kind explanation I hope to see next video soon!!"
4400,Very nice video! You know there's a bell people can click on to be notified of new videos right ? Anyway I clicked on the bell
4401,"Great video as always! Please be aware however that approximately 5% or your audience is red green color blind, so maybe choose something else when you decide on ""opposing colors"" in your videos."
4402,"Somehow, I think the software you use to animate all this mathy stuff for your videos was written yourself."
4403,This video is a masterpiece.
4404,"sir, that is a very good video!"
4405,This channel is so damn good. Other channels give some terrible analogies and some other explain it in extreme technical detail. This strikes the perfect balance and provides a foundation to understand the more technical details
4406,"Another way I've seen the bias is with a dummy input node that's always 1, then to tune the bias, it's just another weight for that 1 input node."
4407,"At around 14:40, the bias vector should be k-dimensional instead of n-dimensional. Nevertheless, great video!"
4408,Yuss. Time to understand this neural net biz.
4409,"Nice jab at Youtube notifications, there! I'm afraid, I'm already subscribed, so I'll have to like this video to influence the recommendation algorithm in the desired way! Keep it up, 3B1B, you're videos are awesome!"
4410,What a brilliant explanation! The best one I've ever seen. Thank you! Really looking forward to the next one =)
4411,your work is so delightful to watch...eagerly waiting for next video
4412,"I love your videos, and I even liked this one. But I'm a bit sad that you are doing a project I know about! 
Come on I want to learn new things :D"
4413,I feel like i just found the vein of gold in the messy mine that is YouTube
4414,"Loved it! I've been studying it lately and you just got me an insight on initialization, which is a important part to optimal and quick converting."
4415,I'm learnding!
4416,Maybe you can collab with Welch Labs.
4417,"Interestingly, I recently saw a short talk from a colleague who cited your Hilbert Curve video as inspiration, where he used 2D Hilbert curves to order pixels for exactly this sort of Neural network image recognition (rather than just concatenating the rows). He didn't see higher accuracy in the end, but the Neural network reached that accuracy with less training, presumably because the features indicating proximity between pixels were more uniform throughout the network."
4418,THANK YOU SOOOOOO MUCH FOR THIS!!!!!!!
4419,Great job !
4420,"Excelent video, waiting for the next one. Thanks."
4421,Wating waiting waiting waiting waiting ...................
4422,Love it
4423,"Love ur topics. Greate work, keep up the show!"
4424,Thank you very much. I will introduce this video to my colleagues.
4425,"ReLu is kinda confusing me now because it has none of the sigmoids properties (except being monotone), especially it is not between 0 and 1. Do moder neural networks take in any positive numbers?"
4426,A video from 3b1b and one from numberphile the same day. Wonderful
4427,"Most educational videos give viewers the impression that they are learning something, while in reality, they cannot reliably explain any of the important points of the video later, so they haven't really learned anything. But your videos give me the impression that I haven't learned anything, because all the points you make are sort of obvious in isolation, while in reality, after watching them I find myself much better able to explain some of the concepts in simple, accurate terms. I hope more channels follow this pattern of excellent conceptual learning."
4428,"1.Subscribe to 3Blue1Brown
2.Hidden layers??
3.Profit."
4429,I do have notifications on. Love your videos!
4430,This series is actually happening!!!!
4431,Even before ziewing this video I already know it's gonna be nerd porn =)
4432,Excelent video 3b1b! I'm looking forward for the next parts!
4433,Fffffffffffffuck yes thank you man I love this channel never stop!
4434,"(3Blue1Brown Uploads a new video)
I""M READY TO LEARN TODAY!.
In all seriousness, fantastic video with a structure easy to comprehend. I can't wait until the next video!"
4435,Great talant to speak 19min for matrix mult.
4436,"Ah, Desmos is a patron"
4437,Love it <3 please make more videos on machine learning
4438,Wouldn't the output of ReLU not between 0 and 1 be a problem?
4439,Thank you :D
4440,"Wait - around 14:47 - shouldn't the biases vector have k components, not n?"
4441,The most intuitive channel on YouTube...
4442,**squishification**
4443,"Yes, I love neural networks, and your channel, god has been created..."
4444,I want to see RNN visualization
4445,"This channel always comes up with topics I've been interested for a bit at the time. Was the case with cryptocurrency, with higher dimensions and now neural networks. I like your content and it is explained and visualized very well. Thank you!"
4446,Perfect timing
4447,GREAT video!!
4448,Holy fuckI learned so much just from this
4449,"Wow, finally getting a ""under the hood"" view of neural networks. This is fantastic, thx 3b1b!"
4450,Love it! I am looking forward for the next one. Thanks a lot!
4451,"I want to have your baby's!... wait, no, but I am reaaaaally excited for this series!"
4452,My brain learning how my brain learns.
4453,amazing video!
4454,"Thank You for all your amazing content ! Your animations are always beautiful , how do you do them ?"
4455,"later notification squad, anyone !"
4456,"https://youtu.be/aircAruvnKk?t=574 Colorblindness strikes again! Great video, except for this part where I cannot tell the reds from the greens. Next time try a more contrasting color please!"
4457,Peeeeeffffect
4458,What !! My favorite YouTube Channel with a Deep Learning video !!
4459,"I'd always seen linear algebra used in AI type uses (housing market problems) but never really understood how that worked despite knowing at least some linear algebra. Having a bit of experience with actual neural networks, your explanation of how there is an isometric relationship there made it click. What a wonderful video!"
4460,When is essence of probability???
4461,"Amazing! I had done a bit of research into neural networks myself for a school project (which I aced :P), but you always manage to give the viewer an even better understanding of the topic, even if they know all about it already. Although I really need a video about LSTMs, I still haven't played with those enough to really understand how they work."
4462,this is the first time I've had any decent understanding of this subject.
4463,lol way to go
4464,I like the way u think.. u use things like assigning frequency values or layers that filter to comply ur ideas
4465,This was just an example of multi-layered feed forward network üòì
4466,"0:19 That's not a 3; that's a baby snake.

Or as ViHart would call it, snakesnakesnakesnakesnakesnakesnakesnake."
4467,One day 3Blue1Brown and Welch Labs will change the way we learn.
4468,wow. ve been waiting for this for such a long time...
4469,I'm in love with you
4470,"Thank you 3b1b. This video certainly gave me a deep enough understanding to allow my neural networks to retain the information.

EDIT: seems like I'm not the only one making lame puns about the title."
4471,ninice series
4472,My...I love you XD
4473,"Most fascinating channel on YT, hands down."
4474,"Wow, this was deep."
4475,"This series is going to be a killer.  
Human society will be on a next level when this channel is popular."
4476,"NO... Not now 3B1B, I have two deadlines due in 6 hours"
4477,Simulate a cats brain with a neural network.
4478,"Hey, I love Machine Learning?!
So glad to see you doing it!"
4479,This is deep.
4480,I just watched Welch labs machine learning playlist a few weeks ago. It was mind-blowing. I'm glad you're getting into machine learning too! : )
4481,"I have been into deep learning for a while, but I've never dreamed about finding it on 3b1b channel, insanity :D"
4482,It's an easy task when you really understand how it can be solved
4483,wow interesting
4484,Good stuff
4485,FEEELSGOODMAN
4486,Ayyye
4487,"Dear 3B1B, what software / workflow do you use for your presentations / animations?"
4488,YESSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
4489,PART 1? THERE WILL BE MORE? YAS 3BLUE1BROWN IS DOING NEURAL NETWORKS! TODAY IS A GOOD DAY
4490,secound!
4491,Yes!
